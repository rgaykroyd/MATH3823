[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH3823 Generalized Linear Models",
    "section": "",
    "text": "Appendix B: Background to Analysis of Variance"
  },
  {
    "objectID": "index.html#b.1-analysis-of-variance",
    "href": "index.html#b.1-analysis-of-variance",
    "title": "MATH3823 Generalized Linear Models",
    "section": "B.1 Analysis of Variance",
    "text": "B.1 Analysis of Variance\nConsider the four models fitted to the birth weight data. Figure 1 shows the data set along with the corresponding fitted model as a single line, for the models which do not take \\(\\texttt{Sex}\\) into account, and two lines, for the models which include \\(\\texttt{Sex}\\).\n\n\n\n\n\n\n\n(a) Model 0\n\n\n\n\n\n\n\n(b) Model 1\n\n\n\n\n\n\n\n\n\n(c) Model 2\n\n\n\n\n\n\n\n(d) Model 3\n\n\n\n\nFigure 1: Birthweight and gestational age data with superimposed fitted regression lines from various competing models.\n\n\nThe residual sum of squares (RSS) takes into account the vertical distance between the fitted model and the data values. Let \\(R_k\\) denote the residual sum of squares for Model \\(k:\\) \\(R_k = \\sum_{i=1}^n (y_i - \\hat{\\mu}_{ki})^2,\\) where \\(\\hat{\\mu}_{ki}\\) is the fitted value for individual \\(i\\) under \\(\\texttt{Model}\\) \\(k\\) and let \\(r_k\\) denote the corresponding residual degrees of freedom for \\(\\texttt{Model}\\) \\(k\\) (the number of observations minus the number of model parameters). The table below shows these values for the four models fitted to the data.\n\n\nTable 1: Summary of the residual sums of squares\n\n\n\n\n\n\n\n\n\n\\(\\texttt{Model}~k\\)\n\\(R_k\\)\n\\(r_k\\)\n\\(R_k-R_{k-1}\\)\n\\(r_k-r_{k-1}\\)\n\n\n\n\n\\(\\texttt{0}\\)\n1829873\n23\n\n\n\n\n\\(\\texttt{1}\\)\n816074\n22\n1013799\n1\n\n\n\\(\\texttt{2}\\)\n658771\n21\n157304\n1\n\n\n\\(\\texttt{3}\\)\n652425\n20\n6346\n1\n\n\n\n\nThe table also shows the change in residual sums of squares, \\(R_k-R_{k-1}\\), which measures the improvement in the fit due to the extra parameters used in Model \\(k\\) compared to Model \\(k-1\\). The RSS and changes in RSS values are also shown in Figure 2. It is clear that there is a substantial reduction in RSS moving from Model \\(0\\) to Model \\(1\\), but small reductions as further parameters are added to the model. We might guess that Model \\(1\\) will be the ``best’’ model, but a it is not acceptable to base a choice on our personal subjective opinion but instead a sequence of hypothesis tests will be used.\n\n\n\n\n\n\n\n(a) Residual sum of squares\n\n\n\n\n\n\n\n(b) Change in RSS\n\n\n\n\nFigure 2: Birthweight and gestational age data with superimposed fitted regression lines from various competing models.\n\n\nHere, a sequence of three hypothesis tests is considered: Starting with \\[\\mbox{Test 1} \\quad H_0: \\texttt{Model } 0 \\text{ is true};  H_1: \\texttt{Model } 1 \\text{ is true}.\\] Which can be judged by comparing \\(R_1-R_0=\\) 1013799 which follows a \\(\\sigma^2\\chi^2\\) distribution on \\(r_1-r_0=1\\) degrees of freedom (\\((R_1-R_0)/\\sigma^2\\) follows a \\(\\chi^2_1\\)distribution) with \\(R_1=\\) 816074 which follows a \\(\\sigma^2\\chi^2\\) distribution on \\(r_1=22\\) degrees of freedom (\\(R_1/\\sigma^2\\) follows a \\(\\chi^2_{22}\\) distribution. Fortunately, taking the ratio eliminates \\(\\sigma^2\\) giving the test statistics \\[\nF_{01} = \\frac{(R_1-R_0)/(r_1-r_0)}{R_1/r_1}=\\frac{1013799/1}{816074/22}=27.33\n\\] If \\(H_0\\) is true, then we would expect this to be close to 1. The 5%, 1% and 0.1% critical values for the distribution are 4.3, 7.95, 14.38, and the observed F statistics is much larger than all these and hence \\(\\mbox{p-value}&lt;0.001\\) meaning we reject \\(H_0\\) in favour of \\(H_1\\).\nIf \\(H_0\\) had been accepted then the sequence would stops and \\(\\texttt{Model}~0\\) declared the best, whereas \\(H_0\\) is rejected and the next test is considered \\[\\mbox{Test 2} \\quad H_0: \\texttt{Model } 1 \\text{ is true};  H_1: \\texttt{Model } 2 \\text{ is true}.\\] If \\(H_0\\) is accepted here then the sequence stops and \\(\\texttt{Model}~1\\) is declared the best, whereas is \\(H_0\\) is rejected then the last test is considered \\[\\mbox{Test 3} \\quad H_1: \\texttt{Model } 2 \\text{ is true};  H_1: \\texttt{Model } 3 \\text{ is true}.\\] If \\(H_0\\) is accepted here then the sequence stops and \\(\\texttt{Model}~2\\) is declared the best, whereas if \\(H_0\\) is rejected then \\(\\texttt{Model } 3\\) is declared the best."
  },
  {
    "objectID": "index.html#b.2-distributions-derived-from-the-gaussian-distribution",
    "href": "index.html#b.2-distributions-derived-from-the-gaussian-distribution",
    "title": "MATH3823 Generalized Linear Models",
    "section": "B.2 Distributions derived from the Gaussian distribution",
    "text": "B.2 Distributions derived from the Gaussian distribution\n\nThe Gaussian (normal) distribution\nIf \\(X\\sim N(\\mu, \\sigma^2)\\) then \\[\nf(x) = \\frac{1}{ \\sqrt {2\\pi \\sigma^2} }\n\\exp \\left\\{ - \\frac12\n\\frac{(x-\\mu)^2}{\\sigma^2} \\right\\} , \\hspace{10mm}\n-\\infty &lt;x &lt;\\infty .\n\\] Properties:\n\nThe parameter \\(\\mu=\\mbox{E}[X]\\) is a location parameter and \\(\\sigma^2=\\mbox{Var}[X]\\) is a scale parameter.\nIf \\(X \\sim N(\\mu, \\sigma^2)\\) then \\(aX+b \\sim N(a\\mu+b, a^2\\sigma^2).\\)\nIf \\(X_i \\sim N(\\mu_i, \\sigma_i^2), i=1,...,n\\) (independent) then \\(\\sum a_i X_i \\sim N(\\sum a_i\\mu, \\sum a_i^2\\sigma^2).\\)\nA special case is when \\(\\mu=0\\) and \\(\\sigma^2=1\\) which is called the standard normal distribution.\n\n\n\nThe Chi-squared distribution\nIf \\(X\\) has a Chi-squared distribution, \\(X\\sim \\chi_\\nu ^2\\) then \\[\nf(x) = \\frac{ (\\frac12) ^{\\frac{\\nu}{2}} x^{\\frac{\\nu}{2}-1} e^{-\\frac12 x}}{\\Gamma \\left(\\frac{\\nu}{2}\\right)}, \\hspace{10mm}\nx\\ge 0, \\nu &gt;0 \\mbox{ and integer}.\n\\] with \\(\\mbox{E}[X] = \\nu\\) and \\(\\mbox{Var}[X] = 2 \\nu\\).\nProperties:\n\nThe parameter \\(\\nu\\) is a shape parameter and is called the degrees of freedom. The pdf is positive skew, but becomes more symmetric as \\(\\nu\\) increases.\nIf \\(Z \\sim N(0,1)\\) then \\(Z^2 \\sim \\chi_1^2\\).\nIf \\(X_i \\sim \\chi_{\\nu_i} ^2, i=1,...,n\\) (independent) then \\(\\sum X_i \\sim \\chi_\\nu ^2\\), where \\(\\nu = \\sum \\nu_i\\).\nIf \\(Z_i \\sim N(0,1), i=1,...,n\\) (independent) then \\(\\sum Z_i^2 \\sim \\chi_n ^2\\).\nThis is a special case of the gamma distribution, with \\(\\alpha = \\nu/2\\) and \\(\\lambda = \\frac12\\), that is \\(\\gamma ({\\frac{\\nu}{2}}, \\frac12)\\).\n\n\n\nThe t- and F-distributions\nIf \\(X\\) has a t-distribution, \\(X\\sim t_{\\nu}\\) then \\[\nf(x) =\n\\frac{1}{\\sqrt {\\pi \\nu}}\n\\frac{\\Gamma ((\\nu+1)/2)}{\\Gamma(\\nu/2)}\n\\left(1+ \\frac{x^2}{\\nu}\\right)^{-\\frac12 (\\nu+1)}\n\\hspace{5mm} -\\infty&lt;x&lt;\\infty,\n\\] where \\(\\nu &gt;0\\) and integer.\nProperties:\n\nThe parameter \\(\\nu\\) is called the degrees of freedom.\nIf \\(X \\sim N(0,1)\\) and \\(Y \\sim \\chi_\\nu ^2\\) (independent) then \\[\n\\frac{X}{\\sqrt {Y/\\nu}} \\sim t_\\nu .\n\\]\nIf \\(X \\sim t_{\\nu}\\) then \\(X^2 \\sim F_{1,\\nu}\\).\n\\(t_{\\nu} \\rightarrow N(0,1)\\) as \\(\\nu \\rightarrow \\infty\\).\n\nIf \\(X\\) has an F-distribution, \\(X\\sim F_{\\nu_1, \\nu_2}\\) then \\[\nf (x) =\n\\frac{\\nu_1 ^{\\frac{\\nu_1}{2}} \\nu_2 ^{\\frac{\\nu_2}{2}} x ^{\\frac{\\nu_1}{2}-1} }\n{B(\\frac{\\nu_1}{2}, \\frac{\\nu_2}{2}) (\\nu_2+\\nu_1 x)^{\\frac{\\nu_2+\\nu_1}{2}} } \\hspace{10mm} x\\ge 0.\n\\] where \\(\\nu_1, \\nu_2 &gt; 0\\) and integer are know as the degrees of freedom.\nProperties:\n\nThe parameters \\(\\nu_1\\) and \\(\\nu_2\\) are called the degrees of freedom.\nIf \\(X_1 \\sim \\chi_{\\nu_2} ^2\\) and \\(X_2 \\sim \\chi_{\\nu_2} ^2\\) (independent) then \\[\n\\frac{X_1/\\nu_1}{X_2/\\nu_2} \\sim F_{\\nu_1, \\nu_2}.\n\\]\nIf \\(X \\sim F_{\\nu_1, \\nu_2}\\) then \\(1/X \\sim F_{\\nu_2, \\nu_1}\\), hence, \\(Pr(F_{\\nu_1, \\nu_2} &lt; c) = Pr(F_{\\nu_2, \\nu_1} &gt; 1/c).\\)"
  }
]