[
  {
    "objectID": "0_preface.html",
    "href": "0_preface.html",
    "title": "Overview",
    "section": "",
    "text": "Official Module Description"
  },
  {
    "objectID": "0_preface.html#welcome",
    "href": "0_preface.html#welcome",
    "title": "Overview",
    "section": "Welcome!",
    "text": "Welcome!\nHere is a short video [4 mins] to introduce the module."
  },
  {
    "objectID": "0_preface.html#preface",
    "href": "0_preface.html#preface",
    "title": "Overview",
    "section": "Preface",
    "text": "Preface\nThese lecture notes are produced for the University of Leeds module MATH3823 - Generalized Linear Models for the academic year 2023-24. Please note that this material also forms part of the module MATH5824 - Generalized Linear and Additive Models. They are based on the lecture notes used previously for this module and I am grateful to previous module lecturers for their considerable effort: Lanpeng Ji, Amanda Minter, John Kent, Wally Gilks, and Stuart Barber. This year, again, I am using Quarto (a successor to RMarkdown) from RStudio to produce both the html and PDF, and then GitHub to create the website which can be accessed at rgaykroyd.github.io/MATH3823/. Please note that the PDF versions will only be made available on the University of Leeds Minerva system. Although I am a long-term user of RStudio, I am a novice at Quarto/RMarkdown and a complete beginner using Github and hence please be patient if there are hitches along the way.\nRG Aykroyd, Leeds, January 3, 2024"
  },
  {
    "objectID": "0_preface.html#changes-since-last-year",
    "href": "0_preface.html#changes-since-last-year",
    "title": "Overview",
    "section": "Changes since last year",
    "text": "Changes since last year\nFeedback from the students last year was very positive, but there were consistent comments regarding two issues: (1) a shortage of practice exercises and the opportunity to discuss these in class, and (2) limited RStudio support in preparation for the assessment. For the first of these, additional exercises have been prepared and are included in the learning material. Also, I am trying some short quizzes so that you can check your basic knowledge. Further, I intend to set-aside some lecture time for us to discuss selected exercises. For the second, an additional computer session has been added, in Week 5 (26 February - 1 March), this is 3 weeks before the assessed practice in Week 8 (18 - 22 March). Further, a few new instructional videos will be available addressing some RStudio topics. Together, these represents a considerable about of extra work for me, but I hope that they are helpful and so please give your feedback whenever there is an opportunity."
  },
  {
    "objectID": "0_preface.html#generative-ai-usage-within-this-module",
    "href": "0_preface.html#generative-ai-usage-within-this-module",
    "title": "Overview",
    "section": "Generative AI usage within this module",
    "text": "Generative AI usage within this module\nThe assessments for this module fall in the red category for using Generative AI which means you must not use Generative AI tools. The purpose and format of the assessments makes it inappropriate or impractical for AI tools to be used.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nStatistical ethics and sensitive data\nPlease note that from time to time we will be using data sets from situations which some might perceive as sensitive. All such data sets will, however, be derived from real-world studies which appear in textbooks or in scientific journals. The daily work of many statisticians involves applying their professional skills in a wide variety of situations and as such it is important to include a range of commonly encountered examples in this module. Whenever possible, sensitive topics will be signposted in advance. If you feel that any examples may be personally upsetting then, if possible, please contact the module lecturer in advance. If you are significantly effected by any of these situations, then you can seek support from the Student Counselling and Wellbeing service."
  },
  {
    "objectID": "0_preface.html#module-summary",
    "href": "0_preface.html#module-summary",
    "title": "Overview",
    "section": "Module summary",
    "text": "Module summary\nLinear regression is a tremendously useful statistical technique but is very limited. Generalised linear models extend linear regression in many ways - allowing us to analyse more complex data sets. In this module we will see how to combine continuous and categorical predictors, analyse binomial response data and model count data."
  },
  {
    "objectID": "0_preface.html#objectives",
    "href": "0_preface.html#objectives",
    "title": "Overview",
    "section": "Objectives",
    "text": "Objectives\nOn completion of this module, students should be able to:\n\ncarry out regression analysis with generalised linear models including the use of link functions;\nunderstand the use of deviance in model selection;\nappreciate the problems caused by overdispersion;\nfit and interpret the special cases of log linear models and logistic regression;\nuse a statistical package with real data to fit these models to data and to write a report giving and interpreting the results."
  },
  {
    "objectID": "0_preface.html#syllabus",
    "href": "0_preface.html#syllabus",
    "title": "Overview",
    "section": "Syllabus",
    "text": "Syllabus\nGeneralised linear model; probit model; logistic regression; log linear models."
  },
  {
    "objectID": "0_preface.html#university-module-catalogue",
    "href": "0_preface.html#university-module-catalogue",
    "title": "Overview",
    "section": "University Module Catalogue",
    "text": "University Module Catalogue\nFor any further details, please see MATH3823 Module Catalogue page"
  },
  {
    "objectID": "1_intro.html#overview",
    "href": "1_intro.html#overview",
    "title": "1  Introduction",
    "section": "1.1 Overview",
    "text": "1.1 Overview\nIn previous modules you have studied linear models with a normally distributed error term, such as simple linear regression, multiple linear regression and ANOVA for normally distributed observations. In this module we will study generalized linear models.\nOutline of the module:\n\nRevision of linear models with normal errors.\nIntroduction to generalized linear models, GLMs.\nLogistic regression models.\nLoglinear models, including contingency tables.\n\n\n\n\n\n\n\nImportant\n\n\n\nThis module will make extensive use of \\(\\mathbf{R}\\) and hence it is very important that you are comfortable with its use. If you need some revision, then material is available on Minerva under RStudio Support.\n\n\nThe purpose of a generalized linear model is to describe the dependence of a response variable \\(y\\) on a set of \\(p\\) explanatory variables \\(\\b{x}=(x_1, x_2, \\ldots, x_p)\\) where, conditionally on \\(\\b{x}\\), observation \\(y\\) has a distribution which is not necessarily normal. Note that the normal distribution situation is a special case of the general framework and we will study that in the next Chapter.\nPlease be aware that in this learning material we may use lowercase letters, for example \\(y\\) or \\(y_i,\\) to denote both observed values or random variables, which is being considered should be clear from the context.\n\n\n\n\n\n\nImportant\n\n\n\nThis module will make extensive use of many basic ideas from statistics. If you need some revision, then see Appendix A: Basic material on Minerva under Basic Pre-requisite Material."
  },
  {
    "objectID": "1_intro.html#motivating-example",
    "href": "1_intro.html#motivating-example",
    "title": "1  Introduction",
    "section": "1.2 Motivating example",
    "text": "1.2 Motivating example\nTable 1.1 shows data1 on the number of beetles killed by five hours of exposure to 8 different concentrations of gaseous carbon disulphide.\n\n\nTable 1.1: Numbers of beetles killed by five hours of exposure to 8 different concentrations of gaseous carbon disulphide\n\n\n\n\n\n\n\nDose\n\\(x_i\\)\nNo. of beetle\n\\(m_i\\)\nNo. killed\n\\(y_i\\)\n\n\n\n\n1.6907\n59\n6\n\n\n1.7242\n60\n13\n\n\n1.7552\n62\n18\n\n\n1.7842\n56\n28\n\n\n1.8113\n63\n52\n\n\n1.8369\n59\n53\n\n\n1.8610\n62\n61\n\n\n1.8839\n60\n60\n\n\n\n\nFigure 1.1 (a) shows the same data with a linear regression line superimposed. Although this line goes close to the plotted points, we can see some fluctuations around it. More seriously, this is a stupid model: it would predict a mortality rate of greater than 100% at a dose of 1.9 units, and a negative mortality rate at 1.65 units!\n\n\nCode\npar(mar=c(4,4,0,1))\n\nbeetle = read.table(\"https://rgaykroyd.github.io/MATH3823/Datasets/beetle.txt\", header=T)\n\ndose = beetle$dose\nmortality = beetle$died/beetle$total\n\nplot(dose, mortality, pch=16, col=\"blue\",\n     xlim=c(1.65, 1.90), xlab =\"Dose\",\n     ylim=c(-0.1, 1.1),  ylab=\"Mortality\")\nabline(h=c(0,1), lty=2)\n\n# Fit a linear model\nlm.fit = lm(mortality ~ dose)\nabline(lm.fit, col=\"red\")\n\n# Fit a logisitc model\nplot(dose, mortality, pch=16, col=\"blue\",\n     xlim=c(1.65, 1.90), xlab =\"Dose\",\n     ylim=c(-0.1, 1.1),  ylab=\"Mortality\")\nabline(h=c(0,1), lty=2)\n\ny = cbind(beetle$died, beetle$total-beetle$died)\nglm.fit = glm(y ~ dose, family=binomial(link='logit'))\n\noutput.dose = seq(1.6,1.95,0.001)\nfitted = predict(glm.fit, data.frame(dose=output.dose), type=\"response\")\nlines(output.dose, fitted, col=\"red\")\n\n\n\n\n\n\n\n\n(a) Linear model\n\n\n\n\n\n\n\n(b) Logistic model\n\n\n\n\nFigure 1.1: Beetle mortality rates with fitted dose- response curves.\n\n\n\nA more sensible dose–response relationship for the beetle mortality data might be based on the logistic function (to be defined later), as plotted in Figure 1.1 (b). The resulting curve is a closer, more-sensible, fit. Later in this module we will see how this curve was fitted using maximum likelihood estimation for an appropriate generalized linear model.\nThis is an example of a dose-response experiment which are widely used in medical and pharmaceutical situations.\n\n\n\n\n\n\nWarning\n\n\n\nWarning of potentially sensitive material. For further information on dose-response experiments see, for example, www.britannica.com/science/dose-response-relationship."
  },
  {
    "objectID": "1_intro.html#focus-on-correlation-quiz",
    "href": "1_intro.html#focus-on-correlation-quiz",
    "title": "1  Introduction",
    "section": "Focus on correlation quiz",
    "text": "Focus on correlation quiz\nTest your knowledge recall and comprehension to reinforce idea of linear relationships and correlation.\n\n\nFor each situation, choose one of the following statements which you think is most likely to apply.\n\nThe diastolic blood pressure and the weight of patients attending a heart health clinic at the Leeds General Infirmary. positive correlationuncorrelatednegative correlationother\nThe daily stock market closing prices of British Telecom and Virgin Media shares on the London Stock Exchange. positive correlationuncorrelatednegative correlationother\nThe daily rainfall and hours of sunshine collected at a weather monitoring station in the Pennines of Yorkshire. positive correlationuncorrelatednegative correlationother\nThe number of road accidents occurring at a busy roundabout and the UK Retail Prices Index. positive correlationuncorrelatednegative correlationother\n\n\n\n\nClick here to see explanations\n\n\nIt is well documented that people who are over-weight (or at least high BMI) are more likely to have high blood pressure. This is true for systolic and diastolic blood pressure as well as for men and women. It is not certain if the relationship will be linear, but it will lead to a strong positive correlation value. This is likely to be a causal relationship, excess weight causes high blood pressure.\nAlthough these two companies are in the same business sectors, technology and entertainment, it is unlikely that direct competition will be the main factor in the relative behaviour – if it were, then we might expect a negative correlation, that is one does well if the other does badly. Instead, they are both likely to be driven by the same economic and social trends. This is not a causal relationship but both are being driven by an (unseen) third variable. It does, however, lead to a correlation.\nAlthough both weather features, rainfall and sun, can happen at the same time – perhaps leading to a rainbow – and there are very many cases when neither occurs – for example a cloudy but dry day – there is a general pattern that it will not rain when it is sunny and it will not be sunny when it rains. This leads to a negative correlation and a moderate value is likely even if the relationship is not linear. Again, this is not a causal relationship but is being driven, perhaps, by the presence of clouds.\nIt is hard to imagine that there will be a relationship between the number of road accidents and an inflation measure, hence a value of correlation close to zero – though do not expect to ever get a value of exactly zero. It is not completely, inconceivable that the number of road accidents will be higher when the economy is active, but this is unlikely to lead to a substantial correlation value – if you know otherwise, let me know!\n\n\n\n\n\nFrom each of the following scatter plots, choose from the drop down list which correlation value is most likely.\n\nWhat is the most likely correlation for the data below? -1.0-0.7-0.30.0+0.3+0.7+1.0::: {.cell-output-display} \n\n\n\nWhat is the most likely correlation for the data below? -1.0-0.7-0.30.0+0.3+0.7+1.0::: {.cell-output-display} \n\n\n\nWhat is the most likely correlation for the data below? -1.0-0.7-0.30.0+0.3+0.7+1.0::: {.cell-output-display}  :::\nWhat is the most likely correlation for the data below? -1.0-0.7-0.30.0+0.3+0.7+1.0::: {.cell-output-display}  :::\nWhat is the most likely correlation for the data below? -1.0-0.7-0.30.0+0.3+0.7+1.0::: {.cell-output-display}  :::\nWhat is the most likely correlation for the data below? -1.0-0.7-0.30.0+0.3+0.7+1.0::: {.cell-output-display}  :::\n\n:::\n\n\nClick here to see explanations\n\n\nImagine dividing the plot by horizontal and vertical lines at the respective mean values. There would be a majority of points in the top-right and bottom-left indicating a positive correlation, but there are still some on the other quadrants. Hence, +1 is too high and 0.3 is too low, but would be suitable if the points were closer to a line or more dispersed, respectively. For information, the exact value is 0.70.\nAgain, imagine dividing the plot by horizontal and vertical lines at the respective mean values. This time the majority of points would be in the top-left and bottom-right quadrants, and there is moderate spread. A value -1 is too extreme and -0.3 is too close to zero, but would be suitable if the points were closer to a line or more dispersed, respectively. For information, the exact value is -0.60.\nA similar situation to Question 6, but there is noticeably more spread. Although the majority of points are in the top-left and bottom-right quadrants there are substantial numbers in the other quadrants. It would be inaccurate to say that this shows uncorrelated variables as there is a definite negative slope to the pattern. For information, the exact value is -0.30.\nThis is a difficult one as there is a clear relationship, but it is quadratic rather than linear. For information, the exact value is 0.30. Perhaps without the accompanying graph, this correlation value would be misleading.\nDividing the plot by horizontal and vertical lines at the respective mean values leaves very similar numbers of points in al four quadrants. This indicates that the correlation will be close to zero – here is no relationship. For information, the exact value is 0.01.\nAlmost all of the points are in the top-left and bottom-right quadrants indicating a negative correlation. The points are very close to the linear and hence a value close to -1 is likely – such extreme cases are rare. For information, the exact value is -0.995.\n\n\n:::"
  },
  {
    "objectID": "1_intro.html#revision-of-least-squares-estimation",
    "href": "1_intro.html#revision-of-least-squares-estimation",
    "title": "1  Introduction",
    "section": "1.3 Revision of least-squares estimation",
    "text": "1.3 Revision of least-squares estimation\nSuppose that we have \\(n\\) paired data values \\((x_1, y_1),\\dots, (x_n, y_n)\\) and that we believe these are related by a linear model\n\\[\ny_i = \\alpha+\\beta x_i +\\epsilon_i\n\\]\nfor all \\(i\\in \\{1, 2,\\dots,n\\}\\), where \\(\\epsilon_1,\\dots,\\epsilon_n\\) are independent and identically distributed (iid) with \\(\\mbox{E}[\\epsilon_i]=0\\) and \\(\\mbox{Var}[\\epsilon_i]=\\sigma^2\\). The aim will be to find values of the model parameters, \\(\\alpha, \\beta \\text{ and } \\sigma^2\\) using the data. Specifically, we will estimate \\(\\alpha\\) and \\(\\beta\\) using the values which minimize the residual sum of squares (RSS)\n\\[\nRSS(\\alpha, \\beta) = \\sum_{i=1}^n \\left(y_i-(\\alpha+\\beta x_i)\\right)^2.\n\\tag{1.1}\\]\nThis measures how close the data points are around the regression line and hence the resulting estimates, \\(\\hat\\alpha\\) and \\(\\hat\\beta\\), will give us a fitted regression line which is closest to the data.\nFigure 1.2 illustrates this process. The data points are fixed but we are free to choose values for \\(\\alpha\\) and \\(\\beta\\), that is to move the line up and down and to rotate it as needed. In general, the data points, however, do not sit exactly on any line. For any values of \\(\\alpha\\) and \\(\\beta\\) the value on the line \\(y=\\alpha+\\beta x\\) can be calculated and the discrepancy, as measured in the vertical direction, is defined as the residual, \\(r_i=y_i-\\left(\\alpha+\\beta x_i\\right)\\). Then, the residual sum of squares is formed as the sum of the squares of these individual residuals.\n\n\n\n\n\nFigure 1.2: Diagram showing linear regression method\n\n\n\n\nIt can be shown that Equation 1.1 takes its minimum when the parameters are given by\n\\[\n\\hat\\alpha = \\bar y -\\hat\\beta\\bar x, \\quad \\mbox{and} \\quad\n\\hat\\beta = \\frac{s_{xy}}{s^2_x}\n\\tag{1.2}\\]\nwhere \\(\\bar x\\) and \\(\\bar y\\) are the sample means,\n\\[\ns_{xy}=\\frac{1}{n-1}\\sum_{i=1}^n (x_i-\\bar x)(y_i-\\bar y)\n\\]\nis the sample covariance and\n\\[\ns^2_x = \\frac{1}{n-1} \\sum_{i=1}^n (x_i-\\bar x)^2\n\\]\nis the sample variance of the \\(x\\) values. It can be shown that these estimators are unbiased, that is \\(\\mbox{E}[\\hat\\alpha]=\\alpha\\) and \\(\\mbox{E}[\\hat\\beta]=\\beta\\) – see Section 1.5.\nThe fitted regression lines is then given by \\(\\hat y = \\hat \\alpha +\\hat \\beta x\\), the fitted values by \\(\\hat y_i = \\hat \\alpha +\\hat \\beta x_i\\), and the model residuals by \\(r_i= \\hat \\epsilon_i= y_i-\\hat y_i\\) for all \\(i\\in \\{1,\\dots,n\\}.\\)\nTo complete the model fitting, we also estimate the error variance, \\(\\sigma^2\\), using \\[\n\\hat \\sigma^2 = \\frac{1}{n-2} \\sum _{i=1}^n r_i^2.\n\\tag{1.3}\\]\nNote that, by construction, \\(\\bar r=0\\) and, further, it can be shown that \\(\\hat \\sigma^2\\) is an unbiased estimator of \\(\\sigma^2\\), that is \\(\\mbox{E}[\\hat\\sigma^2]=\\sigma^2\\).\n\n\nCode\nxbar = mean(dose)\nybar = mean(mortality)\ns2x = var(dose)\nsxy = cov(dose, mortality)\n\nbetahat = sxy/s2x\nalphahat = ybar-betahat*xbar\n\ns2hat = sum((mortality-alphahat-betahat*dose)^2)/(length(dose)-2)\n\n\nReturning to the above beetle data example, we have \\(\\hat\\alpha=\\)-8.947843, \\(\\hat\\beta=\\) 5.324937, and \\(\\hat \\sigma^2 =\\) 0.0075151.\nWe will interpret the output later, but in \\(\\b{R}\\), the fitting can be done with a single command with corresponding fitting output from a second command:\n\n\nCode\nlm.fit = lm(mortality ~ dose)\n\nsummary(lm.fit)\n\n\n\nCall:\nlm(formula = mortality ~ dose)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.10816 -0.06063  0.00263  0.05119  0.12818 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -8.9478     0.8717  -10.27 4.99e-05 ***\ndose          5.3249     0.4857   10.96 3.42e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.08669 on 6 degrees of freedom\nMultiple R-squared:  0.9524,    Adjusted R-squared:  0.9445 \nF-statistic: 120.2 on 1 and 6 DF,  p-value: 3.422e-05\n\n\n\n\n\n\n\n\nImportant\n\n\n\nYou should have met \\(\\b{R}\\) output like this in previous statistics modules, but if you need some revision then see Appendix-C: Background to Analysis of Variance on Minerva under Basic Pre-requisite Material."
  },
  {
    "objectID": "1_intro.html#sec-typevariables",
    "href": "1_intro.html#sec-typevariables",
    "title": "1  Introduction",
    "section": "1.4 Types of variables",
    "text": "1.4 Types of variables\nThe way a variable enters a model will depends on its type. The most common five types of variable are:\n\nQuantitative\n\nContinuous: for example, height; weight; duration. Real valued. Note that although recorded data is rounded it is still usually best regarded as continuous.\nCount (discrete): for example, number of children in a family; accidents at a road junction; number of items sold. Non-negative and integer-valued.\n\nQualitative\n\nOrdered categorical (ordinal): for example, severity of illness (Mild/ Moderate/Severe); degree classification (first/ upper-second/ lower-second/ third).\nUnordered categorical (nominal):\n\nDichotomous (binary): two categories: for example sex (M/ F); agreement (Yes/ No); coin toss (Head/ Tail).\nPolytomous (also known as polychotomous): more than two categories: for example blood group (A/ B/ O); eye colour (Brown/ Blue/ Green).\n\n\n\nNote that although dichotomous is clearly a special case of polytomous, making the distinction is usually worthwhile as it often leads to a simplified modelling and testing approach."
  },
  {
    "objectID": "1_intro.html#focus-on-data-type-quiz",
    "href": "1_intro.html#focus-on-data-type-quiz",
    "title": "1  Introduction",
    "section": "Focus on data type quiz",
    "text": "Focus on data type quiz\n\n\nTest your knowledge recall and comprehension to reinforce ideas ready for later in the module\nFor each of the following situations what is the most appropriate data type: nominal, ordinal, discrete, or continuous?\n\nThe eye colour of 100 patients visiting the Yorkshire Cancer Research Centre, for example, grey, green, brown, blue…. nominalordinaldiscretecontinuous\nThe nationality of students at the University of Leeds, for example, British, Chinese, Greek, Indian…. nominalordinaldiscretecontinuous\nThe five-star ratings submitted by 50 customers on TripAdvisor for the Leeds Queens Hotel, for example 1 star, 2 star,… 5 star. nominalordinaldiscretecontinuous\nThe diastolic blood pressure of 20 male and 20 female patients attending a heart health clinic at the Leeds General Infirmary in a study to investigate differences between men and women, for example, 80 mm Hg, 130 mm Hg,… nominalordinaldiscretecontinuous\nThe daily stock market closing price of British Telecom shares on the London Stock Exchange over a year to study the change over time, for example, 114.75p, 115.10p,… nominalordinaldiscretecontinuous\nThe January monthly rainfall collected since 1961 at a weather monitoring station in the Pennines of Yorkshire, for example, 8 mm , 12 mm,… nominalordinaldiscretecontinuous\nThe level of satisfaction of 100 randomly chosen voters with the policies of a political party, for example, agree, fully agree, neither agree nor disagree, disagree, and fully disagree. nominalordinaldiscretecontinuous\nThe number of new people following the TheRoyalFamily twitter page per day over a year, for example 459, 700,… to study the change due to a royal wedding. nominalordinaldiscretecontinuous\nThe number of road accidents occurring per month, at a busy roundabout over a 10-year period to study the change over time, for example, 0, 1, 2,… nominalordinaldiscretecontinuous\nThe number of Scottish strawberries in 50 randomly selected boxes bought from ASDA supermarket, for example, 50, 58, 68,… nominalordinaldiscretecontinuous\n\n\n\n\nClick here to see explanations\n\n\nEye colour is qualitative and can take any one of an unordered set of categories. Although the eye colours are categories, there is no clear ordering to the colours.\nNationality is qualitative and can take any one of an unordered set of categories. Although the nationality are categories, there is no clear ordering to the countries.\nThe rating is qualitative and can take any one of set of categories but the categories are clearly ordered, 5 star is better than 4 start etc. Although the ratings are represented by integers, there is no reason why the difference between 1 and 2 stars has the same interpretation as between 4 and 5 stars and hence it cannot be discrete.\nAlthough the recorded values might take only integer values, blood pressure is a measurement and could take be any real number.\nShare price is a measurement and could be any real number, even though in practice it will be rounded.\nRainfall is a measurement and although the recorded values might take only integer values, rainfall could be any real number.\nSatisfaction score is qualitative and can take any one of set of categories but the categories are clearly ordered, fully agree is better than agree etc. Although the scores could be represented by numerical values, e.g. 1,2,3,4,5, there is no reason why the difference between 1 and 2 has the same interpretation as between 4 and 5 and hence it is not discrete.\nThe number of people is a quantitative count which is limited to the non-negative integers. The variable is discrete.\nThe number of accidents is a quantitative count, being limited to the non-negative integers and hence is discrete.\nThe number of strawberries is a quantitative a count which is limited to the non-negative integers. The variable is discrete."
  },
  {
    "objectID": "1_intro.html#sec-exercises1",
    "href": "1_intro.html#sec-exercises1",
    "title": "1  Introduction",
    "section": "1.5 Exercises",
    "text": "1.5 Exercises\n\n\n\n\n\n\nImportant\n\n\n\nUnless otherwise stated, data files will be available online at: rgaykroyd.github.io/MATH3823/Datasets/filename.ext, where filename.ext is the stated filename with extension.\n\n\n\n1.1 Consider again the beetle data in Table 1.1. Perform the calculations by hand and then check the answers using \\(\\b{R}\\) – a copy of the data is available in the file beetle.txt. Finally plot the fitted regression line on a scatter plot of the data.\n\n\nClick here to see hints.\n\nSee the code chunk used to produce Figure 1.1.\n\n1.2 Consider the following synthetic data:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(i=1\\)\n\\(i=2\\)\n\\(i=3\\)\n\\(i=4\\)\n\\(i=5\\)\n\\(i=6\\)\n\\(i=7\\)\n\\(i=8\\)\n\n\n\n\n\\(x_i\\)\n-1\n0\n1\n2\n2.5\n3\n4\n6\n\n\n\\(y_i\\)\n-2.8\n-1.1\n7.2\n8.0\n8.9\n9.2\n14.8\n24.7\n\n\n\nPlot the data to check that a linear model is suitable and then fit a linear regression model. Do you think that the fitted model can be reliably used to predict the values of \\(y\\) when \\(x=5\\) and \\(x=10\\)? Justify your answers.\n\n\nClick here to see hints.\n\nWhich is more reliable prediction for a value within the range of the data (interpolation) or outside the range of the data (extrapolation)?\n\n1.3 Starting from Equation 1.1, derive the estimation equations given in Equation 1.2. Show that \\(\\hat\\alpha\\) and \\(\\hat\\beta\\) are unbiased estimators of \\(\\alpha\\) and \\(\\beta\\). What can be said about \\(\\hat\\sigma^2\\) as an estimator of \\(\\sigma^2\\)?\n\n\nClick here to see hints.\n\nFor unbiasedness of intercept and slope check your MATH1712 lecture notes. For the latter, there is a careful theoretical proof about the variance parameter, but here only an intuitive explanation is expected.\n\n1.4 The Brownlee’s Stack Loss Plant Data2 is already available in \\(\\mathbf{R}\\), with background details on the help page, \\(\\texttt{?stackloss}\\).\nAfter plotting all pairs of variables, which of \\(\\texttt{Air.Flow}\\), \\(\\texttt{Water.Temp}\\) and \\(\\texttt{Acid.Conc}\\) do you think could be used to model \\(\\texttt{stack.loss}\\) using a linear regression? Justify your answer.\nPerform a simple linear regression with using \\(\\texttt{stack.loss}\\) as the response variable and your chosen variable as the explanatory variable. Add the fitted regression line to a scatter plot of the data and comment.\n\n\nClick here to see hints.\n\nYou already met this example in MATH1712 – check your lecture note for guidance.\n\n1.5 In an experiment conducted by de Silva et al. in 20203 data was obtained to investigate falling objects and gravity, as first consider by Galileo and Newton. A copy of the data is available in the file physics_from_data.csv.\nSuppose that we wish to develop a method to predict the maximum Reynolds number from a single explanatory variable. Which of the variables do you think helps explain Reynolds number the best? Why do you think this?\n\n\nClick here to see hints.\n\nRead the data into \\(\\b{R}\\) and perform a simple linear regression of the maximum Reynolds number as the response variable and, in turn, each of the other variables as the explanatory variable. Plot the data, with and add the corresponding fitted linear models.\n\n\n\n\n\n\n\n\nNote\n\n\n\nExercise 1 Solutions can be found here."
  },
  {
    "objectID": "1_intro.html#footnotes",
    "href": "1_intro.html#footnotes",
    "title": "1  Introduction",
    "section": "",
    "text": "Dobson and Barnett, 3rd edn, p.127↩︎\nBrownlee, K. A. (1960, 2nd ed. 1965) Statistical Theory and Methodology in Science and Engineering. New York: Wiley. pp. 491–500.↩︎\nde Silva BM, Higdon DM, Brunton SL, Kutz JN. Discovery of Physics From Data: Universal Laws and Discrepancies. Front Artif Intell. 2020 Apr 28;3:25. doi: 10.3389/frai.2020.00025. PMID: 33733144; PMCID: PMC7861345.↩︎"
  },
  {
    "objectID": "2_linearmodels.html#sec-essentialsoverview",
    "href": "2_linearmodels.html#sec-essentialsoverview",
    "title": "2  Essentials of Normal Linear Models",
    "section": "2.1 Overview",
    "text": "2.1 Overview\nIn many fields of application, we might assume the response variable is normally distributed. For example: heights, weights, log prices, etc.\nThe data1 in Table 2.1 record the birth weights of 12 girls and 12 boys and their gestational ages (time from conception to birth).\n\n\nTable 2.1: Gestational ages (in weeks) and birth weights (in grams) for 24 babies (12 girls and 12 boys).\n\n\n\n\n(a) Girls\n\n\nGestational Age\nBirth weight\n\n\n\n\n40\n3317\n\n\n36\n2729\n\n\n40\n2935\n\n\n37\n2754\n\n\n42\n3210\n\n\n39\n2817\n\n\n40\n3126\n\n\n37\n2539\n\n\n36\n2412\n\n\n38\n2991\n\n\n39\n2875\n\n\n40\n3231\n\n\n\n\n\n\n(b) Boys\n\n\nGestational Age\nBirth weight\n\n\n\n\n40\n2968\n\n\n38\n2795\n\n\n40\n3163\n\n\n35\n2925\n\n\n36\n2625\n\n\n37\n2847\n\n\n41\n3292\n\n\n40\n3473\n\n\n37\n2628\n\n\n38\n3176\n\n\n40\n3421\n\n\n38\n3975\n\n\n\n\n\n\nA key question is, can we predict the birth weight of a baby born at a given gestational age using these data. For this we will need to make assumptions about the relationship between birth weight and gestational age, and any associated natural variation – that is we require a model.\nFirst we should explore the data. Figure 2.1 (a) shows a histogram of the birth weights indicating a spread around modal group 2800-3000 grams; Figure 2.1 (b) indicates slightly higher birth weights for the boys than the girls; and Figure 2.1 (c) shows an increasing relationship between weight and age. Together, these suggest that gestational age and sex are likely to be important for predicting weight.\n\n\nCode\npar(mar=c(4,4,0,1))\n\nbirthweight = read.table(\"https://rgaykroyd.github.io/MATH3823/Datasets/birthwt-numeric.txt\", header=T)\n\nweight = birthweight$weight\nage = birthweight$age\nsex = birthweight$sex\n\nhist(weight, breaks=6, probability = T, main = \"\", \n     xlab = \"Birth weight (grams)\")\nboxplot(weight~sex, names=c(\"Girl\", \"Boy\"))\n\nplot(age, weight, pch=16, \n     xlab = \"Gestational age (weeks)\",\n     ylab = \"Birth weight (grams)\")\n\n\n\n\n\n\n\n\n(a) Weight distribution\n\n\n\n\n\n\n\n(b) Weight sub-divided by Sex\n\n\n\n\n\n\n\n\n\n(c) Relationship beween variables\n\n\n\n\nFigure 2.1: Birthweight and gestational age for 24 babies.\n\n\n\nBefore considering possible models, Figure 2.2 again shows the relationship between weight and age but this time with the points coloured according to the baby’s sex. This, perhaps, shows the boys to have generally higher weights across the age range than girls.\n\n\nCode\npar(mar=c(4,4,0,1))\n\nbirthweight = read.table(\"https://rgaykroyd.github.io/MATH3823/Datasets/birthwt-numeric.txt\", header=T)\n\nweight = birthweight$weight\nage = birthweight$age\nsex = birthweight$sex\n\nplot(age, weight, col=2-sex, pch=15-11*sex,\n     xlab = \"Gestational age (weeks)\",\n     ylab = \"Birth weight (grams)\")\nlegend(41,2800, c(\"Girl\",\"Boy\"), col = c(2,1), pch=c(15,4))\n\n\n\n\n\nFigure 2.2: Birthweight and gestational age for 12 girls (red squares) and 12 boys (black crosses)."
  },
  {
    "objectID": "2_linearmodels.html#focus-on-modelling-quiz",
    "href": "2_linearmodels.html#focus-on-modelling-quiz",
    "title": "2  Essentials of Normal Linear Models",
    "section": "Focus on modelling quiz",
    "text": "Focus on modelling quiz\nTest your knowledge recall and application to reinforce basic ideas and prepare for similar concepts later in the module.\n\n\nFor each situation, choose one of the following statements which you think is most likely to apply.\n\nWhat is the most useful graphical summary for identifying a potential relationship between two variables? scatter plotkernel density plothistogramboxplotother\nWhat is the most useful numerical summary for identifying a potential linear relationship between two variables? sample meanscorrelationskewvariancesother\nWhich of the following is a true statements about the correlation coefficient? (Choose any that apply.)\n\n Is always a positive number Can be positive or negative Is always between -1 and +1 Can take any real number A value close to -1 means uncorrelated and close to +1 indicates a high correlation\n\nWhich of the following is a true statements about regression? (Choose any that apply.)\n\n All regression models describe linear relationships A linear model can be fitted to any data set After fitting a regression model we should always consider residuals A linear regression model will fit well if the correlation is close to zero A linear regression can sometimes be used to approximate a non-linear relationship\n\nWhich of the following is a true statements about statistical modelling? (Choose any that apply.)\n\n Only use a model that is 100% correct Only use a linear model All models are approximations Models are part deterministic and part random Models only involve a statistical distribution"
  },
  {
    "objectID": "2_linearmodels.html#linear-models",
    "href": "2_linearmodels.html#linear-models",
    "title": "2  Essentials of Normal Linear Models",
    "section": "2.2 Linear models",
    "text": "2.2 Linear models\nContinuing the birth weight example. Of course, there are very many possible models, but here we will consider the following:\n\n\n\n\n\n\n\n\\(\\texttt{Model 0}:\\)\n\\(\\texttt{Weight}=\\alpha\\)\n\n\n\\(\\texttt{Model 1}:\\)\n\\(\\texttt{Weight}=\\alpha + \\beta.\\texttt{Age}\\)\n\n\n\\(\\texttt{Model 2}:\\)\n\\(\\texttt{Weight}=\\alpha + \\beta.\\texttt{Age}+\\gamma.\\texttt{Sex}\\)\n\n\n\\(\\texttt{Model 3}:\\)\n\\(\\texttt{Weight}=\\alpha + \\beta.\\texttt{Age}+\\gamma.\\texttt{Sex} + \\delta.\\texttt{Age}.\\texttt{Sex}\\)\n\n\n\nIn these models, \\(\\texttt{Weight}\\) is called the response variable (sometimes called the dependent variable) and \\(\\texttt{Age}\\) and \\(\\texttt{Sex}\\) are called the covariates or explanatory variables (sometimes called the predictor or independent variables). Here, \\(\\texttt{Age}\\) is a continuous variable whereas \\(\\texttt{Sex}\\) is coded as a dummy variable taking the value 0 for girls and 1 for boys; it is an example of a factor, in this case with just two levels: Girl and Boy.\nNote that \\(\\texttt{Model 0}\\) is a special case of \\(\\texttt{Model 1}\\) (consider the situation when \\(\\beta=0\\)) and that \\(\\texttt{Model 1}\\) is a special case of \\(\\texttt{Model 2}\\) (consider the situation when \\(\\gamma=0\\)) and finally that \\(\\texttt{Model 2}\\) is a special case of \\(\\texttt{Model 3}\\) (consider the situation when \\(\\delta=0\\)) – such models are called nested.\nIn these models, \\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\) and \\(\\delta\\) are model parameters. Parameter \\(\\alpha\\) is called the intercept term; \\(\\beta\\) is called the main effect of \\(\\texttt{Age}\\); and is interpreted as the effect on birth weight per week of gestational age. Similarly, \\(\\gamma\\) is the main effect of \\(\\texttt{Sex}\\), interpreted as the effect on birth weight of being a boy (because girl is the baseline category).\nParameter \\(\\delta\\) is called the interaction effect between \\(\\texttt{Age}\\) and \\(\\texttt{Sex}\\). Take care when interpreting an interaction effect. Here, it does not mean that age somehow affects sex, or vice-versa. It means that the effect of gestational age on birth weight depends on whether the baby is a boy or a girl.\nThese models can be fitted to the data using (Ordinary) Least Squares to produce the results presented in Figure 2.3.\nWhich model should we use?\n\n\nCode\npar(mar=c(4,4,0,1))\n\nbirthweight = read.table(\"https://rgaykroyd.github.io/MATH3823/Datasets/birthwt-numeric.txt\", header=T)\n\nweight = birthweight$weight\nage = birthweight$age\nsex = birthweight$sex\n\n\nplot(age, weight, pch=16, \n     xlab = \"Gestational age (weeks)\",\n     ylab = \"Birth weight (grams)\")\nabline(h=mean(weight))\n\n\nplot(age, weight, pch=16,  \n     xlab = \"Gestational age (weeks)\",\n     ylab = \"Birth weight (grams)\")\nM1.fit = lm(weight~age)\nabline(M1.fit$coefficients[1],M1.fit$coefficients[2])\n\n\nplot(age, weight, pch=15-11*sex, col=2-sex, \n     xlab = \"Gestational age (weeks)\",\n     ylab = \"Birth weight (grams)\")\nlegend(41,2800, c(\"Girl\",\"Boy\"), col = c(2,1),pch=c(15,4))\n\nM2.fit = lm(weight~age+sex)\n\nabline(M2.fit$coefficients[1],M2.fit$coefficients[2], col=2)\nabline(M2.fit$coefficients[1]+M2.fit$coefficients[3],M2.fit$coefficients[2], col=1)\n\n\nplot(age, weight, pch=15+sex, col=2-sex, \n     xlab = \"Gestational age (weeks)\",\n     ylab = \"Birth weight (grams)\")\nlegend(41,2800, c(\"Girl\",\"Boy\"), col = c(2,1), pch=c(15,16))\n\nM3.fit = lm(weight~age+sex+age*sex)\n\nabline(M3.fit$coefficients[1],M3.fit$coefficients[2], col=2)\nabline(M3.fit$coefficients[1]+M3.fit$coefficients[3],M3.fit$coefficients[2]+M3.fit$coefficients[4], col=1)\n\n\n\n\n\n\n\n\n(a) Model 0\n\n\n\n\n\n\n\n(b) Model 1\n\n\n\n\n\n\n\n\n\n(c) Model 2\n\n\n\n\n\n\n\n(d) Model 3\n\n\n\n\nFigure 2.3: Birthweight and gestational age data with superimposed fitted regression lines from various competing models.\n\n\n\nWe know from previous modules that statistical tests can be used to check the importance of regression coefficients and model parameters, but it is also important to use the graphical results, as in Figure 2.3, to guide us.\n\\(\\texttt{Model 0}\\) says that there is no change in birth weight with gestational age which means that we would use the average birth weight as the prediction whatever the gestational age – this makes no sense. As we can easily see from the scatter plot of the data, the fitted line in this case is clearly inappropriate.\n\\(\\texttt{Model 1}\\) does not take into account whether the baby is a girl or a boy, but does model the relationship between birth weight and gestational age. This does seem to provide a good fit and might be adequate for many purposes. Recall from Figure 2.1 (b) and Figure 2.2, however, that for a given gestational age the boys seem to have a higher birth weight than the girls.\n\\(\\texttt{Model 2}\\) does take the sex of the baby into account by allowing separate intercepts in the fitted lines – this means that the lines are parallel. By eye, there is a clear difference between these two lines but it might not be important.\n\\(\\texttt{Model 3}\\) allows for separate slopes as well as intercepts. There is a slight difference in the slopes, with the birth weight of the girls gradually catching-up as the gestational age increases. It is difficult to see, however, if this will be a general pattern or if it is only true for this data set – especially given the relatively small sample size.\nHere, it is not clear by eye which of the fitted models will be the best and hence we should use a statistical test to help. In particular, we can choose between the models using F-tests.\nLet \\(y_i\\) denote the value of the dependent variable \\(\\texttt{Weight}\\) for individual \\(i=1,\\dots,n\\), and let the four models be indexed by \\(k=0,1,2,3\\).\nLet \\(R_k\\) denote the residual sum of squares (RSS) for Model \\(k:\\)\n\\[\nR_k = \\sum_{i=1}^n (y_i - \\hat{\\mu}_{ki})^2,\n\\tag{2.1}\\]\nwhere \\(\\hat{\\mu}_{ki}\\) is the fitted value for individual \\(i\\) under \\(\\texttt{Model}\\) \\(k\\). Let \\(r_k\\) denote the corresponding residual degrees of freedom for \\(\\texttt{Model}\\) \\(k\\) (the number of observations minus the number of model parameters).\nConsider the following hypotheses: \\[\nH_0: \\texttt{Model } 0 \\text{ is true}; \\hspace{5mm} H_1: \\texttt{Model } 1 \\text{ is true}.\n\\] Under the null hypothesis \\(H_0\\), the difference between \\(R_0\\) and \\(R_1\\) will be purely random, so the between-models mean-square \\((R_0 - R_1)/(r_0 - r_1)\\) should be comparable to the residual mean-square \\(R_1/r_1\\). Thus our test statistic for comparing \\(\\texttt{Model } 1\\) to the simpler \\(\\texttt{Model } 0\\) is:\n\\[\nF_{01} = \\frac{(R_0 - R_1)/(r_0 - r_1)}{R_1/r_1}.\n\\tag{2.2}\\]\nIt can be shown that, under the null hypothesis \\(H_0\\), the statistic \\(F_{01}\\) will have an \\(F\\)-distribution on \\(r_0 - r_1\\) and \\(r_1\\) degrees of freedom, which we write: \\(F_{r_0-r_1, r_1}\\). Under the alternative hypothesis \\(H_1\\), the difference \\(R_0-R_1\\) will tend to be larger than expected under \\(H_0\\), and so the observed value \\(F_{01}\\) will probably lie in the upper tail of the \\(F_{r_0-r_1, r_1}\\) distribution.\nReturning to the birth weight data, we obtain the following output from R when we fit \\(\\texttt{Model } 1\\):\n\n\nCode\n# read the data from file into a dataframe called ’birthweight’\nbirthweight = read.table(\"https://rgaykroyd.github.io/MATH3823/Datasets/birthwt.txt\", header=T)\n\n# fit Model 1\nfit1 = lm(weight ~ age, data=birthweight)\n\n# print the parameter estimates from Model 1\ncoefficients(fit1)\n\n\n(Intercept)         age \n -1484.9846    115.5283 \n\n\nCode\n# perform an analysis of variance of Model 1\nanova(fit1)\n\n\nAnalysis of Variance Table\n\nResponse: weight\n          Df  Sum Sq Mean Sq F value   Pr(&gt;F)    \nage        1 1013799 1013799   27.33 3.04e-05 ***\nResiduals 22  816074   37094                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThus we have parameter estimates: \\(\\hat\\alpha = -1484.98\\) and \\(\\hat\\beta = 115.5\\). The Analysis of Variance (ANOVA) table gives: \\(R_0-R_1 = 1013799\\) with \\(r_0-r_1 = 1\\) and \\(R_1 = 816074\\) with \\(r_1 = 22\\).\nIf we wanted \\(R_0\\) and \\(r_0\\) then we can either fit \\(\\texttt{Model 0}\\) or get them by subtraction.\nThe \\(F_{01}\\) statistic, Equation 2.2, is then \\[\nF_{01} = \\frac{103799/1}{816074/22} = 27.33,\n\\] which can be read directly from the ANOVA table in the column headed ‘F value’.\nIs \\(F_{01} = 27.33\\) in the upper tail of the \\(F_{1,22}\\) distribution? (See Figure 2.4 and note that 27.33 is very far to the right.) The final column of the ANOVA table tells us that the probability of observing \\(F_{01} &gt; 27.33\\) is only \\(3.04\\times10^5\\) – this is called a p-value. The *** beside this p-value highlights that its value lies between 0 and 0.001. This indicates that we should reject \\(H_0\\) in favour of \\(H_1\\) – there is very strong evidence for the more complicated model. Thus we would conclude that the effect of gestational age is statistically significant in these data.\n\n\nCode\npar(mar=c(4,4,0,1))\n\ncurve(df(x,1,22), 0,30, \n      ylab=expression(\"PDF of \"*\"F\"[0][1]))\n\n\n\n\n\nFigure 2.4: Probability density function of \\(F_{01}\\) distribution.\n\n\n\n\nNext, consider the following hypotheses:\n\\[\nH_0: \\texttt{Model } 1 \\text{ is true}; \\hspace{5mm} H_1: \\texttt{Model } 2 \\text{ is true}.\n\\]\nUnder the null hypothesis \\(H_0\\), the difference between \\(R_1\\) and \\(R_2\\) will be purely random, so the between-models mean-square \\((R_1 - R_2)/(r_1 - r_2)\\) should be comparable to the residual mean-square \\(R_2/r_2\\). Thus our test statistic for comparing \\(\\texttt{Model } 2\\) to the simpler \\(\\texttt{Model } 1\\) is:\n\\[\nF_{12} = \\frac{(R_1 - R_2)/(r_1 - r_2)}{R_2/r_2}.\n\\tag{2.3}\\]\nUnder the null hypothesis \\(H_0\\), the statistic \\(F_{12}\\) will have an \\(F\\)-distribution on \\(r_1 - r_2\\) and \\(r_2\\) degrees of freedom, which we write: \\(F_{r_1-r_2, r_2}\\). Under the alternative hypothesis \\(H_1\\), the difference \\(R_1-R_2\\) will tend to be larger than expected under \\(H_0\\), and so the observed value \\(F_{12}\\) will probably lie in the upper tail of the \\(F_{r_1-r_2, r_2}\\) distribution.\nReturning to the birth weight data, we obtain the following output from R (where \\(\\texttt{sexM}\\) denotes Boy):\n\n\nCode\n# fit Model 2\nfit2 = lm(weight ~ age + sex, data=birthweight)\n# print the parameter estimates from Model 2\ncoefficients(fit2)\n\n\n(Intercept)         age        sexM \n -1773.3218    120.8943    163.0393 \n\n\nCode\n# perform an analysis of variance on the fitted model\nanova(fit2)\n\n\nAnalysis of Variance Table\n\nResponse: weight\n          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nage        1 1013799 1013799 32.3174 1.213e-05 ***\nsex        1  157304  157304  5.0145   0.03609 *  \nResiduals 21  658771   31370                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThus we have parameter estimates: \\(\\hat \\alpha = -1773.3\\), \\(\\hat\\beta = 120.9\\) and \\(\\hat\\gamma = 163.0\\), the latter being the effect of being a boy compared to the baseline category of being a girl.\nThe Analysis of Variance (ANOVA) table gives: \\(R_1-R_2 = 157304\\) with \\(r_1-r_2=1\\), and \\(R_2=658771\\) with \\(r_2=21\\). The \\(F_{12}\\) statistic, Equation 2.3, is then\n\\[\nF_{12} = \\frac{157304/1}{658771/21} = 5.0145,\n\\]\nwhich can be read directly from the ANOVA table in the column headed ‘F value’. Is \\(F_{12} = 5.01\\) in the upper tail of the \\(F_{1,21}\\) distribution?\nThe final column of the ANOVA table tells us that the probability of observing \\(F_{12} &gt; 5.01\\) is only \\(0.03609\\) – this is called a p-value. The * beside this p-value highlights that its value lies between 0.01 and 0.05. This indicates that we should reject \\(H_0\\) in favour of \\(H_1\\) – there is evidence for the more complicated model. Thus we would conclude that the effect of the sex of the baby, after controlling for gestational age, is statistically significant in these data.\nTo complete the analysis, we should now compare \\(\\texttt{Model }2\\) with \\(\\texttt{Model }3\\) – see Exercises."
  },
  {
    "objectID": "2_linearmodels.html#focus-on-regression-quiz",
    "href": "2_linearmodels.html#focus-on-regression-quiz",
    "title": "2  Essentials of Normal Linear Models",
    "section": "Focus on regression quiz",
    "text": "Focus on regression quiz\nTest your knowledge recall and application to reinforce basic ideas and prepare for similar concepts later in the module.\n\n\nFor each situation, choose one of the following statements which you think is most likely to apply.\n\nWhich of the following is a true statement about correlation and linear regression? (Choose any that apply.)\n\n There is no relationship between correlation and linear regression If the correlation is away from zero then a linear regression should always be used If the correlation is close to zero then a linear regression will not be useful The correlation and the regression slope parameter will have the same sign The correlation and the slope parameter will have the opposite sign\n\nWhich of the following is NOT a true statement about model residuals? (Choose any that apply.)\n\n Can help to identify unusual data points Will always sum to zero Can be used to judge how well the model fits the data Should always be plotted as part of a data analysis All should be zero if the model is a good fit\n\nWhich of the following is a true statement about prediction using linear regression? (Choose any that apply.)\n\n Interpolation should only be used if we know the true relationship is linear Prediction is always reliable Interpolation and extrapolation are both types of prediction We should be careful about extrapolating Interpolation is performed using the fitted model\n\nWhich of the following is NOT an important part of regression model fitting? (Choose any that apply.)\n\n A histogram of the data A scatter plot of the residuals The command lm to fit a linear model A scatter plot of the data The command cor to check for a relationship between variables\n\nWhich of the following is NOT important when performing data analysis? (Choose any that apply.)\n\n Is the data reliable and have suitable data checks been performed Do we need authorization from the person collecting the data before we can use it The analysis should be done professionally. We must not involve our personal views to influence our analysis and conclusions Background information is not needed before analyzing a new data set – it's only the numbers that matter Are the data representative of what we are aiming to investigate"
  },
  {
    "objectID": "2_linearmodels.html#sec-typesnormalvariable",
    "href": "2_linearmodels.html#sec-typesnormalvariable",
    "title": "2  Essentials of Normal Linear Models",
    "section": "2.3 Types of normal linear model",
    "text": "2.3 Types of normal linear model\nHere we consider how normal linear models can be set up for different types of explanatory variable. The dependent variable \\(y\\) is modelled as a linear combination of \\(p\\) explanatory variables \\(\\mathbf{x} =(x_1, x_2,\\ldots, x_p)\\) plus a random error \\(\\epsilon \\sim N(0, \\sigma^2)\\), where ‘~’ means ‘is distributed as’. Several models are of this kind, depending on the number and type of explanatory variables. Table 2.2 lists some types of normal linear models with their explanatory variable types.\n\n\nTable 2.2: Types of normal linear model and their explanatory variable types where indicator function \\(I(x=j)=1\\) if \\(x=j\\) and \\(0\\) otherwise. \n\n\n\n\n\n\n\n\\(p\\)\nExplanatory variables\nModel\n\n\n1\nQuantitative\nSimple linear regression \\(y=\\alpha+\\beta x+\\epsilon\\)\n\n\n&gt;1\nQuantitative\nMultiple linear regression\\(y=\\alpha+\\sum_{i=1}^p\\beta_i x_i+\\epsilon\\)\n\n\n1\nDichotomous (\\(x=1\\) or \\(2\\))\nTwo-sample t-test \\(y=\\alpha+\\delta ~ I(x=2)+\\epsilon\\)\n\n\n1\nPolytomous, \\(k\\) levels \\((x=1,\\ldots,k)\\)\nOne-way ANOVA\\(y=\\alpha+\\sum_{j=1}^k \\delta_j \\ I(x=j)+\\epsilon\\)\n\n\n&gt;1\nQualitative\n\\(p\\)-way ANOVA\n\n\n\n\nFor the two-sample t-test model2, observations in the two groups have means \\(\\alpha+\\beta_1\\) and \\(\\alpha + \\beta_2\\) . Notice, however, that we have three parameters with only two group sample means and hence parameter estimation is not possible. To avoid this identification problem, we either impose a ‘corner’ constraint: \\(\\beta_1=0\\) and then \\(\\beta_2\\) represents the difference in the Group 2 mean relative to a baseline of Group \\(1\\). Alternatively, we may impose a ‘sum-to-zero’ constraint: \\(\\beta_1+ \\beta_2 =0\\), the values \\(\\beta_1=-\\beta_2\\) then give differences in the groups means relative to the overall mean. Table 2.3 shows the effect of the parameter constraint on the group means.\n\n\nTable 2.3: Parameters in the two-sample t-test model after imposing parameter constraint to avoid the identification problem.\n\n\nConstraint\nGroup 1 mean\nGroup 2 mean\n\n\n\n\n\\(\\beta_1=0\\)\n\\(\\alpha\\)\n\\(\\alpha+\\beta_2\\)\n\n\n\\(\\beta_1+\\beta_2=0\\)\n\\(\\alpha-\\beta_2\\)\n\\(\\alpha+\\beta_2\\)\n\n\n\n\nFor the general one-way ANOVA model with \\(k\\) groups, observations in Group \\(j\\) have mean \\(\\alpha + \\delta_j\\) , for \\(j =1, \\ldots, k\\) – that leads to \\(k + 1\\) parameters describing \\(k\\) group means. Again we can impose the ‘corner’ constraint: \\(\\delta_1 = 0\\) and then \\(\\delta_j\\) represents the difference in means between Group \\(j\\) and the baseline Group \\(1\\). Alternatively, we may impose a ‘sum-to-zero’ constraint:\\(\\sum_{j=1}^k \\delta_j =0\\) and again \\((\\delta_1, \\delta_2,\\dots,\\delta_k)\\) then represents an individual group effect relative to the overall data mean."
  },
  {
    "objectID": "2_linearmodels.html#matrix-representation-of-linear-models",
    "href": "2_linearmodels.html#matrix-representation-of-linear-models",
    "title": "2  Essentials of Normal Linear Models",
    "section": "2.4 Matrix representation of linear models",
    "text": "2.4 Matrix representation of linear models\nAll of the models in Table Table 2.2 can be fitted by least squares (OLS). To describe this, a matrix formulation will be most convenient:\n\\[\n\\mathbf{Y} = X\\boldsymbol{\\beta}+\\boldsymbol{\\epsilon}\n\\tag{2.4}\\]\nwhere\n\n\\(\\mathbf{Y}\\) is an \\(n\\times 1\\) vector of observed response values with \\(n\\) being the number of observations.\n\\(X\\) is an \\(n\\times p\\) design matrix, to be discussed below.\n\\(\\boldsymbol{\\beta}\\) is a \\(p\\times 1\\) vector of parameters or coefficients to be estimated.\n\\(\\boldsymbol{\\epsilon}\\) is an \\(n\\times 1\\) vector of independent and identically distributed (IID) random variables, which here \\(\\epsilon \\sim N(0, \\sigma^2)\\) and is called the “error” term.\n\nCreating the design matrix is a key part of the modelling as it describes the important structure of investigation or experiment. The design matrix can be constructed by the following process.\n\nBegin with an \\(X\\) containing only one column: a vector of ones for the overall mean or intercept term (the \\(\\alpha\\) in Table 2.2).\nFor each explanatory variable \\(x_j\\), do the following:\n\nIf a variable \\(x_j\\) is quantitative, add a column to \\(X\\) containing the values of \\(x_j.\\)\nIf \\(x_j\\) is qualitative with \\(k\\) levels, add \\(k\\) “dummy” columns to \\(X\\), taking values 0 and 1, where a 1 in the \\(\\ell\\)th dummy column identifies that the corresponding observation is at level \\(\\ell\\) of factor \\(x_j\\) . For example, suppose we have a factor \\(\\mathbf{x}_j = (M, M, F, M, F)\\) representing the sex of \\(n = 5\\) individuals. This information can be coded into two dummy columns of \\(X\\):\n\n\\[\n\\begin{matrix}\n\\begin{matrix}\nF & M\n\\end{matrix}\\\\\n\\begin{bmatrix}\n0 & 1 \\\\\n0 & 1 \\\\\n1 & 0 \\\\\n0 & 1 \\\\\n1 & 0\n\\end{bmatrix}\n\\end{matrix}\n\\]\nWhen qualitative variables are present, \\(X\\) will be singular – that is, there will be linear dependencies between the columns of \\(X\\). For example, the sum of the two columns above is a vector of ones, the same as the intercept column. We resolve this identification problem by deleting some columns of \\(X\\). This is equivalent to applying the corner constraint \\(\\delta_1 = 0\\) in the one-way ANOVA.\nIn the above example, after removing a column, we get:\n\\[\n\\mathbf{X}=\\begin{bmatrix}\n1 &  1 \\\\\n1 &  1 \\\\\n1 &  0 \\\\\n1 &  1 \\\\\n1 &  0\n\\end{bmatrix}.\n\\]\nEach column of \\(X\\) represents either a quantitative variable, or a level of a qualitative variable. We will use \\(i = 1, \\ldots, n\\) to label the observations (rows of \\(X\\)) and \\(j = 1, \\ldots, p\\) to label the columns of \\(X\\).\n\n\n\nExample: Simple linear regression\nConsider the simple linear regression model \\(y=\\alpha+\\beta x+\\epsilon\\) with \\(\\epsilon \\sim N(0, \\sigma^2)\\). Given data on \\(n\\) pairs \\((x_i, y_i), i = 1, \\ldots, n\\), we write this as\n\\[\ny_i = \\alpha+\\beta x_i+\\epsilon_i, \\quad \\text{for } i=1,2,\\dots,n,\n\\tag{2.5}\\]\nwhere the \\(\\epsilon_i\\) are IID \\(N(0,\\sigma^2)\\). In matrix form, this becomes\n\\[\n\\mathbf{Y}=X\\boldsymbol{\\beta}+\\boldsymbol{\\epsilon}\n\\tag{2.6}\\] with \\[\n\\mathbf{Y}=\\begin{bmatrix}\ny_1\\\\\n\\vdots\\\\\ny_n\n\\end{bmatrix},\n%\n\\hspace{5mm}\n%\nX=\\begin{bmatrix}\n1 & x_1\\\\\n\\vdots & \\vdots\\\\\n1 & x_n\n\\end{bmatrix},\n%\n\\hspace{5mm}\n%\n\\boldsymbol{\\beta}=\n\\begin{bmatrix}\n\\beta_1\\\\\n\\beta_2\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\alpha\\\\\n\\beta\n\\end{bmatrix},\n%\n\\hspace{5mm}\n%\n\\boldsymbol{\\epsilon}=\n\\begin{bmatrix}\n\\epsilon_1\\\\\n\\vdots\\\\\n\\epsilon_n\n\\end{bmatrix}.\n\\] The \\(i\\)th row of Equation 2.6 has the same meaning as Equation 2.5: \\[\ny_i = 1\\times \\beta_1 + x_i\\times \\beta_2 +\\epsilon_i = \\alpha+\\beta x_i +\\epsilon_i,  \\hspace{2mm} \\text{for } i=1,2,\\dots,n.\n\\]\n\nExample: One-way ANOVA\nFor one-way ANOVA with \\(k\\) levels, the model is \\[\ny_i =\\alpha+\\sum_{j=1}^k \\delta_j \\ I(x_i=j)+\\epsilon_i, \\quad \\text{for } i=1, 2, \\dots,n,\n\\] where \\(x_i\\) denotes the group level of individual \\(i\\). So if \\(y_i\\) is from the \\(j\\)th group then \\(y_i \\sim N(\\alpha+\\delta_j, \\sigma^2)\\). Here \\(\\alpha\\) is the intercept and the \\((\\delta_1, \\delta_2, \\dots,\\delta_k)\\) represent the “main effects”.\nWe can store the information about the levels of \\(g\\) in a dummy matrix \\(X^* = (x^*_{ij})\\) where\n\\[\nx^*_{ij} = \\left\\{\n\\begin{array}{cl}\n1, & g_i=j,\\\\\n0, & \\text{otherwise.}\n\\end{array}\n\\right.\n\\]\nThen set \\(X = [1, X^*]\\), where \\(1\\) is an \\(n\\)-vector of \\(1\\)’s. For the male–female example at (1.12), we have \\(n = 5\\) and a sex factor:\n\\[\ng=\\begin{bmatrix}1\\\\ 1 \\\\2\\\\1\\\\2\\end{bmatrix},\n%\n\\hspace{5mm}\n%\nX=\\begin{bmatrix}\n1 & 1 & 0\\\\\n1&1& 0 \\\\ 1& 0 & 1\\\\ 1& 1 & 0 \\\\ 1& 0 & 1\n\\end{bmatrix},\n%\n\\hspace{5mm}\n%\n\\beta=\\begin{bmatrix}\\beta_1\\\\\\beta_2\\\\\\beta_3\\end{bmatrix}\n=\\begin{bmatrix}\\alpha\\\\\\delta_1 \\\\\\delta_2\\end{bmatrix}.\n\\]\nThen the \\(i\\)th row of \\(X\\) becomes \\(\\beta_1 + \\beta_2 = \\alpha + \\delta_1\\) if \\(g_i = 1\\) and \\(\\beta_1 + \\beta_3 = \\alpha + \\delta_2\\) if \\(g_i\\) = 2. That is, the \\(i\\)th row of \\(X\\) is\n\\[\n\\alpha+\\sum_{j=1}^2 \\delta_j I(g_i=j)\n\\] so this model can be written \\(Y=X\\beta+\\epsilon\\). Here, \\(X\\) is singular: its last two columns added together equal its first column. Statistically, the problem is that we are trying to estimate two means (the mean response for Boys and the mean response for Girls) with three parameters (\\(\\alpha\\), \\(\\delta_2\\) and \\(\\delta_2\\)).\nIn practice, we often resolve this aliasing or identification problem by setting one of the parameters to be zero, that is \\(\\delta_1 = 0\\), which corresponds to deleting the second column of \\(X\\))."
  },
  {
    "objectID": "2_linearmodels.html#focus-on-matrix-representations-quiz",
    "href": "2_linearmodels.html#focus-on-matrix-representations-quiz",
    "title": "2  Essentials of Normal Linear Models",
    "section": "Focus on matrix representations quiz",
    "text": "Focus on matrix representations quiz\n\n\nFor each situation, choose one of the following statements which you think is most likely to apply.\n\nWhat is the dimension of the design matrix? n by 1X2p by 1n by p\nWhat is the distribution of the error term? normalrandom variablen by 1IDDmean zero\nWhich quantity represents the model parameters? nYXepsilonbeta\nWhich two terms in the model have the same dimensions? Y and XX and betaY and betaY and epsilonbeta and epsilon\nWhich of the following is a potential problem when using qualitative variables? X can be negativeX is full rankX is singularX is not squareX can be zero"
  },
  {
    "objectID": "2_linearmodels.html#sec-shorthand",
    "href": "2_linearmodels.html#sec-shorthand",
    "title": "2  Essentials of Normal Linear Models",
    "section": "2.5 Model shorthand notation",
    "text": "2.5 Model shorthand notation\nIn R, a qualitative (categorical) variable is called a factor, and its categories are called levels. For example, variable \\(\\texttt{Sex}\\) in the birth weight data (above) has levels coded “M” for ‘Boy’ and “F” for ‘Girl’. It may not be obvious to R whether a variable is quantitative or qualitative. For example, a qualitative variable called \\(\\texttt{Grade}\\) might have categories 1, 2 and 3. If was included in a model, R would treat it as quantitative unless we declare it to be a factor, which we can do with the command:\n\\(\\texttt{grade = as.factor(grade)}\\)\nA convenient model-specification notation has been developed from which the design matrix \\(X\\) can be constructed. Below, \\(E, F, \\ldots\\) denote generic quantitative (continuous) or qualitative (categorical) variables. Terms in this notation may take the following forms:\n\n\\(1\\) : a column of 1’s to accommodate an intercept term (the \\(\\alpha\\)’s of Table 2.2 ). This is included in the model by default.\n\\(E\\) : variable \\(E\\) is included in the model. The design matrix includes \\(k_E\\) columns for \\(E\\). If \\(E\\) is quantitative, \\(k_E = 1\\). If E is qualitative, \\(k_E\\) is the number of levels of \\(E\\) minus 1.\n\\(E +F\\) : both \\(E\\) and \\(F\\) are included the model. The design matrix includes \\(k_E +k_F\\) columns accordingly.\n\\(E : F\\) (sometimes \\(E \\cdot F\\)) : the model includes an interaction between \\(E\\) and \\(F\\); each column that would be included for \\(E\\) is multiplied by each column for \\(F\\) in turn. The design matrix includes \\(k_E \\times k_F\\) columns accordingly.\n\\(E * F\\) : shorthand for \\(1 + E + F + E : F\\): useful for crossed models where \\(E\\) and \\(F\\) are different factors. For example, \\(E\\) labels age groups; \\(F\\) labels medical conditions.\n\\(E/F\\) : shorthand for \\(1 + E + E : F\\): useful for nested models where \\(F\\) is a factor whose levels have meaning only within levels of factor \\(E\\). For example, \\(E\\) labels different hospitals; \\(F\\) labels wards within hospitals.\n\\(\\text{poly}(E; \\ell)\\) : shorthand for an orthogonal polynomial, wherein \\(x\\) contains a set of mutually orthogonal columns containing polynomials in \\(E\\) of increasing order, from order \\(1\\) through order \\(\\ell\\).\n\\(-E\\) : shorthand for removing a term from the model; for example \\(E * F -E\\) is short for \\(1 + F + E : F\\).\n\\(I()\\) : shorthand for an arithmetical expression (not to be confused with the indicator function defined above). For example, \\(I(E + F)\\) denotes a new quantitative variable constructed by adding together quantitative variables \\(E\\) and \\(F\\). This would cause an error if either \\(E\\) or \\(F\\) has been declared as a factor. What would happen in this example if we omitted the \\(I(\\cdot)\\) notation?\n\nThe notation uses “~” as shorthand for “is modelled by” or “is regressed on”. For example,\n\nWeight is regressed on age-group and sex with no interaction between them: \\[\n\\texttt{Weight} \\sim \\texttt{Age} + \\texttt{Sex}\n\\] as for the birthweight data in Figure 1.2c.\nWell being is regressed on age-group and income-group, where income is thought to affect wellbeing differentially by age: \\[\n\\texttt{Wellbeing} \\sim \\texttt{Age} * \\texttt{Income}\n\\]\nClass of degree is regressed on school of the university and on degree subject within the school: \\[\n\\texttt{DegreeClass} \\sim \\texttt{School/Subject}\n\\]\nYield of wheat is regressed on seed-variety and annual rainfall: \\[\n\\texttt{Yield} \\sim \\texttt{Variety} + \\texttt{poly}(\\texttt{Rainfall}, 2)\n\\]\nProfit is regressed on amount invested: \\[\n\\texttt{Profit}\\sim \\texttt{Investment}- 1\n\\] (no intercept term, that is a regression through the origin)."
  },
  {
    "objectID": "2_linearmodels.html#focus-on-model-notation-quiz",
    "href": "2_linearmodels.html#focus-on-model-notation-quiz",
    "title": "2  Essentials of Normal Linear Models",
    "section": "Focus on model notation quiz",
    "text": "Focus on model notation quiz\n\n\nFor each situation, choose one of the following statements which you think is most likely to apply.\n\nWhat R command can be used to covert numerical values into a nominal variable? as.factorfactorialas.numericalas.ordinalas.nominal\nWhich of the following defines a model where variable Y is regressed on variables V1 and V2, but without an interaction? Y ~ V1:V2Y ~ V1 + V2Y ~ V1 + V2 + V1:V2V1 + V2 ~ YY ~ V1*V2\nWhich of the following defines a model where variable Y is regressed on variables V1 and V2, including a constant and an interaction? Y ~ 1+V1/V2Y ~ 1 + V1 + V2Y ~ V1*V2Y ~ 1+V1:V2Y ~ V1 + V2 + V1:V2 -1\nWhich of the following defines a model regression Y on the product of V1 and V2? Y ~ V1:V2Y ~ I(V1 + V2)Y ~ poly(V1,V2)Y ~ V1.V2Y ~ I(V1*V2)\nWhich of the following defines a model where variable Y is regressed on a second-order polynomial in V1? Y ~ poly(V1,2)Y ~ 1+V1Y ~ I(V1)Y ~ V1*V1Y ~ V1^2"
  },
  {
    "objectID": "2_linearmodels.html#fitting-linear-models-in-r",
    "href": "2_linearmodels.html#fitting-linear-models-in-r",
    "title": "2  Essentials of Normal Linear Models",
    "section": "2.6 Fitting linear models in R",
    "text": "2.6 Fitting linear models in R\nA commonly used command for fitting a linear model in R is\\[\\texttt{lm(formula)}.\\]\nLet \\(\\texttt{x,y,z,a,b} \\dots\\) represent R vectors, all of the same length \\(n\\) (perhaps read in from a data file using the \\(\\texttt{read.table}\\) and \\(\\texttt{attach}\\) commands).\nIf \\(\\texttt{a,b}\\) are qualitative variables, then they first need to be declared as factors by \\(\\texttt{a = as.factor(a)}\\), etc.\nThe \\(\\texttt{formula}\\) argument of \\(\\texttt{lm}\\) specifies the required model in compact notation, e.g. \\(\\texttt{y} \\sim \\texttt{x+z}\\) or \\(\\texttt{y} \\sim \\texttt{x + z*a}\\) where \\(\\sim, \\texttt{+,*}\\) have the same meaning as in Section 2.5.\nTo extract information about a fitted linear model, it is best to store the result of \\(\\texttt{lm}\\) as a variable and then to use the following functions:\n\nTo fit a linear model and store the result in \\(\\texttt{my.lm}\\) (for example):\n\\(\\texttt{my.lm = lm(y}\\sim\\texttt{x + a*b})\\)\nTo print various pieces of information including deviance residuals, parameter estimates and standard errors, deviances, and (if specified) correlations of parameter estimates:\n\\(\\texttt{summary(my.lm, correlation=T)}\\)\nTo print the anova table of the fitted model:\n\\(\\texttt{anova(my.lm)}\\)\nTo print the residual degrees of freedom of the fitted model:\n\\(\\texttt{df.residual(my.lm)}\\)\nTo print the vector of fitted values under the fitted model:\n\\(\\texttt{fitted.values(my.lm)}\\)\nTo print the residuals from the fitted model: \\(\\texttt{residuals(my.lm)}\\)\n\nTo print the parameter estimates from the fitted model:\n\\(\\texttt{coefficients(my.lm)}\\)\nTo print the design matrix for a specified model formula: \\(\\texttt{model.matrix(y}\\sim\\texttt{a*b)}\\)\n\nThe functions \\(\\texttt{summary}\\), \\(\\texttt{anova}\\), and possibly \\(\\texttt{model.matrix}\\) are the most useful for printing out information about the fitted model. The results of the other functions can be saved as variables for further computation, if desired.\nExample of fitting a linear model in R\nHere is a toy example of R commands for modelling a response in terms of one quantitative explanatory variable.\n\n\nCode\nset.seed(273686) # for reproducibility\n\n# Create some artificial data\nx = seq(1,12)\ny = 2 + 0.2*x + rnorm(12,0,0.1)\n\n# Fit the linear model\nmy.lm = lm(y ~ x)\n\n# Show summary information of the fitted model\nsummary(my.lm)\n\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.18236 -0.04632 -0.02162  0.09472  0.12353 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.949587   0.061636   31.63 2.35e-11 ***\nx           0.201892   0.008375   24.11 3.43e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1001 on 10 degrees of freedom\nMultiple R-squared:  0.9831,    Adjusted R-squared:  0.9814 \nF-statistic: 581.2 on 1 and 10 DF,  p-value: 3.433e-10\n\n\n\n\nCode\n# Adjust plot layout\npar(mar=c(4,4,1,1))\n\n# Plot data and add the fitted regression line\nplot(y~x, pch=16)\nabline(my.lm)\n\n# Calculate the std dev of the residuals\nresid.sd = sd(my.lm$residuals)\n\n# Plot the residuals against the fitted values\nplot(my.lm$fitted.values, my.lm$residuals, pch=16,\n     xlab=\"Fitted values\", ylab=\"Residuals\",\n     ylim=c(-0.3, 0.3))\n\n# Add zero line and lines =/- 2sd\nabline(h=0, lty=2)\nabline(h=2*resid.sd*c(-1, 1), lty=2, col=\"red\")\n\n\n\n\n\n\n\n\n(a) Data and fitted model\n\n\n\n\n\n\n\n(b) Residual plot\n\n\n\n\nFigure 2.5: Illustration of model fitting on a toy example\n\n\n\nA scatter plot of the data indicates that a linear model would be appropriate and the resulting fitted linear regression describes the data well. A residual plot shows that all residuals are within two standard deviations of zero, again supporting that the model fits well."
  },
  {
    "objectID": "2_linearmodels.html#ethics-in-statistics-and-data-science",
    "href": "2_linearmodels.html#ethics-in-statistics-and-data-science",
    "title": "2  Essentials of Normal Linear Models",
    "section": "2.7 Ethics in statistics and data science",
    "text": "2.7 Ethics in statistics and data science\nA brief introduction to ethics and their relevance to statistics and data science.\nIn previous module you have already seen that professional and ethical consideration are important. Whether that be when we choose which graph to present or what action to take when data is missing or how to deal with suspected outliers. It is important for statisticians and data scientists to be aware of the ethical dimensions of their work and to be able to think these through.\nPlease note that the following was produce with the help of Dr Robbie Morgan from the Interdisciplinary Applied Ethics (IDEA) Centre at the University of Leeds.\nRoughly speaking, ethics concerns what we ought to do (should do) and the kind of person that we ought to be. Of course, we’re particularly interested in the kinds of ethical questions and challenges that you will encounter in your work as data scientists, such as:\n\nHow should we collect data in a way that respects participants?\nWhy is privacy important? How should data scientists protect privacy?\nHow can algorithmic bias wrong members of the public? How should data scientists respond to this?\nTo what extent are data scientists responsible for the impact of their work?\nWhen and how should data scientists challenge authority in the workplace?\n\nThe work that you do as data scientists can be hugely beneficial; it develops solutions for pressing real-world problems, enables organisations to carry out important functions more effectively and efficiently, and can improve the well being of the public by enacting innovation and technological advancement. At the same time, work in data science presents risk of significant harm and injustice. Widespread data collection threatens the privacy and safety of data subjects. Facial recognition and other surveillance technologies are the latest site of long-running debates over the values of privacy and freedom versus security. Biased algorithms can inflict injustices and exacerbate entrenched inequality. Automation can cause unemployment and instability, with unpredictable results as it affects ever more areas of our lives. So, it is very important to be clear about the obligations and responsibilities that data scientists have to their employers, colleagues, and, most importantly, to society as a whole.\nPlease keep these issues in mind throughout any data analysis and modelling work, especially if you follow such a career path after graduation."
  },
  {
    "objectID": "2_linearmodels.html#exercises",
    "href": "2_linearmodels.html#exercises",
    "title": "2  Essentials of Normal Linear Models",
    "section": "2.8 Exercises",
    "text": "2.8 Exercises\n\n\n\n\n\n\nImportant\n\n\n\nUnless otherwise stated, data files will be available online at: rgaykroyd.github.io/MATH3823/Datasets/filename.ext, where filename.ext is the stated filename with extension.\n\n\n\n2.1. For each situation, consider the data description, correlation value, and data visualization. Then, identify whether the variables are related and if a linear model would be suitable.\n\n\nClick here to see hints.\n\nFor each description, think about what you expect to see and then confirm this, or otherwise, with the scatter plot. Does a linear relationship seem appropriate? Also, does variation in the scatter plot and correlation value suggest a strong relationship.\n\n\nChildhood obesity is a serious medical condition which can lead to long-term health problems. A mixed-sex secondary school for ages 11-18 measures height and weight of all its new students (mostly aged 11 years old) on the first day of term and calculates their body mass index (BMI) and the average daily calorie intake was recorded. The correlation between calorie intake and BMI was 0.6 with the data shown in the scatter plot.\n\n\n\n\n\n\n\nDomestic UK “smart meters” aim to record cumulative electricity and gas usage once per day. They use a wireless internet connection to transmit information to the energy providers, but they do not save previous values. The central system records readings as they are received and calculates the difference between successive readings. The scatter plot shows the daily electricity and gas consumption of a typical house collected using an automatic smart meter. The correlation between electricity and gas consumption is 0.4.\n\n\n\n\n\n\n\nTo investigate the punctuality of trains at Leeds City Station, a platform attendant records the number of minutes late, or early, that each train arrives at a particular platform relative to the timetable. The journey distance of each train arriving was later determined and recorded. The scatter plot shows the Distance Traveled, in miles, and the Delay, in minutes with a correlation of 0.1.\n\n\n\n\n\n\n\nThe 2022-23 UK Housing Survey, included questions regarding household income and the number of rooms. The data are shown in a scatter plot with a correlation of 0.7 between the variables.\n\n\n\n\n\n\n2.2. An extra model which could have been considered for the Birth weight data example would be one that says that \\(\\texttt{Weight}\\) is different for girls and boys, but does not depend on gestational age. Investigate this model.\n\n\nClick here to see hints.\n\nWrite down the equation corresponding to this model. Then, load the birth weight data into RStudio and fit the model. How are the fitted model parameters related to the overall birth weight mean and the mean birth weights of the girls and boys? Is this a good fit to the data? Is Sex statistically significant?\n\n2.3. For each given situation, consider the description and then investigate the suitability of a linear model.\n\n\nClick here to see hints.\n\nFor each given data set, produce an appropriate graph within RStudio, fit a linear regression model and add the fitted model to the graph. Comment on the quality of fit.\n\n\nContinuing the childhood obesity example, use the data in file schoolstudy.csv to model the relationship between BMI and calorie intake in 11-year old children. \n\nTo study the profitability of several iron ore (hematite) extraction quarries, small samples are taken from lorries arriving at an iron purification site. The lorries are open-topped with most travelling less than 20 Km but one quarry is more than 100 Km away. A chemical analysis provides a percentage of pure iron in the sample. Quarries with iron content less than 30% are not considered economically viable. The data file iron.csv contains measurements of percentage pure iron arriving at an iron purification site recorded over a 50 year period. Model the relationship between iron purity and time.\n\nA study aims to investigate osteoporosis in women before and after menopause. The X-rays of a randomly selected sample of patients taking routine mammograms are analysed. The age of the patient and their menopause status are recorded, along with a measure of bone density calculated from the X-ray. Use the data in the file bmd.csv to model the relationship between age and Tscore, noting that a value of below -2.5 indicates osteoperosis, between -2.5 and -1.0 indicates osteopenia whereas above -1.0 is normal.\n\nA primary school head teaching wishes to investigate the relationship between social skills of children and the ages of their brothers and sisters. The hypothesis is that those with older siblings will be better able to deal with social interaction. The file siblings.csv contains data on the age of the eldest sibling of a class of 6-year old school children along with a social skills score for each child assessed during the school lunch break. Model the relationship between sibling age and social skills.\n\n2.4. In an experiment to investigate Ohm’s Law, \\(V=IR\\) where \\(V\\) is Voltage, I is current and \\(R\\) is resistance of the material, the following data3 were recorded:\n\n\nTable 2.4: Experimental verification of Ohm’s Law\n\n\nVoltage (Volts)\n4\n8\n10\n12\n14\n18\n20\n24\n\n\nCurrent (mAmps)\n11\n24\n30\n36\n40\n53\n58.5\n70\n\n\n\n\nDoes this data support Ohm’s Law? What is the resistance of the material used?\n\n\nClick here to see hints.\n\nThere is no data file prepared for this and so create your own variables, then perform a linear regression. Comment on the quality of fit. Note that Ohm’s Law is a linear function but without intercept and that the resistance is a constant multiplying the current.\n\n2.5 In an investigation4 into the effect of eating on pulse rate, 6 men and 6 women were tested before and after a meal, with the following results:\n\n\nTable 2.5: At rest pulse rate before and after a meal for men and women \n\n\nMen\nbefore\n105\n79\n79\n103\n87\n97\n\n\n\nafter\n109\n87\n86\n109\n100\n101\n\n\nWomen\nbefore\n74\n73\n82\n78\n86\n77\n\n\n\nafter\n82\n80\n90\n90\n93\n81\n\n\n\n\nSuggest a suitable model for this situation and write down the corresponding design matrix. Calculate the parameter estimates using the matrix regression estimation equation.\nPerform an appropriate analysis in R to find out if there is evidence to suggest that the change in pulse rate due to a meal is the same for men and women.\n\n\nClick here to see hints.\n\nBeware! This time we have categorical variables: “before”/“after” and “Men/Women”. To create the design matrix think of writing down the terms needed to produce each data value and don’t forget to remove columns to make the solution identifiable. Also, don’t do the matrix multiplication/inversion by hand but use R to solve the matrix calculation. If the change in pulse rate is different for men and women then the interaction should be significant.\n\n2.6 A laboratory experiment5 was performed into the effect of seasonal floods on the growth of barley seedlings in a incubator, as measured by their height in mm. Three types of barley seed (Goldmarker, Midas, Igri) were used with two watering condition (Normal and Waterlogged). Further, each combination was repeated four times on different shelves in the laboratory incubator (Top, Second, Third and Bottom shelf). The data are available in the file barley.csv\nSuggest a suitable model for this situation. Identify the response and explanatory variables and list the levels for any qualitative variables. Write down the design matrix for each model you consider.\nPerform appropriate analyses to test if each of the following are important: (a) watering condition, (b) type of barley seed, and (c) shelf position.\nIn the analysis, do not include any interactions involving shelf position. If you find a significant interaction between watering condition and type of barley seed, carefully interpret the parameter estimates.\n\n\nClick here to see hints.\n\nBeware! This example has categorical explanatory variables and so don’t forget to use as.factor. After that you need to carefully form the model in the lm command and interpret the ANOVA table.\n\n\n\n\n\n\n\n\nNote\n\n\n\nExercise 2 Solutions can be found here."
  },
  {
    "objectID": "2_linearmodels.html#footnotes",
    "href": "2_linearmodels.html#footnotes",
    "title": "2  Essentials of Normal Linear Models",
    "section": "",
    "text": "Dobson and Barnett, 3rd edition, Table 2.3.↩︎\nNotice that this is a special case of the one-way ANOVA when there are only two-groups.↩︎\nAykroyd, P.J. (1956). Unpublished.↩︎\nSource unknown.↩︎\nSource unknown.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH3823 Generalized Linear Models",
    "section": "",
    "text": "Weekly schedule\nItems will be added here week-by-week and so keep checking when you need up-to-date information on what you should be doing."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "MATH3823 Generalized Linear Models",
    "section": "",
    "text": "Some sections will be left as directed reading, but please note that material in all sections is examinable.↩︎"
  }
]