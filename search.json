[
  {
    "objectID": "6_loglinearmodel.html#overview",
    "href": "6_loglinearmodel.html#overview",
    "title": "6  Loglinear Models",
    "section": "6.1 Overview",
    "text": "6.1 Overview\nIn this chapter, we deal with data sets in which the response variable \\(y\\) is a count and the explanatory variables are all factors – i.e. qualitative variables. Initially we assume \\(y\\) has a Poisson distribution.\nSee Chapter 9 of Dobson and Barnett (2008). See also Agresti (1996) An introduction to categorical data analysis.\nWe assume a generalized linear model with\n\nresponses (counts) having independent Poisson distributions;\na logarithmic link function (hence the name log-linear model).\n\nConsider, for example, the two-way contingency table in Table 6.1}:\n\n\nTable 6.1: A two-way contingency table with \\(k_1\\) rows and \\(k_2\\) columns, where each entry \\(y_{ij}\\) is a count.\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n\\(\\dots\\)\n\\(j\\)\n\\(\\dots\\)\n\\(k_2\\)\nTotal\n\n\n\n\n1\n\\(y_{11}\\)\n\\(y_{12}\\)\n\\(\\dots\\)\n\\(y_{1j}\\)\n\\(\\dots\\)\n\\(y_{1k_2}\\)\n\\(y_{1+}\\)\n\n\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\n\n\\(i\\)\n\\(y_{i1}\\)\n\\(y_{12}\\)\n\\(\\dots\\)\n\\(y_{1j}\\)\n\\(\\dots\\)\n\\(y_{ik_2}\\)\n\\(y_{i+}\\)\n\n\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\n\n\\(k_1\\)\n\\(y_{k_11}\\)\n\\(y_{k_12}\\)\n\\(\\dots\\)\n\\(y_{1j}\\)\n\\(\\dots\\)\n\\(y_{k_2k_2}\\)\n\\(y_{1k_1}\\)\n\n\nTotal\n\\(y_{+1}\\)\n\\(y_{+2}\\)\n\\(\\dots\\)\n\\(y_{1j}\\)\n\\(\\dots\\)\n\\(y_{+k_2}\\)\n\\(y_{++}\\)\n\n\n\n\nIn row \\(i\\) and column \\(j\\) of Table 6.1} we assume \\[\nY_{ij} \\sim \\text{Po}(\\lambda_{ij})\n\\] where \\[\n\\log \\lambda_{ij} = \\mu + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij}.\n\\tag{6.1}\\] Here \\(\\mu\\) is called the main effect; \\(\\alpha_i\\) is a row effect; \\(\\beta_j\\) is a column effect; and \\((\\alpha \\beta)_{ij}\\) denotes an interaction effect parameter. Some or all of these effects must be present in the model, with constraints to ensure that the model is identifiable – this is sometimes refered ot as the identifiability or aliasing problem. Generally, R function automatically sets the first level of each effect to zero to achieve model identification. Thus, for the two-way model, \\[\n\\alpha_1 = 0, \\quad \\beta_1 = 0, \\quad\n(\\alpha \\beta)_{11}=(\\alpha \\beta)_{1j}=(\\alpha \\beta)_{k_1}=0.\n\\]"
  },
  {
    "objectID": "6_loglinearmodel.html#motivating-examples",
    "href": "6_loglinearmodel.html#motivating-examples",
    "title": "6  Loglinear Models",
    "section": "6.2 Motivating examples",
    "text": "6.2 Motivating examples\n\n6.2.1 Malignant melanoma\nThe data in Table 6.2 are from a study of 400 patients with malignant melanoma, a particular form of skin cancer (see Dobson and Barnett, p.172). For each tumour, its type and site were recorded. The data in Table 6.2 comprise the numbers of tumours \\(y\\) in each combination of site and tumour-type. This is a two-way contingency table. We want to know how melanoma frequency depends on site and type.\n\n\nTable 6.2: Melanoma counts by type and site.\n\n\nType\nHead\nTrunk\nExtremities\nTotal\n\n\n\n\nHutchinson’s melanotic freckle\n22\n2\n19\n34\n\n\nSuperficial spreading melanoma\n16\n54\n115\n185\n\n\nNodular\n19\n33\n73\n125\n\n\nIndeterminate\n11\n17\n28\n56\n\n\nTotal\n68\n106\n226\n400\n\n\n\n\n\n\nTable 6.3: Percentages across columns within rows for melanoma data.\n\n\nType\nHead\nTrunk\nExtremities\nTotal\n\n\n\n\nHutchinson’s melanotic freckle\n64.7\n5.9\n29.4\n100\n\n\nSuperficial spreading melanoma\n8.6\n29.2\n62.2\n100\n\n\nNodular\n15.2\n26.4\n58.4\n100\n\n\nIndeterminate\n19.6\n30.4\n50.0\n100\n\n\nTotal\n17.0\n26.5\n56.5\n100\n\n\n\n\n\n\nTable 6.4: Percentages across rows within columns for melanoma data.\n\n\n\n\n\n\n\n\n\nType\nHead\nTrunk\nExtremities\nTotal\n\n\n\n\nHutchinson’s melanotic freckle\n32.4\n1.9\n4.4\n8.5\n\n\nSuperficial spreading melanoma\n23.5\n50.9\n50.9\n46.3\n\n\nNodular\n27.9\n31.1\n32.3\n31.3\n\n\nIndeterminate\n16.2\n16.0\n12.4\n14.0\n\n\nTotal\n100.0\n99.9\n100.0\n100.0\n\n\n\n\nAlthough we have two factors, and , that we may use as predictors, standard ANOVA regression methods are inappropriate here as the dependent variable is not continuous but is instead a count. We will use log-linear regression, a type of generalized linear model, to analyse these data.\nTable 6.3 shows row and Table 6.4 column percentages for these data. For example, 15.2% of nodular melanomas occurred in the head and neck, 26.4% in the trunk, and 58.4% in the extremities. Compare this to the equivalent figures for Hutchinson’s melanotic freckles: 64.7%, 5.9%, and 29.4% - strikingly different. So different types of melanomas are more likely to occur in different locations.\n\n\n6.2.2 Flu vaccine\nThe data in Table 6.5 are from a randomized controlled trial in which 73 patients were randomized into two groups (see Dobson and Barnett, 2008, p.173). The treatment group was given a flu vaccine, while the control group was given a placebo. Levels of an antibody (HIA) were measured after 6 weeks and classified into three groups: Low, Moderate, and High.\n\n\nTable 6.5: Antibody responses to flu vaccine from a randomized controlled trial.\n\n\nGroup\nLow\nModerate\nHigh\n\n\n\n\nPlacebo\n25\n8\n5\n\n\nVaccine\n6\n18\n11\n\n\nTotal\n31\n26\n16\n\n\n\n\nIs the pattern of response the same for each treatment group? The percentages in Table 6.6 suggest not - row percentages indicate lower responses in the placebo group.\n\n\nTable 6.6: Percentages across rows within columns for antibody responses to flu vaccine.\n\n\nGroup\nLow\nModerate\nHigh\nTotal\n\n\n\n\nPlacebo\n65.8\n21.1\n13.2\n100\n\n\nVaccine\n17.1\n51.4\n31.4\n100\n\n\nTotal\n42.4\n35.6\n22.0\n100\n\n\n\n\n\n\nTable 6.7: Percentages across rows within columns for antibody responses to flu vaccine.\n\n\nGroup\nLow\nModerate\nHigh\n\n\n\n\nPlacebo\n80.6\n30.8\n31.2\n\n\nVaccine\n19.4\n69.2\n68.8\n\n\nTotal\n100\n100\n100"
  },
  {
    "objectID": "6_loglinearmodel.html#maximum-likelihood-estimation",
    "href": "6_loglinearmodel.html#maximum-likelihood-estimation",
    "title": "6  Loglinear Models",
    "section": "6.3 Maximum likelihood estimation",
    "text": "6.3 Maximum likelihood estimation\nRecall that for each cell \\(Y_{ij} \\sim \\text{Po}(\\lambda_{ij})\\) so \\(\\mbox{E}[Y_{ij}]= \\lambda_{ij}\\). However, we estimate \\(\\hat{\\lambda}_{ij} = y_{ij}\\) only for the saturated model given by Equation 6.1. In general, for non-saturated models, the estimate \\(\\hat\\lambda_{ij} \\ne y_{ij}\\).\nConsider the independence model for a 2-way table, that is where \\((\\alpha \\beta)_{ij} = 0\\) for all \\(i,j\\). Here, \\[\n\\log \\lambda_{ij} = \\mu +\\alpha_i + \\beta_j \\tag{6.2}\\] and \\(y_{ij}\\) is the observed count for cell \\((i,j)\\), so the likelihood is given by \\[\nL(\\lambda; y)\n=  \\prod_{i,j} \\frac{e^{-\\lambda_{ij}} \\lambda_{ij}^{y_{ij}}}{y_{ij}!}\n\\] and the log-likelihood is \\[\nl(\\lambda; y)\n=  \\sum_{i,j} \\left\\{ y_{ij} \\log \\lambda_{ij} - \\lambda_{ij} -\n    \\log y_{ij}!\\right\\}.\n\\] Next, using Equation 6.2, gives \\[\nl(\\lambda; y) =  \\sum_{i,j} \\left\\{ y_{ij} (\\mu + \\alpha_i + \\beta_j)\n    - \\exp(\\mu + \\alpha_i + \\beta_j)  - \\log y_{ij}! \\right\\}\n\\] which can be re-written as \\[\nl(\\lambda; y)   =  \\mu y_{++} + \\sum_i \\alpha_i y_{i+} + \\sum_j \\beta_j y_{+ j}\n    - e^{\\mu} \\left(\\sum_i e^{\\alpha_i}\\right) \\left(\\sum_j e^{\\beta_j}\n    \\right) - \\sum_{ij} \\log y_{ij}!.\n\\tag{6.3}\\]\nThe maximum likelihood estimates of the model parameters are obtained in the usual way. Differentiating Equation 6.3 with respect to \\(\\mu\\) and setting the result to zero gives, at the MLE, \\[\ny_{++} = e^{\\hat{\\mu}} \\left(\\sum_i e^{\\hat{\\alpha}_i}\\right)\n\\left(\\sum_j e^{\\hat{\\beta}_j}\\right).\n\\]\nDifferentiating Equation 6.3 with respect to \\(\\alpha_i\\) (where \\(i \\ne 1\\) because \\(\\alpha_1=0\\) to avoid an identifiability problem) and setting the result to zero gives, at the MLE, \\[\ny_{i+} = e^{\\widehat{\\mu}}  e^{\\widehat{\\alpha}_i} \\left(\\sum_j e^{\\widehat{\\beta}_j}\\right).  \\notag \\\\\n% = \\sum_j \\widehat{\\lambda_{ij}} = \\widehat{\\lambda}_{i+} ~~~~(i \\not= 1).\n\\] Differentiating Equation 6.3 with respect to \\(\\beta_j\\) (where \\(j \\not= 1\\) because \\(\\beta_1=0\\)) and setting the result to zero gives, at the MLE, \\[\ny_{+j} = e^{\\widehat{\\mu}}  e^{\\widehat{\\beta}_j} \\left(\\sum_i e^{\\widehat{\\alpha}_i}\\right).\n\\] Then \\[\n\\frac{y_{i+}\\ y_{+j}}{y_{++}} = e^{\\widehat{\\mu}+\\widehat{\\alpha}_i+\\widehat{\\beta}_j}  \n= \\widehat{\\lambda}_{ij}.  \n\\tag{6.4}\\] It follows that \\[\n\\widehat{\\lambda}_{i+} = y_{i+} \\quad\n\\widehat{\\lambda}_{+j} = y_{+j} \\quad  \n\\widehat{\\lambda}_{++} = y_{++}.  \n\\]\nThus, the total fitted count in row \\(i\\) is identical to the total observed count in row \\(i\\). Further, the total fitted count in column \\(j\\) is equal to the total observed count in column \\(j\\) and the total fitted count equals the total observed count."
  },
  {
    "objectID": "6_loglinearmodel.html#model-fitting-in-r",
    "href": "6_loglinearmodel.html#model-fitting-in-r",
    "title": "6  Loglinear Models",
    "section": "6.4 Model fitting in R",
    "text": "6.4 Model fitting in R\nConsider an analysis of Melanoma data introduced in Section 6.2.1.\nTo test the independence of \\(\\texttt{Type}\\) and \\(\\texttt{Size}\\), we fit the model \\[\n\\text{Count} \\sim \\text{Type} + \\text{Site}\n\\] assuming Poisson counts and a logarithmic link function, using the commands:\n\n\nCode\nmelanoma = read.table(\"https://rgaykroyd.github.io/MATH3823/Datasets/melanoma.txt\", header=TRUE)\nattach(melanoma)\n\ntype.F = as.factor(type)\nsite.F = as.factor(site)\n\nglm1 = glm(count ~ type.F + site.F, family='poisson')\nsummary(glm1)\n\n\n\nCall:\nglm(formula = count ~ type.F + site.F, family = \"poisson\")\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-3.0453  -1.0741   0.1297   0.5857   5.1354  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   1.7544     0.2040   8.600  < 2e-16 ***\ntype.F2       1.6940     0.1866   9.079  < 2e-16 ***\ntype.F3       1.3020     0.1934   6.731 1.68e-11 ***\ntype.F4       0.4990     0.2174   2.295  0.02173 *  \nsite.F2       0.4439     0.1554   2.857  0.00427 ** \nsite.F3       1.2010     0.1383   8.683  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 295.203  on 11  degrees of freedom\nResidual deviance:  51.795  on  6  degrees of freedom\nAIC: 122.91\n\nNumber of Fisher Scoring iterations: 5\n\n\nWe see that the residual deviance for this model is \\(51.795\\) on \\(6\\) degrees of freedom. If the model is true, the residual deviance will have an approximate \\(\\chi^2\\) distribution on \\(6\\) degrees of freedom. If the model is not true, the residual deviance will probably be too large to correspond to this distribution. Thus we calculate the \\(p\\)-value in the upper tail of the \\(\\chi^2_6\\) distribution, which can be computed using the command:\n\n\nCode\npchisq(51.795,6,lower.tail=F)\n\n\n[1] 2.050465e-09\n\n\nThis strongly indicates that the independence model is inadequate. Therefore, unless we can spot alternative simplifications, we will have to use the saturated model.\nWe can look at residuals from the model see where the departures from independence occur. The largest residual is for Hutchinson’s freckle on the head and neck, which occurs more often than would be expected under independence.\nFor the saturated model, \\(\\widehat{\\lambda}_{ij} = y_{ij}\\). In this example, \\(\\sum_{ij} \\widehat{\\lambda}_{ij} = \\sum_{ij} y_{ij} = 400\\), so the probability of a tumour being in category \\((i,j)\\) is \\(y_{ij}/400\\) — just the observed proportions.\nNote that in Table 6.2 we have a total of \\(y_{++} = 400\\) observations. If the data were truly Poisson, this would be a suspiciously round number. In reality, this total was fixed by design, so we should take into account the fact that \\(y_{++} = 400\\) and fit a more suitable model, such as the multinomial model which we will meet in the next chapter."
  },
  {
    "objectID": "6_loglinearmodel.html#multi-way-contingency-tables",
    "href": "6_loglinearmodel.html#multi-way-contingency-tables",
    "title": "6  Loglinear Models",
    "section": "6.5 Multi-way contingency tables",
    "text": "6.5 Multi-way contingency tables\nWe can generalize the model notation introduced in Equation 6.1 to accommodate multi-way contingency tables; that is tables of counts indexed by multiple factors. For example, for a 3-way table of cell counts \\(y_{ijk}\\), the saturated model would be written: \\[\nY_{ijk} \\sim \\text{Po}(\\lambda_{ijk})\n\\] where \\[\n\\log \\lambda_{ijk} =\n\\mu + \\alpha_i + \\beta_j +\\gamma_k\n+ (\\alpha \\beta)_{ij}\n+ (\\alpha\\gamma)_{ik}\n+ (\\beta \\gamma)_{jk}\n+ (\\alpha \\beta \\gamma)_{ijk}.\n\\] Here, \\(\\alpha_i\\), \\(\\beta_j\\) and \\(\\gamma_j\\) are main effects; \\((\\alpha \\beta)_{ij}\\), \\((\\alpha\\gamma)_{ik}\\), and \\((\\beta \\gamma)_{jk}\\) are two-way interaction effects; and \\((\\alpha \\beta \\gamma)_{ijk}\\) is a three-way interaction.\nThis approach is easily extended to tables of any number of factors. Note, however, that not all terms need be present in a model, thought 3-way or higher-order interactions might sometimes be required.\nA hierarchical model is one in which each term in the model is accompanied by all lower-order terms. For example, including the term \\((\\alpha\\beta\\gamma)_{ijk}\\) would require inclusion of each of the terms \\(\\alpha_i, \\beta_j, \\gamma_k, (\\alpha\\beta)_{ij}, (\\alpha\\gamma)_{ik}, (\\beta\\gamma)_{jk}\\). We will be concerned only with hierarchical contingency table models. Non-hierarchical models are sometimes appropriate, depending on the nature of the factors involved, but hierarchical models are more generally applicable and easier to interpret."
  },
  {
    "objectID": "6_loglinearmodel.html#exercises",
    "href": "6_loglinearmodel.html#exercises",
    "title": "6  Loglinear Models",
    "section": "6.6 Exercises",
    "text": "6.6 Exercises\nExercises to be added later."
  },
  {
    "objectID": "5_logisticmodel.html#introduction",
    "href": "5_logisticmodel.html#introduction",
    "title": "5  Modelling Proportions",
    "section": "5.1 Introduction",
    "text": "5.1 Introduction\nIn this chapter we will focus on applications of generalized linear modelling where the response variable follows a binomial distribution. This can arise when the outcome is binary, that is it can take one of only two possible values, or is the sum of a set of such binary outcomes. These two outcomes record whether some event of interest has occurred or not. In the simplest case, the response variable, \\(B\\) say, is defined as \\[\nB =\n\\begin{cases}\n1 & \\text{if the event has occurred} \\\\\n0 & \\text{if the event has not occurred}\n\\end{cases}\n\\tag{5.1}\\] and we set \\(Pr(B=1)=p\\), and hence \\(Pr(B=0)=1-p\\). This is, of course, the definition of a Bernoulli trial leading to a Bernoulli random variables, \\(B\\sim \\text{Bernoulli}(p)\\).\nSuppose now that there are \\(m\\) similar binary outcomes, \\(B_i, i=1,\\dots,m\\) with \\(Pr(B_i=1)=p\\), and that the total number of times that the event occurred is recorded as the response \\(Y\\). Assuming that \\(m\\) is known before the trials start, that \\(p\\) is fixed and that the individual Bernoulli trials are independent then \\(Y\\) follows a binomial distribution, \\(Y\\sim \\text{B}(m, p)\\) with probability mass function \\[\nf(y) = {m \\choose y} p^y(1-p)^{m-y}.\n\\tag{5.2}\\] Note that the binomial random variable can be thought of as the sum of i.i.d Bernoulli random variables, \\(Y= B_1+\\cdots + B_m\\), if that is helpful.\nThe Binomial distribution \\(\\text{B}(m, p)\\) is often described in terms of success and failure and a binomial distribution in terms of the number of successes in \\(m\\) independent trials, where \\(p\\) is the probability of success in each trial. The term success need not correspond to a favourable outcome; it is merely the language traditionally used in connection with this model. For example, success might correspond to death.\nOf course, the special case with \\(m=1\\) reduces to the Bernoulli distribution. Further, if \\(Y_1 \\sim \\text{B}(m_1,p)\\) and independently \\(Y_2 \\sim \\text{B}(m_2,p)\\), then \\(Y_1+Y_2\\) also follows a binomial distribution, \\(\\text{B}(m_1+m_2,p)\\) – note that is only valid when \\(p\\) is common.\nRecall that the binomial can be re-written in the exponential family form of Equation 3.3 with \\[\nf(y)  = \\exp \\left\\{ y\\ \\mbox{logit } p  + m \\log (1-p) + \\log {m \\choose y}\\right\\},\n\\] with natural or canonical parameter \\(\\theta=\\text{logit} \\ p = \\log \\left( p(1-p)\\right),\\), scale parameter \\(\\phi=1\\), \\(b(\\theta)=m\\log(1+e^\\theta)\\) and \\(c(y,\\phi)=\\log{m\\choose y}\\) as in Table 3.3."
  },
  {
    "objectID": "5_logisticmodel.html#sec-linearlogistic",
    "href": "5_logisticmodel.html#sec-linearlogistic",
    "title": "5  Modelling Proportions",
    "section": "5.2 The linear logistic model",
    "text": "5.2 The linear logistic model\nThroughout this module we have assumed that a response variable, \\(Y\\), depends on a set of explanatory variables, \\(\\mathbf{x} = \\{x_1,\\dots, x_p\\}\\). In particular, for binomial counts from \\(\\text{B}(m,p)\\), \\(0<p<1\\) with mean \\(\\mu=mp\\) we set the link function, \\(g(\\mu)\\), equal to the linear predictor \\(\\eta = \\sum \\beta_j x_j\\) and hence have \\[\ng(\\mu) = \\sum_{j=1}^p \\beta_j x_j = \\mathbf{x}^T \\boldsymbol{\\beta}\n\\] where \\(\\boldsymbol{\\beta} = \\{\\beta_1,\\dots,\\beta_p\\}\\) are the linear predictor parameters. Recall that the canonical logit link Equation 3.14 for the binomial leads to the systematic part of the model \\[\n\\text{logit}(p) = \\log \\left( \\frac{p}{1-p}\\right) = \\mathbf{x}^T \\boldsymbol{\\beta}\n\\tag{5.3}\\] but that other links function are possible, such as the probit, Equation 3.15, the complementary log-log, Equation 3.16, and the cauchit Equation 3.17.\nThis model can alternatively be written \\[\nY\\ \\sim \\text{B}(m,p), \\quad \\text{where }\np=\\frac{\\exp \\{\\mathbf{x}^T \\boldsymbol{\\beta}\\}}{1+\\exp\\{ \\mathbf{x}^T \\boldsymbol{\\beta}\\}}\n\\tag{5.4}\\] which makes the dependence of \\(Y\\) on \\(\\mathbf{x}\\), and \\(\\boldsymbol{\\beta}\\), more explicit.\nIn general, we might want to consider \\(n\\) independent binomial random variables representing subgroups of the sample with \\(Y_1\\sim \\text{B}(m_1,p_1), \\dots Y_n\\sim \\text{B}(m_n,p_n)\\), see Table 5.1 and hence \\[\n\\mathbf{Y}\\ \\sim \\text{B}(\\mathbf{m},\\mathbf{p}), \\quad \\text{and }\n\\mathbf{p}=\\frac{\\exp \\{X \\boldsymbol{\\beta}\\}}{1+\\exp\\{ X \\boldsymbol{\\beta}\\}},\n\\] with response \\(\\mathbf{Y}=\\{Y_1,\\dots, Y_n\\}\\), \\(\\mathbf{m}=\\{m_1,\\dots, m_n\\}\\), \\(\\mathbf{p}=\\{p_1,\\dots, p_n\\}\\), \\(X\\) being the \\(n \\times p\\) design matrix and \\(\\boldsymbol{\\beta} = \\{\\beta_1,\\dots,\\beta_p\\}\\) the linear predictor parameters.\n\n\nTable 5.1: Linear logistic model\n\n\n\nGroup 1\nGroup 2\n\\(\\dots\\)\nGroup \\(n\\)\n\n\n\n\nSuccesses\n\\(Y_1\\)\n\\(Y_2\\)\n\\(\\dots\\)\n\\(Y_n\\)\n\n\nFailures\n\\(m_1-Y_1\\)\n\\(m_2-Y_2\\)\n\\(\\dots\\)\n\\(m_n-Y_n\\)\n\n\nTotal\n\\(m_1\\)\n\\(m_2\\)\n\\(\\dots\\)\n\\(m_n\\)\n\n\n\n\nThen we can write down the log likelihood of \\(\\boldsymbol{\\beta}\\) using Equation 4.12 and Equation 5.2 as \\[\nl(\\boldsymbol{\\beta}; \\mathbf{y})\n=\n\\sum_{i=1}^n\n\\left\\{\ny_i \\, \\log (p_i) + (m_i-y_i) \\log (1-p_i) + \\log {m_i \\choose y_i}\n\\right\\}\n\\tag{5.5}\\] where \\[\np_i=\\frac{\\exp \\{\\mathbf{x}_i^T \\boldsymbol{\\beta}\\}}{1+\\exp\\{ \\mathbf{x}_i^T \\boldsymbol{\\beta}\\}}.\n\\] We would then use the principle of maximum likelihood to estimate \\(\\boldsymbol{\\beta}\\) \\[\n\\hat{\\boldsymbol{\\beta}} = \\text{arg} \\max_{\\boldsymbol{\\beta}} \\,\nl(\\boldsymbol{\\beta}; \\mathbf{y}).\n\\] Then, the fitted probability are given by \\[\n\\hat{p}_i=\\frac{\\exp \\{\\mathbf{x}_i^T \\hat{\\boldsymbol{\\beta}}\\}}{1+\\exp\\{ \\mathbf{x}_i^T \\hat{\\boldsymbol{\\beta}}\\}}\n\\] and corresponding fitted values \\[\n\\hat y_i = m_i \\, \\hat{p}_i.\n\\]\nThe Pearson residuals for binomial data take the form\n\\[\ne^P_i  =\n\\frac{y_i - m_i\\hat{p}_i}{\\sqrt{m_i \\hat{p}_i (1 - \\hat{p}_i)}} .\n\\] From the general result \\(\\text{Var}(Y_i)=\\phi b''(\\theta)\\) in Proposition 3.1, it can be shown that\\(\\text{Var}(Y_i)=m_i {p}_i (1 - {p}_i)\\). Then, using the estimate of \\(p_i\\) leads to the denominator above.\nFor large \\(m_i\\), the usual Normal approximation to the Binomial means that the Pearson residuals are approximately \\(N(0, 1)\\) distributed.\nIt can be shown that the deviance for the fitted model is \\[\nD =\n2 \\sum_{i=1}^n \\left\\{ y_i \\log \\left( \\frac{y_i} {m_i \\hat{p}_i} \\right) +\n(m_i - y_i) \\log \\left( \\frac{m_i - y_i}{m_i (1 - \\hat{p}_i)} \\right)\n\\right\\},\n\\tag{5.6}\\] which is approximately \\(\\chi^2_{n-r}\\) distributed if the model is correct, where \\(r\\) is the number of degrees of freedom in the model (i.e. the number of columns of the design matrix).\nThis formula can be shown to be equivalent to \\[\nD = 2 \\sum_{j=1}^2 \\sum_{i=1}^n O_{ji} \\log\n\\frac{O_{ji}}{E_{ji}}\n\\] where \\(O_{ji}\\) denotes the observed value and \\(E_{ji}\\) denotes the expected value in cell \\((j,i)\\) of the \\(2 \\times n\\) table of successes and failures:\n\nObserved and expected frequencies in the linear logistic model\n\n\n\nGroup 1\nGroup 2\n\\(\\dots\\)\nGroup \\(n\\)\n\n\n\n\nSuccesses, \\(j=1\\)\n\\(O_{11}\\)\n\\(O_{12}\\)\n\\(\\dots\\)\n\\(O_{1n}\\)\n\n\n\n\\(E_{11}\\)\n\\(E_{12}\\)\n\\(\\dots\\)\n\\(E_{1n}\\)\n\n\nFailures, \\(j=2\\)\n\\(O_{21}\\)\n\\(O_{22}\\)\n\\(\\dots\\)\n\\(O_{2n}\\)\n\n\n\n\\(E_{21}\\)\n\\(E_{22}\\)\n\\(\\dots\\)\n\\(E_{2n}\\)\n\n\n\nAnother goodness-of-fit statistic is the Pearson chi-squared statistic: \\[\nX^2 = \\sum_{j=1}^2 \\sum_{i=1}^n \\frac{(O_{ji} - E_{ji})^2}{E_{ji}}.\n\\] This is asymptotically equivalent to the deviance Equation 5.6 (proof is by Taylor series expansion; omitted). Thus, asymptotically, \\(X^2\\) is also approximately \\(\\chi^2_{n-r}\\) distributed. Both approximations can be poor if the expected frequencies are small, but \\(X^2\\) copes slightly better with this problem. See Dobson, p.136 for more details."
  },
  {
    "objectID": "5_logisticmodel.html#overdispersion",
    "href": "5_logisticmodel.html#overdispersion",
    "title": "5  Modelling Proportions",
    "section": "5.3 Overdispersion",
    "text": "5.3 Overdispersion\nExamination of residuals and deviances may indicate that a model is not an adequate fit to the data. One possible reason is overdispersion. This can occur for any error distribution where the variance is linked to the mean — e.g. Binomial, Poisson. In the binomial case, overdispersion is called extra-Binomial variation.\nRecall that if \\(Y_i \\sim \\text{Bin}(m_i, p_i)\\), \\(\\text{Var}(Y_i) = m_i p_i (1 - p_i)\\). Overdispersion occurs if observations which have been modelled by a \\(\\text{Bin}(m_i, \\hat{p}_i)\\) distribution have substantially greater variation than \\(m_i \\hat{p}_i (1 - \\hat{p}_i)\\). This will lead to a value of \\(D\\) substantially greater than the expected value of \\(n - r\\). This can occur if the model is missing appropriate explanatory variables or has the wrong link function, or if the \\(Y_i\\) are not independent.\nOne solution is to include an extra parameter \\(\\tau\\) in the model so that \\(\\text{Var}(Y_i) = \\tau \\times m_i p_i (1 - p_i)\\). For more details, see Section 7.7 of Dobson or Chapter 6 of Collett (1991) Modelling Binary data, Chapman & Hall.\nThe \\(\\texttt{glm}\\) function in R allows for extra-Binomial variation by setting \\(\\texttt{family=quasibinomial()}\\) with the usual link functions available."
  },
  {
    "objectID": "5_logisticmodel.html#application-to-doseresponse-experiments",
    "href": "5_logisticmodel.html#application-to-doseresponse-experiments",
    "title": "5  Modelling Proportions",
    "section": "5.4 Application to dose–response experiments",
    "text": "5.4 Application to dose–response experiments\nA variable dose of some reagent is administered to each study subject, and the occurrence of a specific response is recorded. This is a dose–response experiment, one of the first uses of regression models for Bernoulli (or Binomial) responses.\nFor example, the Table 5.2 (including the information from Table 1.1) gives the number of beetles killed \\(y_i\\) and the number not killed \\((m_i-y_i)\\) out of a total number \\(m_i\\) that were exposed to a dose \\(x_i\\) of gaseous carbon disulphide, for \\(n=8\\) dose levels \\(i=1,\\dots,8\\) (Dobson: pp.109 in 1st edn; pp.119 in 2nd edn; pp.127 in 3rd edn). The proportion killed \\(p_i = y_i/m_i\\) at each dose level \\(i\\) is also shown in Table 5.2 and plotted in Figure 5.1.\n\n\nTable 5.2: Numbers of beetles killed by five hours of exposure to 8 different concentrations of gaseous carbon disulphide\n\n\n\n\n\n\n\n\n\nDose\n\\(x_i\\)\nNo. of beetle\n\\(m_i\\)\nNo. killed\n\\(y_i\\)\nNo. not killed\n\\(m_i-y_i\\)\nProportion\n\\(p_i=y_i/m_i\\)\n\n\n\n\n1.6907\n59\n6\n53\n0.10\n\n\n\n\n1.7242\n60\n13\n47\n0.22\n\n\n\n\n1.7552\n62\n18\n44\n0.29\n\n\n\n\n1.7842\n56\n28\n28\n0.50\n\n\n\n\n1.8113\n63\n52\n11\n0.83\n\n\n\n\n1.8369\n59\n53\n6\n0.90\n\n\n\n\n1.8610\n62\n61\n1\n0.98\n\n\n\n\n1.8839\n60\n60\n0\n1.00\n\n\n\n\n\n\n\n\nCode\nbeetle = read.table(\"https://rgaykroyd.github.io/MATH3823/Datasets/beetle.txt\", header=T)\n\ndose = beetle$dose\nmortality = beetle$died/beetle$total\n\nplot(dose, mortality, pch=16,\n     xlim=c(1.65, 1.90), xlab =\"Dose\",\n     ylim=c(-0.1, 1.1),  ylab=\"Mortality\")\nabline(h=c(0,1), lty=2)\n\ny = cbind(beetle$died, beetle$total-beetle$died)\nglm.fit = glm(y ~ dose, family=binomial(link='logit'))\n\noutput.dose = seq(1.6,1.95,0.001)\nfitted = predict(glm.fit, data.frame(dose=output.dose), type=\"response\")\nlines(output.dose, fitted)\n\n\n\n\n\nFigure 5.1: Beetle mortality rates with fitted dose-response curves.\n\n\n\n\nNow Equation 5.4 motivates modelling the beetle data as \\[\nY_i \\sim \\text{B}(m_i,p_i), \\quad \\mbox{for } i=1,\\dots, n=8\n\\] where \\[\n\\eta_i = \\alpha + \\beta x_i\n\\] and so \\[\np_i=\\frac{\\exp\\{\\alpha+\\beta x_i\\}}{(1+\\exp\\{\\alpha+\\beta x_i\\})}.\n\\]\nTo fit this model in R, a matrix with columns containing the numbers killed \\(y_i\\) and the numbers not killed \\(m_i-y_i\\) is first calculated\n\n\nCode\ny = cbind(beetle$died, beetle$total-beetle$died)\n\nhead(y)\n\n\n     [,1] [,2]\n[1,]    6   53\n[2,]   13   47\n[3,]   18   44\n[4,]   28   28\n[5,]   52   11\n[6,]   53    6\n\n\nwhich is then used in the \\(\\texttt{glm}\\) command\n\n\nCode\nglm.fit = glm(y ~ dose, family=binomial)\n\n\nThe model parameter estimates are given by\n\n\nCode\ncoefficients(glm.fit)\n\n\n(Intercept)        dose \n  -60.71745    34.27033 \n\n\nand hence\n\n\n\n\\[\n\\hat p_i = \\frac{\\exp\\{-60.7 + 34.3 x_i\\}}{(1+\\exp\\{ -60.7 + 34.3x_i\\})}.\n\\]\nFurther, the deviance if given by\n\n\nCode\ndeviance(glm.fit)\n\n\n[1] 11.23223\n\n\nor more helpfully a full summary, which contains parameter estimates and deviance values using\n\n\nCode\nsummary(glm.fit)\n\n\n\nCall:\nglm(formula = y ~ dose, family = binomial)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.5941  -0.3944   0.8329   1.2592   1.5940  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -60.717      5.181  -11.72   <2e-16 ***\ndose          34.270      2.912   11.77   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 284.202  on 7  degrees of freedom\nResidual deviance:  11.232  on 6  degrees of freedom\nAIC: 41.43\n\nNumber of Fisher Scoring iterations: 4\n\n\nor the anova table\n\n\nCode\nanova(glm.fit)\n\n\nAnalysis of Deviance Table\n\nModel: binomial, link: logit\n\nResponse: y\n\nTerms added sequentially (first to last)\n\n     Df Deviance Resid. Df Resid. Dev\nNULL                     7    284.202\ndose  1   272.97         6     11.232\n\n\nClearly, many of the results are repeated in the various R output.\nWhen considering testing model goodness of fit, it is important to distinguish between deviance values which compare a given model with the saturated model and deviance values which compare two competing models. For example, in the Analysis of Deviance Table, the right hand values compare the Null model with the saturated and the model including dose with the saturated. Whereas the left hand value is relevant to the comparison of Null model with the mode including dose.\nSummary of results:\nFrom the output, we can first test if the Null model, which contains only a constant term, is a good fit to the data using the hypotheses: \\[\nH_0: \\mbox{Null model is true} \\mbox{ against } H_1: \\mbox{Null model is false}.\n\\] From the output, note that the Null deviance is \\(D_0=284.202\\) and that this model has \\(r_0=1\\) parameters. Therefore, \\(D_0\\) follows a \\(\\chi^2\\) distribution with \\(n-r_0=8-1=7\\) degrees of freedom and hence the p-value is:\n\n\nCode\npchisq(284.202, 7, lower.tail = FALSE)\n\n\n[1] 1.425247e-57\n\n\nThis means that we reject \\(H_0\\) as the observed value of \\(D\\) is in the upper tail of the \\(\\chi^2\\) distribution.\nIf we had obtained a non-significant result, then we would stop the analysis and conclude that the Null model is an adequate fit.\nNext, consider testing the model which includes the explanatory variable where the the hypotheses are \\[\nH_0: \\mbox{Null model is true} \\mbox{ against } H_1: \\mbox{Model with dose is true}.\n\\] From the output, the deviance of the proposed model is \\(D_1=11.232\\) with \\(r_1=2\\) parameters. The test statistics is then \\(D_0-D_1=284.202-11.232=272.97\\) which follows a \\(\\chi^2\\) distribution with \\(r_1-r_0=2-1=1\\) degrees of freedom. The p-value is then\n\n\nCode\npchisq(272.97, 1, lower.tail = FALSE)\n\n\n[1] 2.556369e-61\n\n\nand we reject \\(H_0\\) in favour of \\(H_1\\) and conclude that dose is important.\nWe can finish the testing by checking if this model is a good fit to the data. That is, \\[\nH_0: \\mbox{The model with dose is true} \\mbox{ against } H_1: \\mbox{The model with dose is false}\n\\] The deviance of this is \\(D_1=11.232\\) with \\(r_1=2\\) parameters which follows a \\(\\chi^2\\) distribution with \\(n-r_1=8-2=6\\) degrees of freedom and has p=value\n\n\nCode\npchisq(11.232, 6, lower.tail = FALSE)\n\n\n[1] 0.08146544\n\n\nwhich means that we accept \\(H_0\\) at the 5% level and conclude that this model is an adequate fit to the data.\nFinally, we can look at the residuals:\n\n\nCode\nfit.resids = residuals(glm.fit, type=\"deviance\")\n\nplot(dose, fit.resids, pch=16)\nabline(h=0, lty=2)\n\n\n\n\n\nThese show a very slight u-shaped pattern and hence it would be work exploring fitting using the other link functions. For this data set, the complementary log-log (cloglog) link has a slightly better fit, see Figure 3.4, but a slight pattern remains.\nBefore finishing, suppose that we wish to predict, by hand, the probability for dose level \\(x_0=1.85\\), say, and the expected number of beetles killed when \\(m_0=60\\) beetles, say, are exposed.\nThe probability is predicted as \\[\n\\hat p_i = \\frac{\\exp\\{-60.7 + 34.3 x_0\\}}{(1+\\exp\\{ -60.7 + 34.3x_0\\})} = 0.936\n\\] and the expected number of beetles \\[\nm_0 \\, \\hat p_i =  56.2\n\\] Finally, suppose that we wish to find the minimum dose which kills 95% of the beetles, that is \\(p_0=0.95\\) which requires us to invert the logistic equation.\nIn general, suppose we wish to find \\(x_p\\) which corresponds to proportion \\(p\\) then \\[\nx_p= \\frac{1}{\\beta} \\left\\{ \\log\\left(\\frac{p}{1-p}\\right)-\\alpha \\right\\}\n\\] and hence here \\(\\hat x_p= 1.856\\)."
  },
  {
    "objectID": "5_logisticmodel.html#exercises",
    "href": "5_logisticmodel.html#exercises",
    "title": "5  Modelling Proportions",
    "section": "5.5 Exercises",
    "text": "5.5 Exercises\nTo be added later."
  },
  {
    "objectID": "7_extendedloglinearmodel.html#overview",
    "href": "7_extendedloglinearmodel.html#overview",
    "title": "7  Extensions to Loglinear models",
    "section": "7.1 Overview",
    "text": "7.1 Overview\nIn the melanoma example, see Table 6.2, the overall total number of observations was exactly \\(400\\). If the data were truly Poisson, this would be a suspiciously round number. In reality, this total was fixed by design, so we should take into account the fact that \\(y_{++} = 400\\) and fit a more appropriate model. Also, recall the flu vaccine example of Table 6.5. Suppose that some of the marginal totals are fixed by design. For example, that the number of patients in each arm of the trial (38 in the Placebo group, 35 in the Vaccine group) was fixed before data collection started. In this case, we should also take into account the fact that \\(y_{1+}=38\\) and \\(y_{2+}=35\\) and fit a more suitable model.\nBefore we can re-consider these data sets, we first need to establish some theoretical results to deal with such conditional distribution cases."
  },
  {
    "objectID": "7_extendedloglinearmodel.html#contingency-tables-with-fixed-marginals",
    "href": "7_extendedloglinearmodel.html#contingency-tables-with-fixed-marginals",
    "title": "7  Extensions to Loglinear models",
    "section": "7.2 Contingency tables with fixed marginals",
    "text": "7.2 Contingency tables with fixed marginals\nConsider, again, a general two-way contingency table such as that in Table 7.1. The fixed marginals are the column sums: \\(y_{+j},\\) the row sums: \\(y_{i+},\\) and the overall sum: \\(y_{++}.\\)\n\n\nTable 7.1: A two-way contingency table with \\(k_1\\) rows and \\(k_2\\) columns, where each entry \\(y_{ij}\\) is a count.\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n\\(\\dots\\)\n\\(j\\)\n\\(\\dots\\)\n\\(k_2\\)\nTotal\n\n\n\n\n1\n\\(y_{11}\\)\n\\(y_{12}\\)\n\\(\\dots\\)\n\\(y_{1j}\\)\n\\(\\dots\\)\n\\(y_{1k_2}\\)\n\\(y_{1+}\\)\n\n\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\n\n\\(i\\)\n\\(y_{i1}\\)\n\\(y_{12}\\)\n\\(\\dots\\)\n\\(y_{1j}\\)\n\\(\\dots\\)\n\\(y_{ik_2}\\)\n\\(y_{i+}\\)\n\n\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\n\n\\(k_1\\)\n\\(y_{k_11}\\)\n\\(y_{k_12}\\)\n\\(\\dots\\)\n\\(y_{k_1j}\\)\n\\(\\dots\\)\n\\(y_{k_1k_2}\\)\n\\(y_{k_1+}\\)\n\n\nTotal\n\\(y_{+1}\\)\n\\(y_{+2}\\)\n\\(\\dots\\)\n\\(y_{+j}\\)\n\\(\\dots\\)\n\\(y_{+k_2}\\)\n\\(y_{++}\\)\n\n\n\n\nSuppose that we have fixed row totals \\(y_{i+},\\) or a fixed overall total \\(y_{++}\\), how would such constraints affect the distribution of the random variables \\(Y_{ij}\\)?\nTo answer this question, we will need the following results.\n\nProposition 7.1 Let \\(Y_{ij}\\), for \\(i=1,\\dots,k_1\\) and \\(j=1,\\dots, k_2\\), be the responses in a two-way contingency table, with \\(k_1\\) rows and \\(k_2\\) columns. Further, assume that the counts follow independent Poisson distributions with corresponding means \\(\\lambda_{ij}\\), that is \\(Y_{ij}\\sim \\mbox{Po}(\\lambda_{ij})\\). Then, the distributions of the row sums are, \\[\nY_{i+} \\sim \\mbox{Po}(\\lambda_{i+}),\n\\] where \\(\\displaystyle Y_{i+} = \\sum_{j=1}^{k_2} Y_{ij}\\) and \\(\\displaystyle \\lambda_{i+}=\\sum_{j=1}^{k_2} \\lambda_{ij}\\).\nNotes:\n\nHere we are not yet assuming that the marginal totals are fixed.\nSimilar results hold when considering column sums and the overall sum.\n\n\nProof: See MATH2715, Chapter 7 on MGF Properties of Linear Functions.\n\nProposition 7.2 Let \\(Y_{ij}\\sim \\mbox{Po}(\\lambda_{ij})\\) for \\(i=1,\\dots,k_1\\) and \\(j=1,\\dots, k_2\\) then the conditional distribution of each count in a row given that the row sum is fixed is given by \\[\n\\left( Y_{ij} \\, |\\,   Y_{i+} = m \\right)\n\\sim\n\\mbox{Bin}\\left(m,\n\\frac{\\lambda_{ij}}{\\lambda_{i+}}\n\\right).\n\\] That is, conditioning on the sum of independent Poisson random variables induces a Binomial distribution.\n\nProof: First define \\(Y_{i-j}= Y_{i+}-Y_{ij}\\) with correspondingly \\(y_{i-j}= m-y_{ij}\\) and also \\(\\lambda_{i-j}= \\lambda_{i+}-\\lambda_{ij}\\).\nConsider the conditional probability mass function \\[\\begin{align*}\np(Y_{ij} = y_{ij} | Y_{i+} = m)  \n& = \\frac{p(Y_{ij} = y_{ij}, Y_{i+} = m)} {p(Y_{i+} = m)}\n\\quad \\text{by conditional probability definition}\\\\\n& = \\frac{p(Y_{ij} = y_{ij}, Y_{i-j} = m-y_{ij})} {p(Y_{i+} = m)} \\quad \\text{rearranging} \\\\\n& = \\frac{p(Y_{ij} = y_{ij})p(Y_{i-j} = m-y_{ij})} {p(Y_{i+} = m)} \\quad \\text{by independence}.\n\\end{align*}\\] Then, each of these follows a Poisson distribution: \\(Y_{ij} \\sim \\text{Po}(\\lambda_{ij})\\), \\(Y_{i-j}\\sim \\text{Po}(\\lambda_{i-j})\\), and \\(Y_{i+} \\sim \\text{Po}(\\lambda_{i+})\\). Therefore, \\[\\begin{align*}\np(Y_{ij} = y_{ij} | Y_{i+} = m)  \n& =\n\\frac{\\lambda_{ij}^{y_{ij}}e^{-\\lambda_{ij}}}{y_{ij}!}\n\\frac{\\lambda_{i-j}^{m-y_{ij}}e^{-\\lambda_{i-j}}}{m-y_{ij}!}\n\\Bigg/\n\\frac{\\lambda_{i+}^{m} e^{-\\lambda_{i+}}}{m!} \\\\\n& =\n{m \\choose y_{ij}}\n\\left(\\frac{\\lambda_{ij}}{\\lambda_{i+}} \\right) ^{y_{ij}}\n\\left(\\frac{\\lambda_{i-j}}{\\lambda_{i+}} \\right) ^{m-y_{ij}}\\\\\n& =\n{m \\choose y_{ij}}\n\\pi ^{y_{ij}}\n\\left(1-\\pi\\right) ^{m-y_{ij}}\n\\end{align*}\\]\nwhich is the probability mass function of a \\(\\text{Bin}(m, \\pi)\\) random variable, where \\(\\pi = \\lambda_{ij} / \\lambda_{i+}\\).\n\nProposition 7.3 Let \\(Y_{ij}\\sim \\mbox{Po}(\\lambda_{ij})\\) for \\(i=1,\\dots,k_1\\) and \\(j=1,\\dots, k_2\\) then the joint conditional distribution of the counts in a row given that the row sum is fixed is given by \\[\np(Y_{i1},\\dots, Y_{i k_2} \\, | \\, Y_{i+}=m)\n=\n\\frac{m!}{y_{i1}!\\cdots y_{i k_2}!}\n\\pi_{i1}^{y_{i1}} \\cdots\\pi_{ik_2}^{y_{ik_2}}\n\\] with \\[\n\\pi_{ij}=\\lambda_{ij}/\\lambda_{i+}\n\\quad \\mbox{ and } \\quad\n\\sum_{j=1}^{k_2} \\pi_{ij} =1.\n\\] This distribution is called the Multinomial distribution, with index \\(m\\) and parameters \\(\\pi_{ij}, j=1,\\dots k_2\\), which can be denoted \\[\n(Y_{i1},\\dots, Y_{i k_2} \\, | \\, Y_{i+}=m)\n\\sim\n\\text{Mult}(m, \\pi_{i1}, \\dots \\pi_{ik_2}\n).\n\\]\n\nProof: Omitted.\n\nProposition 7.4 Let \\(Y_{ij}\\sim \\mbox{Po}(\\lambda_{ij})\\) for \\(i=1,\\dots,k_1\\) and \\(j=1,\\dots, k_2\\) then the conditional distribution of the counts given that the overall sum is fixed is \\[\n(Y_{11},\\dots, Y_{k_1k_2} \\, | \\, Y_{++}=m)\n\\sim\n\\text{Mult}(m, \\pi_{11},\\dots, \\pi_{k_1k_2}).\n\\] with probability mass function given by \\[\np(Y_{11},\\dots, Y_{k_1k_2} \\, | \\, Y_{++}=m)\n=\n\\frac{m!}{y_{11}!\\cdots y_{k_1k_2}!}\n\\pi_1^{y_1} \\cdots\\pi_{k_1k_2}^{y_{k_1k_2}}\n\\] where \\(\\pi_{ij} = \\lambda_{ij}/\\lambda_{++}\\).\n\nProof: Omitted."
  },
  {
    "objectID": "7_extendedloglinearmodel.html#product-multinomial-models",
    "href": "7_extendedloglinearmodel.html#product-multinomial-models",
    "title": "7  Extensions to Loglinear models",
    "section": "7.3 Product-multinomial models",
    "text": "7.3 Product-multinomial models\nTo be added soon."
  },
  {
    "objectID": "7_extendedloglinearmodel.html#multi-way-contingency-tables",
    "href": "7_extendedloglinearmodel.html#multi-way-contingency-tables",
    "title": "7  Extensions to Loglinear models",
    "section": "7.4 Multi-way contingency tables",
    "text": "7.4 Multi-way contingency tables"
  },
  {
    "objectID": "7_extendedloglinearmodel.html#exercises",
    "href": "7_extendedloglinearmodel.html#exercises",
    "title": "7  Extensions to Loglinear models",
    "section": "7.5 Exercises",
    "text": "7.5 Exercises\nTo be added later."
  },
  {
    "objectID": "7_extendedloglinearmodel.html#model-fitting-in-r",
    "href": "7_extendedloglinearmodel.html#model-fitting-in-r",
    "title": "7  Extensions to Loglinear models",
    "section": "7.4 Model fitting in R",
    "text": "7.4 Model fitting in R"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH3823 Generalized Linear Models",
    "section": "",
    "text": "\\[\n\\def\\b#1{\\mathbf{#1}}\n\\]\n\n\nWeekly schedule\n\n\n\n\n\n\nWeek 11 (8 - 12 May)\n\n\n\nRevision and examination preparation.\n\n\n\n\n\n\n\n\nWeek 10 (1 - 5 May)\n\n\n\nAny remaining module material. To be confirmed after Week 9.\n\n\n\n\n\n\n\n\nWeek 9 (27 - 31 March)\n\n\n\n\nBefore next Lecture: Re-read Chapter 6: Log-linear Models.\nLecture on Tuesday: Start Chapter 7: Extensions to Loglinear models.\nLecture on Thursday: Continue Chapter 7: Extensions to Loglinear models.\nWeekly feedback: Relevant exercise questions from last year (solutions online).\n\n\n\n\n\n\n\n\n\nCoursework Practical Sessions (20 - 24 March)\n\n\n\n\nCoursework for this module involves a single written report worth 20% of the module grade. This will mainly involve investigating different models using R and interpreting the results. Tasks are expected to be handed out on 14 March with extended hand-in deadline expect to be 5 April. See Learning Resources / Practical Assessment for details of the tasks and for submission links.\n\n\n\n\n\n\n\n\n\nWeek 8 (20 - 24 March)\n\n\n\n\nBefore next Lecture: Re-read Chapter 5: Sections 5.1-5.3.\nLecture on Tuesday: Cancelled due to UCU strike.\nComputer Practical on Tuesday: Cancelled due to UCU strike.\nComputer Practical on Wednesday: Cancelled due to UCU strike.\nLecture on Thursday: Continue Chapter 6: Loglinear Modelling.\nWeekly feedback: Relevant exercise questions from last year (solutions online).\n\n\n\n\n\n\n\n\n\nWeek 7 (13 - 17 March)\n\n\n\n\nBefore next Lecture: Re-read Chapter 5: Sections 5.1-5.3.\nLecture on Tuesday: Complete Chapter 5: by looking at Section 5.4 - Application to dose-response experiments.\nLecture on Thursday: Cancelled due to UCU strike. Please self-study Chapter 6: Loglinear Modelling with Section 6.1 - Overview and Section 6.2 - Motivating Examples.\nWeekly feedback: Relevant exercise questions from last year (solutions online).\n\n\n\n\n\n\n\n\n\nWeek 6 (6 - 10 March)\n\n\n\n\nBefore next Lecture: Re-read Chapter 4: Sections 4.1-4.4.\nLecture on Tuesday: Complete Chapter 4: with Section 4.5 Fitting GLMs in R\nLecture on Thursday: Start Chapter 5: Logistic Regression with Sections 5.1 & 5.2.\nWeekly feedback: Relevant exercise questions from last year (solutions online).\n\n\n\n\n\n\n\n\n\nWeek 5 (27 February - 3 March)\n\n\n\n\nBefore next Lecture: Re-read Chapter 4: Section 4.1.\nLecture on Tuesday: Cancelled due to illness. Please read Chapter 4: Section 4.2.\nLecture on Thursday: Chapter 4: Sections 4.3, 4.4 & 4.5.\nWeekly feedback: Start Exercises in Chapter 4.\n\n\n\n\n\n\n\n\n\nWeek 4 (20 - 24 February)\n\n\n\n\nBefore next Lecture: Be confident with all material up to, and including, Section 3.4 Moments of exponential-family distributions.\nLecture on Tuesday: Chapter 3: Sections 3.5 & 3.6\nLecture on Thursday: Start Chapter 4 by covering Section 4.1.\nWeekly feedback: Complete Exercises in Chapter 3.\n\n\n\n\n\n\n\n\n\nWeek 3 (13 - 17 February)\n\n\n\n\nBefore next Lecture: Be confident with material in Chapter 2: Essentials of Normal Linear Models.\nLecture on Tuesday: Cancelled due to UCU strike. Instead, self-study Chapter 3: Sections 3.1 & 3.2.\nLecture on Thursday: Cancelled due to UCU strike. Instead, self-study Chapter 3: Sections 3.3 & 3.4.\nBefore next Lecture: Complete questions and check solutions, including video(s), for all Exercises in Chapters 1 and 2. Start Exercises in Chapter 3.\n\n\n\n\n\n\n\n\n\nWeek 2 (6 - 10 February)\n\n\n\n\nBefore next Lecture: Please re-read Section 2.1: Overview and read Section 2.2: Types of normal linear model.\nLecture on Tuesday: We will briefly cover all remaining material in Chapter 2: Essentials of Normal Linear Models.\nBefore next Lecture: Please re-read Chapter 2 carefully.\nLecture on Thursday: Cancelled due to UCU strike.\nWeekly feedback: Self-study the Exercises in Section 2.6 – solutions to be posted during Week 3.\n\n\n\n\n\n\n\n\n\nWeek 1 (30 January - 3 February)\n\n\n\n\nBefore next Lecture: Please read the Overview.\nLecture on Tuesday: We will briefly cover all material in Chapter 1: Introduction.\nBefore next Lecture: Please re-read Chapter 1 carefully.\nLecture on Thursday: Start Chapter 2: Essentials of Normal Linear Models with Section 2.1: Overview.\nWeekly feedback: Self-study the Exercises in Section 1.5 – solutions to be posted during Week 1."
  }
]