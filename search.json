[
  {
    "objectID": "1_intro.html#overview",
    "href": "1_intro.html#overview",
    "title": "1  Introduction",
    "section": "1.1 Overview",
    "text": "1.1 Overview\nIn previous modules you have studied linear models with a normally distributed error term, such as simple linear regression, multiple linear regression and ANOVA for normally distributed observations. In this module we will study generalized linear models.\nOutline of the module:\n\nRevision of linear models with normal errors.\nIntroduction to generalized linear models, GLMs.\nLogistic regression models.\nLoglinear models, including contingency tables.\n\n\n\n\n\n\n\nImportant\n\n\n\nThis module will make extensive use of \\(\\mathbf{R}\\) and hence it is very important that you are comfortable with its use. If you need some revision, then material is available on Minerva under RStudio Support.\n\n\nThe purpose of a generalized linear model is to describe the dependence of a response variable \\(y\\) on a set of \\(p\\) explanatory variables \\(\\b{x}=(x_1, x_2, \\ldots, x_p)\\) where, conditionally on \\(\\b{x}\\), observation \\(y\\) has a distribution which is not necessarily normal.\nNote that in these notes we may use lowercase letters, for example \\(y\\) or \\(y_i,\\) to denote both observed values or random variables, which is being considered should be clear from the context.\n\n\n\n\n\n\nImportant\n\n\n\nThis module will make extensive use of many basic ideas from statistics. If you need some revision, then see Appendix A: Basic material on Minerva under Basic Pre-requisite Material."
  },
  {
    "objectID": "1_intro.html#motivating-example",
    "href": "1_intro.html#motivating-example",
    "title": "1  Introduction",
    "section": "1.2 Motivating example",
    "text": "1.2 Motivating example\nTable 1.1 shows data1 on the number of beetles killed by five hours of exposure to 8 different concentrations of gaseous carbon disulphide.\n\n\nTable 1.1: Numbers of beetles killed by five hours of exposure to 8 different concentrations of gaseous carbon disulphide\n\n\n\n\n\n\n\nDose\n\\(x_i\\)\nNo. of beetle\n\\(m_i\\)\nNo. killed\n\\(y_i\\)\n\n\n\n\n1.6907\n59\n6\n\n\n1.7242\n60\n13\n\n\n1.7552\n62\n18\n\n\n1.7842\n56\n28\n\n\n1.8113\n63\n52\n\n\n1.8369\n59\n53\n\n\n1.8610\n62\n61\n\n\n1.8839\n60\n60\n\n\n\n\nFigure 1.1 (a) shows the same data with a linear regression line superimposed. Although this line goes close to the plotted points, we can see some fluctuations around it. More seriously, this is a stupid model: it would predict a mortality rate of greater than 100% at a dose of 1.9 units, and a negative mortality rate at 1.65 units!\n\n\nCode\nbeetle = read.table(\"https://rgaykroyd.github.io/MATH3823/Datasets/beetle.txt\", header=T)\n\ndose = beetle$dose\nmortality = beetle$died/beetle$total\n\nplot(dose, mortality, pch=16,\n     xlim=c(1.65, 1.90), xlab =\"Dose\",\n     ylim=c(-0.1, 1.1),  ylab=\"Mortality\")\nabline(h=c(0,1), lty=2)\n\nlm.fit = lm(mortality ~ dose)\nabline(lm.fit)\n\nplot(dose, mortality, pch=16,\n     xlim=c(1.65, 1.90), xlab =\"Dose\",\n     ylim=c(-0.1, 1.1),  ylab=\"Mortality\")\nabline(h=c(0,1), lty=2)\n\ny = cbind(beetle$died, beetle$total-beetle$died)\nglm.fit = glm(y ~ dose, family=binomial(link='logit'))\n\noutput.dose = seq(1.6,1.95,0.001)\nfitted = predict(glm.fit, data.frame(dose=output.dose), type=\"response\")\nlines(output.dose, fitted)\n\n\n\n\n\n\n\n\n(a) Linear model\n\n\n\n\n\n\n\n(b) Logistic model\n\n\n\n\nFigure 1.1: Beetle mortality rates with fitted dose- response curves.\n\n\n\nA more sensible dose–response relationship for the beetle mortality data might be based on the logistic function (to be defined later), as plotted in Figure 1.1 (b). The resulting curve is a closer, more-sensible, fit. Later in this module we will see how this curve was fitted using maximum likelihood estimation for an appropriate generalized linear model.\nThis is an example of a dose-response experiment which are widely used in medical and pharmaceutical situations.\n\n\n\n\n\n\nWarning\n\n\n\nWarning of potentially sensitive material. For further information on dose-response experiments see, for example, www.britannica.com/science/dose-response-relationship."
  },
  {
    "objectID": "1_intro.html#revision-of-least-squares-estimation",
    "href": "1_intro.html#revision-of-least-squares-estimation",
    "title": "1  Introduction",
    "section": "1.3 Revision of least-squares estimation",
    "text": "1.3 Revision of least-squares estimation\nSuppose that we have \\(n\\) paired data values \\((x_1, y_1),\\dots, (x_n, y_n)\\) and that we believe these are related by a linear model\n\\[\ny_i = \\alpha+\\beta x_i +\\epsilon_i\n\\]\nfor all \\(i\\in \\{1, 2,\\dots,n\\}\\), where \\(\\epsilon_1,\\dots,\\epsilon_n\\) are independent and identically distributed (iid) with \\(\\mbox{E}(\\epsilon_i)=0\\) and \\(\\mbox{Var}(\\epsilon_i)=\\sigma^2\\). The aim will be to find values of the model parameters, \\(\\alpha, \\beta \\text{ and } \\sigma^2\\) using the data. Specifically, we will estimate \\(\\alpha\\) and \\(\\beta\\) using the values which minimize the residual sum of squares (RSS)\n\\[\nRSS(\\alpha, \\beta) = \\sum_{i=1}^n \\left(y_i-(\\alpha+\\beta x_i)\\right)^2.\n\\tag{1.1}\\]\nThis measures how close the data points are around the regression line and hence the resulting estimates, \\(\\hat\\alpha\\) and \\(\\hat\\beta\\), will give us a fitted regression line which is closest to the data.\nIt can be shown that Equation 1.1 takes its minimum when the parameters are given by\n\\[\n\\hat\\alpha = \\bar y -\\hat\\beta\\bar x, \\quad \\mbox{and} \\quad\n\\hat\\beta = \\frac{s_{xy}}{s^2_x}\n\\tag{1.2}\\]\nwhere \\(\\bar x\\) and \\(\\bar y\\) are the sample means,\n\\[\ns_{xy}=\\frac{1}{n-1}\\sum_{i=1}^n (x_i-\\bar x)(y_i-\\bar y)\n\\]\nis the sample covariance and\n\\[\ns^2_x = \\frac{1}{n-1} \\sum_{i=1}^n (x_i-\\bar x)^2\n\\]\nis the sample variance of the \\(x\\) values. It can be shown that these estimators are unbiased, that is \\(\\mbox{E}[\\hat\\alpha]=\\alpha\\) and \\(\\mbox{E}[\\hat\\beta]=\\beta\\) – see Section 1.5.\nThe fitted regression lines is then given by \\(\\hat y = \\hat \\alpha +\\hat \\beta x\\), the fitted values by \\(\\hat y_i = \\hat \\alpha +\\hat \\beta x_i\\), and the model residuals by \\(r_i= \\hat \\epsilon_i= y_i-\\hat y_i\\) for all \\(i\\in \\{1,\\dots,n\\}\\).\nTo complete the model fitting, we also estimate the error variance, \\(\\sigma^2\\), using \\[\n\\hat \\sigma^2 = \\frac{1}{n-2} \\sum _{i=1}^n r_i^2.\n\\tag{1.3}\\]\nNote that, by construction, \\(\\bar r=0\\) and, further, it can be shown that \\(\\hat \\sigma^2\\) is an unbiased estimator of \\(\\sigma^2\\), that is \\(\\mbox{E}[\\hat\\sigma^2]=\\sigma^2\\).\n\n\nCode\nxbar = mean(dose)\nybar = mean(mortality)\ns2x = var(dose)\nsxy = cov(dose, mortality)\n\nbetahat = sxy/s2x\nalphahat = ybar-betahat*xbar\n\ns2hat = sum((mortality-alphahat-betahat*dose)^2)/(length(dose)-2)\n\n\nReturning to the above beetle data example, we have \\(\\hat\\alpha=\\)-8.947843, \\(\\hat\\beta=\\) 5.324937, and \\(\\hat \\sigma^2 =\\) 0.0075151.\nWe will interpret the output later, but in \\(\\b{R}\\), the fitting can be done with a single command with corresponding fitting output from a second command:\n\n\nCode\nlm.fit = lm(mortality ~ dose)\n\nsummary(lm.fit)\n\n\n\nCall:\nlm(formula = mortality ~ dose)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.10816 -0.06063  0.00263  0.05119  0.12818 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -8.9478     0.8717  -10.27 4.99e-05 ***\ndose          5.3249     0.4857   10.96 3.42e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.08669 on 6 degrees of freedom\nMultiple R-squared:  0.9524,    Adjusted R-squared:  0.9445 \nF-statistic: 120.2 on 1 and 6 DF,  p-value: 3.422e-05\n\n\n\n\n\n\n\n\nImportant\n\n\n\nYou should have met \\(\\b{R}\\) output like this in previous statistics modules, but if you need some revision then see Appendix-C: Background to Analysis of Variance on Minerva under Basic Pre-requisite Material."
  },
  {
    "objectID": "1_intro.html#types-of-variables",
    "href": "1_intro.html#types-of-variables",
    "title": "1  Introduction",
    "section": "1.4 Types of variables",
    "text": "1.4 Types of variables\nThe way a variable enters a model will depends on its type. The most common five types of variable are:\n\nQuantitative\n\nContinuous: for example, height; weight; duration. Real valued. Note that although recorded data is rounded it is still usually best regarded as continuous.\nCount (discrete): for example, number of children in a family; accidents at a road junction; number of items sold. Non-negative and integer-valued.\n\nQualitative\n\nOrdered categorical (ordinal): for example, severity of illness (Mild/ Moderate/Severe); degree classification (first/ upper-second/ lower-second/ third).\nUnordered categorical (nominal):\n\nDichotomous (binary): two categories: for example sex (M/ F); agreement (Yes/ No); coin toss (Head/ Tail).\nPolytomous (also known as polychotomous): more than two categories: for example blood group (A/ B/ O); eye colour (Brown/ Blue/ Green).\n\n\n\nNote that although dichotomous is clearly a special case of polytomous, making the distinction is usually worthwhile as it often leads to a simplified modelling and testing approach."
  },
  {
    "objectID": "1_intro.html#sec-exercises1",
    "href": "1_intro.html#sec-exercises1",
    "title": "1  Introduction",
    "section": "1.5 Exercises",
    "text": "1.5 Exercises\n\n\n\n\n\n\nImportant\n\n\n\nUnless otherwise stated, data files will be available online at: rgaykroyd.github.io/MATH3823/Datasets/filename.ext, where filename.ext is the stated filename with extension.\n\n\n\n1.1 Consider again the beetle data in Table 1.1. Perform the calculations by hand and then check the answers using \\(\\b{R}\\) – a copy of the data is available in the file beetle.txt. Finally plot the fitted regression line on a scatter plot of the data. [Hint: See the code chunk used to produce Figure 1.1.]\n1.2 Consider the following synthetic data:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(i=1\\)\n\\(i=2\\)\n\\(i=3\\)\n\\(i=4\\)\n\\(i=5\\)\n\\(i=6\\)\n\\(i=7\\)\n\\(i=8\\)\n\n\n\n\n\\(x_i\\)\n-1\n0\n1\n2\n2.5\n3\n4\n6\n\n\n\\(y_i\\)\n-2.8\n-1.1\n7.2\n8.0\n8.9\n9.2\n14.8\n24.7\n\n\n\nPlot the data to check that a linear model is suitable and then fit a linear regression model. Do you think that the fitted model can be reliably used to predict the values of \\(y\\) when \\(x=5\\) and \\(x=10\\)? Justify your answers.\n1.3 Starting from Equation 1.1, derive the estimation equations given in Equation 1.2. Further, show that \\(\\hat\\alpha\\) and \\(\\hat\\beta\\) are unbiased estimators of \\(\\alpha\\) and \\(\\beta\\). [Hint: Check your MATH1712 lecture notes.]\nWhat can be said about \\(\\hat\\sigma^2\\) as an estimator of \\(\\sigma^2\\)? [Hint: There is a careful theoretical proof, but here only an intuitive explanation is expected.]\n1.4 The Brownlee’s Stack Loss Plant Data2 is already available in \\(\\mathbf{R}\\), with background details on the help page, \\(\\texttt{?stackloss}\\). [Hint: You already met this example in MATH1712.]\nAfter plotting all pairs of variables, which of \\(\\texttt{Air.Flow}\\), \\(\\texttt{Water.Temp}\\) and \\(\\texttt{Acid.Conc}\\) do you think could be used to model \\(\\texttt{stack.loss}\\) using a linear regression? Justify your answer.\nPerform a simple linear regression with using \\(\\texttt{stack.loss}\\) as the response variable and your chosen variable as the explanatory variable. Add the fitted regression line to a scatter plot of the data and comment.\n1.5 In an experiment conducted by de Silva et al. in 20203 data was obtained to investigate falling objects and gravity, as first consider by Galileo and Newton. A copy of the data is available in the file physics_from_data.csv.\nRead the data file into \\(\\b{R}\\) and perform a simple linear regression of the maximum Reynolds number as the response variable and, in turn, each of the other variables as the explanatory variable.\nPlot the data and add the corresponding fitted linear models. Which variable do you think helps explain Reynolds number the best? Why do you think this?\n\n\nHere are an infinite number of further numerical examples from maths e.g. (thanks to https://www.mathcentre.ac.uk/):\nFinding the intersercept\nFinding the slope - Part 1\nFinding the slope - Part 2"
  },
  {
    "objectID": "0_preface.html",
    "href": "0_preface.html",
    "title": "Overview",
    "section": "",
    "text": "Official Module Description"
  },
  {
    "objectID": "0_preface.html#preface",
    "href": "0_preface.html#preface",
    "title": "Overview",
    "section": "Preface",
    "text": "Preface\nThese lecture notes are produced for the University of Leeds module MATH3823 - Generalized Linear Models for the academic year 2022-23. Please note that this material also forms part of the module MATH5824 - Generalized Linear and Additive Models. They are based on those used previously for this module and I am grateful to previous module lecturers for their considerable effort: Lanpeng Ji, Amanda Minter, John Kent, Wally Gilks, and Stuart Barber. This is the first year, however, that they have been produced in accessible format and hence some errors might occur during this conversion process. For information, I am using Quarto (a successor to RMarkdown) from RStudio to produce both the html and PDF, and then GitHub to create the website which can be accessed at rgaykroyd.github.io/MATH3823/. Please note that the PDF versions will only be made available on the University of Leeds Minerva system. Although I am a long-term user of RStudio, I have not previously used Quarto/RMarkdown nor Github and hence please be patient if there are hitches along the way.\nRG Aykroyd, Leeds, November 22, 2022\n\n\n\n\n\n\n\n\nWarning\n\n\n\nStatistical ethics and sensitive data\nPlease note that from time to time we will be using data sets from situations which some might perceive as sensitive. All such data sets will, however, be derived from real-world studies which appear in textbooks or in scientific journals. The daily work of many statisticians involves applying their professional skills in a wide variety of situations and as such it is important to include a range of commonly encountered examples in this module. Whenever possible, sensitive topics will be signposted in advance. If you feel that any examples may be personally upsetting then, if possible, please contact the module lecturer in advance. If you are significantly effected by any of these situations, then you can seek support from the Student Counselling and Wellbeing service."
  },
  {
    "objectID": "0_preface.html#module-summary",
    "href": "0_preface.html#module-summary",
    "title": "Overview",
    "section": "Module summary",
    "text": "Module summary\nLinear regression is a tremendously useful statistical technique but is very limited. Generalised linear models extend linear regression in many ways - allowing us to analyse more complex data sets. In this module we will see how to combine continuous and categorical predictors, analyse binomial response data and model count data."
  },
  {
    "objectID": "0_preface.html#objectives",
    "href": "0_preface.html#objectives",
    "title": "Overview",
    "section": "Objectives",
    "text": "Objectives\nOn completion of this module, students should be able to:\n\ncarry out regression analysis with generalised linear models including the use of link functions;\nunderstand the use of deviance in model selection;\nappreciate the problems caused by overdispersion;\nfit and interpret the special cases of log linear models and logistic regression;\nuse a statistical package with real data to fit these models to data and to write a report giving and interpreting the results."
  },
  {
    "objectID": "0_preface.html#syllabus",
    "href": "0_preface.html#syllabus",
    "title": "Overview",
    "section": "Syllabus",
    "text": "Syllabus\nGeneralised linear model; probit model; logistic regression; log linear models."
  },
  {
    "objectID": "0_preface.html#university-module-catalogue",
    "href": "0_preface.html#university-module-catalogue",
    "title": "Overview",
    "section": "University Module Catalogue",
    "text": "University Module Catalogue\nFor any further details, please see MATH3823 Module Catalogue page"
  },
  {
    "objectID": "2_linearmodels.html#overview",
    "href": "2_linearmodels.html#overview",
    "title": "2  Essentials of Normal Linear Models",
    "section": "2.1 Overview",
    "text": "2.1 Overview\nIn many fields of application, we might assume the response variable is normally distributed. For example: heights, weights, log prices, etc.\nThe data1 in Table 2.1 record the birth weights of 12 girls and 12 boys and their gestational ages (time from conception to birth).\n\n\nTable 2.1: Gestational ages (in weeks) and birth weights (in grams) for 24 babies (12 girls and 12 boys).\n\n\n\n\n(a) Girls\n\n\nGestational Age\nBirth weight\n\n\n\n\n40\n3317\n\n\n36\n2729\n\n\n40\n2935\n\n\n37\n2754\n\n\n42\n3210\n\n\n39\n2817\n\n\n40\n3126\n\n\n37\n2539\n\n\n36\n2412\n\n\n38\n2991\n\n\n39\n2875\n\n\n40\n3231\n\n\n\n\n\n\n(b) Boys\n\n\nGestational Age\nBirth weight\n\n\n\n\n40\n2968\n\n\n38\n2795\n\n\n40\n3163\n\n\n35\n2925\n\n\n36\n2625\n\n\n37\n2847\n\n\n41\n3292\n\n\n40\n3473\n\n\n37\n2628\n\n\n38\n3176\n\n\n40\n3421\n\n\n38\n3975\n\n\n\n\n\n\nA key question is, can we predict the birth weight of a baby born at a given gestational age using these data. For this we will need to make assumptions about the relationship between birth weight and gestational age, and any associated natural variation – that is we require a model.\nFirst we should explore the data. Figure 2.1 (a) shows a histogram of the birth weights indicating a spread around modal group 2800-3000 grams; Figure 2.1 (b) indicates slightly higher birth weights for the boys than the girls; and Figure 2.1 (c) shows an increasing relationship between weight and age. Together, these suggest that gestational age and sex are likely to be important for predicting weight.\n\n\nCode\nbirthweight = read.table(\"https://rgaykroyd.github.io/MATH3823/Datasets/birthwt-numeric.txt\", header=T)\n\nweight = birthweight$weight\nage = birthweight$age\nsex = birthweight$sex\n\nhist(weight, breaks=6, probability = T, main = \"\", \n     xlab = \"Birth weight (grams)\")\nboxplot(weight~sex, names=c(\"Girl\", \"Boy\"))\n\nplot(age, weight, pch=16, \n     xlab = \"Gestational age (weeks)\",\n     ylab = \"Birth weight (grams)\")\n\n\n\n\n\n\n\n\n(a) Weight distribution\n\n\n\n\n\n\n\n(b) Weight sub-divided by Sex\n\n\n\n\n\n\n\n\n\n(c) Relationship beween variables\n\n\n\n\nFigure 2.1: Birthweight and gestational age for 24 babies.\n\n\n\nBefore considering possible models, Figure 2.2 again shows the relationship between weight and age but this time with the points coloured according to the baby’s sex. This, perhaps, shows the boys to have generally higher weights across the age range than girls.\n\n\nCode\nbirthweight = read.table(\"https://rgaykroyd.github.io/MATH3823/Datasets/birthwt-numeric.txt\", header=T)\n\nweight = birthweight$weight\nage = birthweight$age\nsex = birthweight$sex\n\nplot(age, weight, col=2-sex, pch=15+sex,\n     xlab = \"Gestational age (weeks)\",\n     ylab = \"Birth weight (grams)\")\nlegend(41,2800, c(\"Girl\",\"Boy\"), col = c(2,1), pch=c(15,16))\n\n\n\n\n\nFigure 2.2: Birthweight and gestational age for 12 girls (red squares) and 12 boys (black dots).\n\n\n\n\nOf course, there are very many possible models, but here we will consider the following:\n\n\n\n\n\n\n\n\\(\\texttt{Model 0}:\\)\n\\(\\texttt{Weight}=\\alpha\\)\n\n\n\\(\\texttt{Model 1}:\\)\n\\(\\texttt{Weight}=\\alpha + \\beta.\\texttt{Age}\\)\n\n\n\\(\\texttt{Model 2}:\\)\n\\(\\texttt{Weight}=\\alpha + \\beta.\\texttt{Age}+\\gamma.\\texttt{Sex}\\)\n\n\n\\(\\texttt{Model 3}:\\)\n\\(\\texttt{Weight}=\\alpha + \\beta.\\texttt{Age}+\\gamma.\\texttt{Sex} + \\delta.\\texttt{Age}.\\texttt{Sex}\\)\n\n\n\nIn these models, \\(\\texttt{Weight}\\) is called the response variable (sometimes called the dependent variable) and \\(\\texttt{Age}\\) and \\(\\texttt{Sex}\\) are called the covariates or explanatory variables (sometimes called the predictor or independent variables). Here, \\(\\texttt{Age}\\) is a continuous variable whereas \\(\\texttt{Sex}\\) is coded as a dummy variable taking the value 0 for girls and 1 for boys; it is an example of a factor, in this case with just two levels: Girl and Boy.\nNote that \\(\\texttt{Model 0}\\) is a special case of \\(\\texttt{Model 1}\\) (consider the situation when \\(\\beta=0\\)) and that \\(\\texttt{Model 1}\\) is a special case of \\(\\texttt{Model 2}\\) (consider the situation when \\(\\gamma=0\\)) and finally that \\(\\texttt{Model 2}\\) is a special case of \\(\\texttt{Model 3}\\) (consider the situation when \\(\\delta=0\\)) – such models are called nested.\nIn these models, \\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\) and \\(\\delta\\) are model parameters. Parameter \\(\\alpha\\) is called the intercept term; \\(\\beta\\) is called the main effect of \\(\\texttt{Age}\\); and is interpreted as the effect on birth weight per week of gestational age. Similarly, \\(\\gamma\\) is the main effect of \\(\\texttt{Sex}\\), interpreted as the effect on birth weight of being a boy (because girl is the baseline category).\nParameter \\(\\delta\\) is called the interaction effect between \\(\\texttt{Age}\\) and \\(\\texttt{Sex}\\). Take care when interpreting an interaction effect. Here, it does not mean that age somehow affects sex, or vice-versa. It means that the effect of gestational age on birth weight depends on whether the baby is a boy or a girl.\nThese models can be fitted to the data using (Ordinary) Least Squares to produce the results presented in Figure 2.3.\nWhich model should we use?\n\n\nCode\nbirthweight = read.table(\"https://rgaykroyd.github.io/MATH3823/Datasets/birthwt-numeric.txt\", header=T)\n\nweight = birthweight$weight\nage = birthweight$age\nsex = birthweight$sex\n\nplot(age, weight, pch=16, \n     xlab = \"Gestational age (weeks)\",\n     ylab = \"Birth weight (grams)\")\nabline(h=mean(weight))\n\n\nplot(age, weight, pch=16,  \n     xlab = \"Gestational age (weeks)\",\n     ylab = \"Birth weight (grams)\")\nM1.fit = lm(weight~age)\nabline(M1.fit$coefficients[1],M1.fit$coefficients[2])\n\n\nplot(age, weight, pch=15+sex, col=2-sex, \n     xlab = \"Gestational age (weeks)\",\n     ylab = \"Birth weight (grams)\")\nlegend(41,2800, c(\"Girl\",\"Boy\"), col = c(2,1),pch=c(15,16))\n\nM2.fit = lm(weight~age+sex)\n\nabline(M2.fit$coefficients[1],M2.fit$coefficients[2], col=2)\nabline(M2.fit$coefficients[1]+M2.fit$coefficients[3],M2.fit$coefficients[2], col=1)\n\n\nplot(age, weight, pch=15+sex, col=2-sex, \n     xlab = \"Gestational age (weeks)\",\n     ylab = \"Birth weight (grams)\")\nlegend(41,2800, c(\"Girl\",\"Boy\"), col = c(2,1), pch=c(15,16))\n\nM3.fit = lm(weight~age+sex+age*sex)\n\nabline(M3.fit$coefficients[1],M3.fit$coefficients[2], col=2)\nabline(M3.fit$coefficients[1]+M3.fit$coefficients[3],M3.fit$coefficients[2]+M3.fit$coefficients[4], col=1)\n\n\n\n\n\n\n\n\n(a) Model 0\n\n\n\n\n\n\n\n(b) Model 1\n\n\n\n\n\n\n\n\n\n(c) Model 2\n\n\n\n\n\n\n\n(d) Model 3\n\n\n\n\nFigure 2.3: Birthweight and gestational age data with superimposed fitted regression lines from various competing models.\n\n\n\nWe know from previous modules that statistical tests can be used to check the importance of regression coefficients and model parameters, but it is also important to use the graphical results, as in Figure 2.3, to guide us.\n\\(\\texttt{Model 0}\\) says that there is no change in birth weight with gestational age which means that we would use the average birth weight as the prediction whatever the gestational age – this makes no sense. As we can easily see from the scatter plot of the data, the fitted line in this case is clearly inappropriate.\n\\(\\texttt{Model 1}\\) does not take into account whether the baby is a girl or a boy, but does model the relationship between birth weight and gestational age. This does seem to provide a good fit and might be adequate for many purposes. Recall from Figure 2.1 (b) and Figure 2.2, however, that for a given gestational age the boys seem to have a higher birth weight than the girls.\n\\(\\texttt{Model 2}\\) does take the sex of the baby into account by allowing separate intercepts in the fitted lines – this means that the lines are parallel. By eye, there is a clear difference between these two lines but it might not be important.\n\\(\\texttt{Model 3}\\) allows for separate slopes as well as intercepts. There is a slight difference in the slopes, with the birth weight of the girls gradually catching-up as the gestational age increases. It is difficult to see, however, if this will be a general pattern or if it is only true for this data set – especially given the relatively small sample size.\nHere, it is not clear by eye which of the fitted models will be the best and hence we should use a statistical test to help. In particular, we can choose between the models using F-tests.\nLet \\(y_i\\) denote the value of the dependent variable \\(\\texttt{Weight}\\) for individual \\(i=1,\\dots,n\\), and let the four models be indexed by \\(k=0,1,2,3\\).\nLet \\(R_k\\) denote the residual sum of squares (RSS) for Model \\(k:\\)\n\\[\nR_k = \\sum_{i=1}^n (y_i - \\hat{\\mu}_{ki})^2,\n\\tag{2.1}\\]\nwhere \\(\\hat{\\mu}_{ki}\\) is the fitted value for individual \\(i\\) under \\(\\texttt{Model}\\) \\(k\\). Let \\(r_k\\) denote the corresponding residual degrees of freedom for \\(\\texttt{Model}\\) \\(k\\) (the number of observations minus the number of model parameters).\nConsider the following hypotheses: \\[\nH_0: \\texttt{Model } 0 \\text{ is true}; \\hspace{5mm} H_1: \\texttt{Model } 1 \\text{ is true}.\n\\] Under the null hypothesis \\(H_0\\), the difference between \\(R_0\\) and \\(R_1\\) will be purely random, so the between-models mean-square \\((R_0 - R_1)/(r_0 - r_1)\\) should be comparable to the residual mean-square \\(R_1/r_1\\). Thus our test statistic for comparing \\(\\texttt{Model } 1\\) to the simpler \\(\\texttt{Model } 0\\) is:\n\\[\nF_{01} = \\frac{(R_0 - R_1)/(r_0 - r_1)}{R_1/r_1}.\n\\tag{2.2}\\]\nIt can be shown that, under the null hypothesis \\(H_0\\), the statistic \\(F_{01}\\) will have an \\(F\\)-distribution on \\(r_0 - r_1\\) and \\(r_1\\) degrees of freedom, which we write: \\(F_{r_0-r_1, r_1}\\). Under the alternative hypothesis \\(H_1\\), the difference \\(R_0-R_1\\) will tend to be larger than expected under \\(H_0\\), and so the observed value \\(F_{01}\\) will probably lie in the upper tail of the \\(F_{r_0-r_1, r_1}\\) distribution.\nReturning to the birth weight data, we obtain the following output from \\(\\b{R}\\) when we fit \\(\\texttt{Model } 1\\):\n\n\nCode\n# read the data from file into a dataframe called ’birthweight’\nbirthweight = read.table(\"https://rgaykroyd.github.io/MATH3823/Datasets/birthwt.txt\", header=T)\n\n# fit Model 1\nfit1 = lm(weight ~ age, data=birthweight)\n\n# print the parameter estimates from Model 1\ncoefficients(fit1)\n\n\n(Intercept)         age \n -1484.9846    115.5283 \n\n\nCode\n# perform an analysis of variance of Model 1\nanova(fit1)\n\n\nAnalysis of Variance Table\n\nResponse: weight\n          Df  Sum Sq Mean Sq F value   Pr(>F)    \nage        1 1013799 1013799   27.33 3.04e-05 ***\nResiduals 22  816074   37094                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThus we have parameter estimates: \\(\\hat\\alpha = -1484.98\\) and \\(\\hat\\beta = 115.5\\). The Analysis of Variance (ANOVA) table gives: \\(R_0-R_1 = 1013799\\) with \\(r_0-r_1 = 1\\) and \\(R_1 = 816074\\) with \\(r_1 = 22\\).\nIf we wanted \\(R_0\\) and \\(r_0\\) then we can either fit \\(\\texttt{Model 0}\\) or get them by subtraction.\nThe \\(F_{01}\\) statistic, Equation 2.2, is then\n\\[\nF_{01} = \\frac{113799/1}{816074/22} = 27.33,\n\\] which can be read directly from the ANOVA table in the column headed ‘F value’.\nIs \\(F_{01} = 27.33\\) in the upper tail of the \\(F_{1,22}\\) distribution? (See Figure 2.4 and note that 27.33 is very far to the right.) The final column of the ANOVA table tells us that the probability of observing \\(F_{01} > 27.33\\) is only \\(3.04\\times10^5\\) – this is called a p-value. The *** beside this p-value highlights that its value lies between 0 and 0.001. This indicates that we should reject \\(H_0\\) in favour of \\(H_1\\) – there is very strong evidence for the more complicated model. Thus we would conclude that the effect of gestational age is statistically significant in these data.\n\n\nCode\ncurve(df(x,1,22), 0,30, \n      ylab=expression(\"PDF of \"*\"F\"[0][1]))\n\n\n\n\n\nFigure 2.4: Probability density function of \\(F_{01}\\) distribution.\n\n\n\n\nNext, consider the following hypotheses:\n\\[\nH_0: \\texttt{Model } 1 \\text{ is true}; \\hspace{5mm} H_1: \\texttt{Model } 2 \\text{ is true}.\n\\]\nUnder the null hypothesis \\(H_0\\), the difference between \\(R_1\\) and \\(R_2\\) will be purely random, so the between-models mean-square \\((R_1 - R_2)/(r_1 - r_2)\\) should be comparable to the residual mean-square \\(R_2/r_2\\). Thus our test statistic for comparing \\(\\texttt{Model } 2\\) to the simpler \\(\\texttt{Model } 1\\) is:\n\\[\nF_{12} = \\frac{(R_1 - R_2)/(r_1 - r_2)}{R_2/r_2}.\n\\tag{2.3}\\]\nUnder the null hypothesis \\(H_0\\), the statistic \\(F_{12}\\) will have an \\(F\\)-distribution on \\(r_1 - r_2\\) and \\(r_2\\) degrees of freedom, which we write: \\(F_{r_1-r_2, r_2}\\). Under the alternative hypothesis \\(H_1\\), the difference \\(R_1-R_2\\) will tend to be larger than expected under \\(H_0\\), and so the observed value \\(F_{12}\\) will probably lie in the upper tail of the \\(F_{r_1-r_2, r_2}\\) distribution.\nReturning to the birth weight data, we obtain the following output from \\(\\b{R}\\) (where \\(\\texttt{sexM}\\) denotes Boy):\n\n\nCode\n# fit Model 2\nfit2 = lm(weight ~ age + sex, data=birthweight)\n# print the parameter estimates from Model 2\ncoefficients(fit2)\n\n\n(Intercept)         age        sexM \n -1773.3218    120.8943    163.0393 \n\n\nCode\n# perform an analysis of variance on the fitted model\nanova(fit2)\n\n\nAnalysis of Variance Table\n\nResponse: weight\n          Df  Sum Sq Mean Sq F value    Pr(>F)    \nage        1 1013799 1013799 32.3174 1.213e-05 ***\nsex        1  157304  157304  5.0145   0.03609 *  \nResiduals 21  658771   31370                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThus we have parameter estimates: \\(\\hat \\alpha = -1773.3\\), \\(\\hat\\beta = 120.9\\) and \\(\\hat\\gamma = 163.0\\), the latter being the effect of being a boy compared to the baseline category of being a girl.\nThe Analysis of Variance (ANOVA) table gives: \\(R_1-R_2 = 157304\\) with \\(r_1-r_2=1\\), and \\(R_2=658771\\) with \\(r_2=21\\). The \\(F_{12}\\) statistic, Equation 2.3, is then\n\\[\nF_{12} = \\frac{157304/1}{658771/21} = 5.0145,\n\\]\nwhich can be read directly from the ANOVA table in the column headed ‘F value’. Is \\(F_{12} = 5.01\\) in the upper tail of the \\(F_{1,21}\\) distribution?\nThe final column of the ANOVA table tells us that the probability of observing \\(F_{12} > 5.01\\) is only \\(0.03609\\) – this is called a p-value. The * beside this p-value highlights that its value lies between 0.01 and 0.05. This indicates that we should reject \\(H_0\\) in favour of \\(H_1\\) – there is evidence for the more complicated model. Thus we would conclude that the effect of the sex of the baby, after controlling for gestational age, is statistically significant in these data.\nTo complete the analysis, we should now compare \\(\\texttt{Model }2\\) with \\(\\texttt{Model }3\\) – see Exercises."
  },
  {
    "objectID": "2_linearmodels.html#types-of-normal-linear-model",
    "href": "2_linearmodels.html#types-of-normal-linear-model",
    "title": "2  Essentials of Normal Linear Models",
    "section": "2.2 Types of normal linear model",
    "text": "2.2 Types of normal linear model\nHere we consider how normal linear models can be set up for different types of explanatory variable. The dependent variable \\(y\\) is modelled as a linear combination of \\(p\\) explanatory variables \\(\\b{x} =(x_1, x_2,\\ldots, x_p)\\) plus a random error \\(\\epsilon \\sim N(0, \\sigma^2)\\), where ‘~’ means ‘is distributed as’. Several models are of this kind, depending on the number and type of explanatory variables. Table 2.2 lists some types of normal linear models with their explanatory variable types.\n\n\nTable 2.2: Types of normal linear model and their explanatory variable types where indicator function \\(I(x=j)=1\\) if \\(x=j\\) and \\(0\\) otherwise.\n\n\n\n\n\n\n\n\\(p\\)\nExplanatory variables\nModel\n\n\n1\nQuantitative\nSimple linear regression \\(y=\\alpha+\\beta x+\\epsilon\\)\n\n\n>1\nQuantitative\nMultiple linear regression\\(y=\\alpha+\\sum_{i=1}^p\\beta_i x_i+\\epsilon\\)\n\n\n1\nDichotomous (\\(x=1\\) or \\(2\\))\nTwo-sample t-test \\(y=\\alpha+\\delta ~ I(x=2)+\\epsilon\\)\n\n\n1\nPolytomous, \\(k\\) levels \\((x=1,\\ldots,k)\\)\nOne-way ANOVA\\(y=\\alpha+\\sum_{j=1}^k \\delta_j \\ I(x=j)+\\epsilon\\)\n\n\n>1\nQualitative\n\\(p\\)-way ANOVA\n\n\n\n\nFor the two-sample t-test model2, observations in the two groups have means \\(\\alpha+\\beta_1\\) and \\(\\alpha + \\beta_2\\) . Notice, however, that we have three parameters with only two group sample means and hence parameter estimation is not possible. To avoid this identification problem, we either impose a ‘corner’ constraint: \\(\\beta_1=0\\) and then \\(\\beta_2\\) represents the difference in the Group 2 mean relative to a baseline of Group \\(1\\). Alternatively, we may impose a ‘sum-to-zero’ constraint: \\(\\beta_1+ \\beta_2 =0\\), the values \\(\\beta_1=-\\beta_2\\) then give differences in the groups means relative to the overall mean. Table 2.3 shows the effect of the parameter constraint on the group means.\n\n\nTable 2.3: Parameters in the two-sample t-test model after imposing parameter constraint to avoid the identification problem.\n\n\nConstraint\nGroup 1 mean\nGroup 2 mean\n\n\n\n\n\\(\\beta_1=0\\)\n\\(\\alpha\\)\n\\(\\alpha+\\beta_2\\)\n\n\n\\(\\beta_1+\\beta_2=0\\)\n\\(\\alpha-\\beta_2\\)\n\\(\\alpha+\\beta_2\\)\n\n\n\n\nFor the general one-way ANOVA model with \\(k\\) groups, observations in Group \\(j\\) have mean \\(\\alpha + \\delta_j\\) , for \\(j =1, \\ldots, k\\) – that leads to \\(k + 1\\) parameters describing \\(k\\) group means. Again we can impose the ‘corner’ constraint: \\(\\delta_1 = 0\\) and then \\(\\delta_j\\) represents the difference in means between Group \\(j\\) and the baseline Group \\(1\\). Alternatively, we may impose a ‘sum-to-zero’ constraint:\\(\\sum_{j=1}^k \\delta_j =0\\) and again \\((\\delta_1, \\delta_2,\\dots,\\delta_k)\\) then represents an individual group effect relative to the overall data mean."
  },
  {
    "objectID": "2_linearmodels.html#matrix-representation-of-linear-models",
    "href": "2_linearmodels.html#matrix-representation-of-linear-models",
    "title": "2  Essentials of Normal Linear Models",
    "section": "2.3 Matrix representation of linear models",
    "text": "2.3 Matrix representation of linear models\nAll of the models in Table Table 2.2 can be fitted by least squares (OLS). To describe this, a matrix formulation will be most convenient:\n\\[\n\\mathbf{Y} = X\\boldsymbol{\\beta}+\\boldsymbol{\\epsilon}\n\\tag{2.4}\\]\nwhere\n\n\\(\\mathbf{Y}\\) is an \\(n\\times 1\\) vector of observed response values with \\(n\\) being the number of observations.\n\\(X\\) is an \\(n\\times p\\) design matrix, to be discussed below.\n\\(\\boldsymbol{\\beta}\\) is a \\(p\\times 1\\) vector of parameters or coefficients to be estimated.\n\\(\\boldsymbol{\\epsilon}\\) is an \\(n\\times 1\\) vector of independent and identically distributed (IID) random variables, which here \\(\\epsilon \\sim N(0, \\sigma^2)\\) and is called the “error” term."
  },
  {
    "objectID": "2_linearmodels.html#construction-of-the-design-matrix",
    "href": "2_linearmodels.html#construction-of-the-design-matrix",
    "title": "2  Essentials of Normal Linear Models",
    "section": "2.4 Construction of the design matrix",
    "text": "2.4 Construction of the design matrix\nCreating the design matrix is a key part of the modelling as it describes the important structure of investigation or experiment. The design matrix can be constructed by the following process.\n\nBegin with an \\(X\\) containing only one column: a vector of ones for the overall mean or intercept term (the \\(\\alpha\\) in Table 2.2).\nFor each explanatory variable \\(x_j\\), do the following:\n\nIf a variable \\(x_j\\) is quantitative, add a column to \\(X\\) containing the values of \\(x_j.\\)\nIf \\(x_j\\) is qualitative with \\(k\\) levels, add \\(k\\) “dummy” columns to \\(X\\), taking values 0 and 1, where a 1 in the \\(\\ell\\)th dummy column identifies that the corresponding observation is at level \\(\\ell\\) of factor \\(x_j\\) . For example, suppose we have a factor \\(\\mathbf{x}_j = (M, M, F, M, F)\\) representing the sex of \\(n = 5\\) individuals. This information can be coded into two dummy columns of \\(X\\):\n\n\\[\n\\begin{matrix}\n\\begin{matrix}\nF & M\n\\end{matrix}\\\\\n\\begin{bmatrix}\n0 & 1 \\\\\n0 & 1 \\\\\n1 & 0 \\\\\n0 & 1 \\\\\n1 & 0\n\\end{bmatrix}\n\\end{matrix}\n\\]\nWhen qualitative variables are present, \\(X\\) will be singular – that is, there will be linear dependencies between the columns of \\(X\\). For example, the sum of the two columns above is a vector of ones, the same as the intercept column. We resolve this identification problem by deleting some columns of \\(X\\). This is equivalent to applying the corner constraint \\(\\delta_1 = 0\\) in the one-way ANOVA.\nIn the above example, after removing a column, we get:\n\\[\n\\mathbf{X}=\\begin{bmatrix}\n1 &  1 \\\\\n1 &  1 \\\\\n1 &  0 \\\\\n1 &  1 \\\\\n1 &  0\n\\end{bmatrix}.\n\\]\nEach column of \\(X\\) represents either a quantitative variable, or a level of a qualitative variable. We will use \\(i = 1, \\ldots, n\\) to label the observations (rows of \\(X\\)) and \\(j = 1, \\ldots, p\\) to label the columns of \\(X\\).\n\n\n2.4.1 Example: Simple linear regression\nConsider the simple linear regression model \\(y=\\alpha+\\beta x+\\epsilon\\) with \\(\\epsilon \\sim N(0, \\sigma^2)\\). Given data on \\(n\\) pairs \\((x_i, y_i), i = 1, \\ldots, n\\), we write this as\n\\[\ny_i = \\alpha+\\beta x_i+\\epsilon_i, \\quad \\text{for } i=1,2,\\dots,n,\n\\tag{2.5}\\]\nwhere the \\(\\epsilon_i\\) are IID \\(N(0,\\sigma^2)\\). In matrix form, this becomes\n\\[\n\\mathbf{Y}=X\\boldsymbol{\\beta}+\\boldsymbol{\\epsilon}\n\\tag{2.6}\\] with \\[\n\\mathbf{Y}=\\begin{bmatrix}\ny_1\\\\\n\\vdots\\\\\ny_n\n\\end{bmatrix},\n%\n\\hspace{5mm}\n%\nX=\\begin{bmatrix}\n1 & x_1\\\\\n\\vdots & \\vdots\\\\\n1 & x_n\n\\end{bmatrix},\n%\n\\hspace{5mm}\n%\n\\boldsymbol{\\beta}=\n\\begin{bmatrix}\n\\beta_1\\\\\n\\beta_2\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\alpha\\\\\n\\beta\n\\end{bmatrix},\n%\n\\hspace{5mm}\n%\n\\boldsymbol{\\epsilon}=\n\\begin{bmatrix}\n\\epsilon_1\\\\\n\\vdots\\\\\n\\epsilon_n\n\\end{bmatrix}.\n\\] The \\(i\\)th row of Equation 2.6 has the same meaning as Equation 2.5: \\[\ny_i = 1\\times \\beta_1 + x_i\\times \\beta_2 +\\epsilon_i = \\alpha+\\beta x_i +\\epsilon_i,  \\hspace{2mm} \\text{for } i=1,2,\\dots,n.\n\\]\n\n\n2.4.2 Example: One-way ANOVA\nFor one-way ANOVA with \\(k\\) levels, the model is \\[\ny_i =\\alpha+\\sum_{j=1}^k \\delta_j \\ I(x_i=j)+\\epsilon_i, \\quad \\text{for } i=1, 2, \\dots,n,\n\\] where \\(x_i\\) denotes the group level of individual \\(i\\). So if \\(y_i\\) is from the \\(j\\)th group then \\(y_i \\sim N(\\alpha+\\delta_j, \\sigma^2)\\). Here \\(\\alpha\\) is the intercept and the \\((\\delta_1, \\delta_2, \\dots,\\delta_k)\\) represent the “main effects”.\nWe can store the information about the levels of \\(g\\) in a dummy matrix \\(X^* = (x^*_{ij})\\) where\n\\[\nx^*_{ij} = \\left\\{\n\\begin{array}{cl}\n1, & g_i=j,\\\\\n0, & \\text{otherwise.}\n\\end{array}\n\\right.\n\\]\nThen set \\(X = [1, X^*]\\), where \\(1\\) is an \\(n\\)-vector of \\(1\\)’s. For the male–female example at (1.12), we have \\(n = 5\\) and a sex factor:\n\\[\ng=\\begin{bmatrix}1\\\\ 1 \\\\2\\\\1\\\\2\\end{bmatrix},\n%\n\\hspace{5mm}\n%\nX=\\begin{bmatrix}\n1 & 1 & 0\\\\\n1&1& 0 \\\\ 1& 0 & 1\\\\ 1& 1 & 0 \\\\ 1& 0 & 1\n\\end{bmatrix},\n%\n\\hspace{5mm}\n%\n\\beta=\\begin{bmatrix}\\beta_1\\\\\\beta_2\\\\\\beta_3\\end{bmatrix}\n=\\begin{bmatrix}\\alpha\\\\\\delta_1 \\\\\\delta_2\\end{bmatrix}.\n\\]\nThen the \\(i\\)th row of \\(X\\) becomes \\(\\beta_1 + \\beta_2 = \\alpha + \\delta_1\\) if \\(g_i = 1\\) and \\(\\beta_1 + \\beta_3 = \\alpha + \\delta_2\\) if \\(g_i\\) = 2. That is, the \\(i\\)th row of \\(X\\) is\n\\[\n\\alpha+\\sum_{j=1}^2 \\delta_j I(g_i=j)\n\\] so this model can be written \\(Y=X\\beta+\\epsilon\\). Here, \\(X\\) is singular: its last two columns added together equal its first column. Statistically, the problem is that we are trying to estimate two means (the mean response for Boys and the mean response for girls) with three parameters (\\(\\alpha\\), \\(\\delta_2\\) and \\(\\delta_2\\)).\nIn practice, we often resolve this aliasing or identification problem by setting one of the parameters to be zero, that is \\(\\delta_1 = 0\\), which corresponds to deleting the second column of \\(X\\))."
  },
  {
    "objectID": "2_linearmodels.html#model-shorthand-notation",
    "href": "2_linearmodels.html#model-shorthand-notation",
    "title": "2  Essentials of Normal Linear Models",
    "section": "2.5 Model shorthand notation",
    "text": "2.5 Model shorthand notation\nIn R, a qualitative (categorical) variable is called a factor, and its categories are called levels. For example, variable \\(\\texttt{Sex}\\) in the birthweight data (above) has levels coded “M” for ‘Boy’ and “F” for ‘Girl’. It may not be obvious to R whether a variable is quantitative or qualitative. For example, a qualitative variable called \\(\\texttt{Grade}\\) might have categories 1, 2 and 3. If was included in a model, R would treat it as quantitative unless we declare it to be a factor, which we can do with the command:\n\\(\\texttt{grade = as.factor(grade)}\\)\nA convenient model-specification notation has been developed from which the design matrix \\(X\\) can be constructed. Below, \\(E, F, \\ldots\\) denote generic quantitative (continuous) or qualitative (categorical) variables. Terms in this notation may take the following forms:\n\n\\(1\\) : a column of 1’s to accommodate an intercept term (the \\(\\alpha\\)’s of Table 2.2 ). This is included in the model by default.\n\\(E\\) : variable \\(E\\) is included in the model. The design matrix includes \\(k_E\\) columns for \\(E\\). If \\(E\\) is quantitative,\\(k_E = 1\\). If E is qualitative, \\(k_E\\) is the number of levels of \\(E\\) minus 1.\n\\(E +F\\) : both \\(E\\) and \\(F\\) are included the model. The design matrix includes \\(k_E +k_F\\) columns accordingly.\n\\(E : F\\) (sometimes \\(E \\cdot F\\)) : the model includes an interaction between \\(E\\) and \\(F\\); each column that would be included for \\(E\\) is multiplied by each column for \\(F\\) in turn. The design matrix includes \\(k_E \\times k_F\\) columns accordingly.\n\\(E * F\\) : shorthand for \\(1 + E + F + E : F\\): useful for crossed models where \\(E\\) and \\(F\\) are different factors. For example, \\(E\\) labels age groups; \\(F\\) labels medical conditions.\n\\(E/F\\) : shorthand for \\(1 + E + E : F\\): useful for nested models where \\(F\\) is a factor whose levels have meaning only within levels of factor \\(E\\). For example, \\(E\\) labels different hospitals; \\(F\\) labels wards within hospitals.\n\\(\\text{poly}(E; \\ell)\\) : shorthand for an orthogonal polynomial, wherein \\(x\\) contains a set of mutually orthogonal columns containing polynomials in \\(E\\) of increasing order, from order \\(1\\) through order \\(\\ell\\).\n\\(-E\\) : shorthand for removing a term from the model; for example \\(E * F -E\\) is short for \\(1 + F + E : F\\).\n\\(I()\\) : shorthand for an arithmetical expression (not to be confused with the indicator function of equation (1.10)). For example, \\(I(E + F)\\) denotes a new quantitative variable constructed by adding together quantitative variables \\(E\\) and \\(F\\). This would cause an error if either \\(E\\) or \\(F\\) has been declared as a factor. What would happen in this example if we omitted the \\(I(\\dot)\\) notation?\n\nThe notation uses “~” as shorthand for “is modelled by” or “is regressed on”. For example,\n\nWeight is regressed on age-group and sex with no interaction between them: \\[\n\\texttt{Weight} \\sim \\texttt{Age} + \\texttt{Sex}\n\\] as for the birthweight data in Figure 1.2c.\nWell being is regressed on age-group and income-group, where income is thought to affect wellbeing differentially by age: \\[\n\\texttt{Wellbeing} \\sim \\texttt{Age} * \\texttt{Income}\n\\]\nClass of degree is regressed on school of the university and on degree subject within the school: \\[\n\\texttt{DegreeClass} \\sim \\texttt{School/Subject}\n\\]\nYield of wheat is regressed on seed-variety and annual rainfall: \\[\n\\texttt{Yield} \\sim \\texttt{Variety} + \\texttt{poly}(\\texttt{Rainfall}, 2)\n\\]\nProfit is regressed on amount invested: \\[\n\\texttt{Profit}\\sim \\texttt{Investment}- 1\n\\] (no intercept term, that is a regression through the origin).\n\nSee Handout 4 for material on intrinsic aliasing to deal with singularity problem."
  },
  {
    "objectID": "2_linearmodels.html#exercises",
    "href": "2_linearmodels.html#exercises",
    "title": "2  Essentials of Normal Linear Models",
    "section": "2.6 Exercises",
    "text": "2.6 Exercises\n2.1. An extra model which could have been considered for the Birthweight data example would be one that says that Weight is different for girls and boys, but does not depend on gestational age.\nWrite down the equation corresponding to this model. Then, load the birthweight data into RStudio and fit the model. How are the fitted model parameters related to the overall birthweight mean and the mean birthweights of the girls and boys? Is this a good fit to the data? Is Sex statistically significant?\n2.2. In an experiment to investigate Ohm’s Law, \\(V=IR\\) where \\(V\\) is Voltage, I is current and \\(R\\) is resistance of the material, the following data3 were recorded:\n\n\nTable 2.4: Experimental verification of Ohm’s Law\n\n\nVoltage (Volts)\n4\n8\n10\n12\n14\n18\n20\n24\n\n\nCurrent (mAmps)\n11\n24\n30\n36\n40\n53\n58.5\n70\n\n\n\n\nDoes this data support Ohm’s Law? What is the resistance of the material used?\n\n\n\n\n\n\nWarning\n\n\n\nWarning of potentially sensitive material.\n\n\n2.3. Data4 were recording on the number of deaths due to breast cancer in 301 counties in the US states of North Carolina, South Carolina and Georgia between 1950 and 1960. Also recorded was the adult white female population for each county. A copy of the data is available in the file Breast_Cancer.txt."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH3823 Generalized Linear Models",
    "section": "",
    "text": "\\[\n\\def\\b#1{\\mathbf{#1}}\n\\]\n\n\nWeekly schedule\n\n\n\n\n\n\nImportant\n\n\n\nOur regular class times are:\n           Tuesday 12-13, Roger Stevens, LT25\n           Thursday 2-3, Roger Stevens, LT23\n\n\n\n\n\n\n\n\nWeek 1 (30 January - 3 February)\n\n\n\n\nBefore next Lecture: Please read the Overview.\nLecture on Tuesday: We will briefly cover all material in Chapter 1: Introduction.\nBefore next Lecture: Please re-read Chapter 1 carefully.\nLecture on Thursday: Start Chapter 2: Essentials of Normal Linear Models with Section 2.1: Overview.\nWeekly feedback: Self-study the Exercises in Section 1.5 – solutions to be posted during Week 1.\n\n\n\n\n\n\n\n\n\nWeek 2 (6 - 10 February)\n\n\n\n\nDetails will be added during Week 1.\n\n\n\n\n\n\n\n\n\nCoursework Practical Sessions (20 - 24 March)\n\n\n\n\nDetails to follow in early March."
  },
  {
    "objectID": "1_intro.html",
    "href": "1_intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "\\[\n\\def\\b#1{\\mathbf{#1}}\n\\]"
  }
]