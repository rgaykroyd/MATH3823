[
  {
    "objectID": "5_logisticmodel.html#introduction",
    "href": "5_logisticmodel.html#introduction",
    "title": "5  Modelling proportions",
    "section": "5.1 Introduction",
    "text": "5.1 Introduction\nIn this chapter we will focus on applications of generalized linear modelling where the response variable follows a binomial distribution. This can arise when the outcome is binary, that is it can take one of only two possible values, or is the sum of a set of such binary outcomes. These two outcomes record whether some event of interest has occurred or not. In the simplest case, the response variable, \\(B\\) say, is defined as \\[\nB =\n\\begin{cases}\n1 & \\text{if the event has occurred} \\\\\n0 & \\text{if the event has not occurred}\n\\end{cases}\n\\tag{5.1}\\] and we set \\(Pr(B=1)=p\\), and hence \\(Pr(B=0)=1-p\\). This is, of course, the definition of a Bernoulli trial leading to a Bernoulli random variables, \\(B\\sim \\text{Bernoulli}(p)\\).\nSuppose now that there are \\(m\\) similar binary outcomes, \\(B_i, i=1,\\dots,m\\) with \\(Pr(B_i=1)=p\\), and that the total number of times that the event occurred is recorded as the response \\(Y\\). Assuming that \\(m\\) is known before the trials start, that \\(p\\) is fixed and that the individual Bernoulli trials are independent then \\(Y\\) follows a binomial distribution, \\(Y\\sim \\text{B}(m, p)\\) with probability mass function \\[\nf(y) = {m \\choose y} p^y(1-p)^{m-y}.\n\\tag{5.2}\\] Note that the binomial random variable can be thought of as the sum of i.i.d Bernoulli random variables, \\(Y= B_1+\\cdots + B_m\\), if that is helpful.\nThe Binomial distribution \\(\\text{B}(m, p)\\) is often described in terms of success and failure and a binomial distribution in terms of the number of successes in \\(m\\) independent trials, where \\(p\\) is the probability of success in each trial. The term success need not correspond to a favourable outcome; it is merely the language traditionally used in connection with this model. For example, success might correspond to death.\nOf course, the special case with \\(m=1\\) reduces to the Bernoulli distribution. Further, if \\(Y_1 \\sim \\text{B}(m_1,p)\\) and independently \\(Y_2 \\sim \\text{B}(m_2,p)\\), then \\(Y_1+Y_2\\) also follows a binomial distribution, \\(\\text{B}(m_1+m_2,p)\\) – note that is only valid when \\(p\\) is common.\nRecall that the binomial can be re-written in the exponential family form of Equation 3.3 with \\[\nf(y)  = \\exp \\left\\{ y\\ \\mbox{logit } p  + m \\log (1-p) + \\log {m \\choose y}\\right\\},\n\\] with natural or canonical parameter \\(\\theta=\\text{logit} \\ p = \\log \\left( p(1-p)\\right),\\), scale parameter \\(\\phi=1\\), \\(b(\\theta)=m\\log(1+e^\\theta)\\) and \\(c(y,\\phi)=\\log{m\\choose y}\\) as in Table 3.3."
  },
  {
    "objectID": "5_logisticmodel.html#sec-linearlogistic",
    "href": "5_logisticmodel.html#sec-linearlogistic",
    "title": "5  Modelling proportions",
    "section": "5.2 The linear logistic model",
    "text": "5.2 The linear logistic model\nThroughout this module we have assumed that a response variable, \\(Y\\), depends on a set of explanatory variables, \\(\\mathbf{x} = \\{x_1,\\dots, x_p\\}\\). In particular, for binomial counts from \\(\\text{B}(m,p)\\), \\(0<p<1\\) with mean \\(\\mu=mp\\) we set the link function, \\(g(\\mu)\\), equal to the linear predictor \\(\\eta = \\sum \\beta_j x_j\\) and hence have \\[\ng(\\mu) = \\sum_{j=1}^p \\beta_j x_j = \\mathbf{x}^T \\boldsymbol{\\beta}\n\\] where \\(\\boldsymbol{\\beta} = \\{\\beta_1,\\dots,\\beta_p\\}\\) are the linear predictor parameters. Recall that the canonical logit link Equation 3.14 for the binomial leads to the systematic part of the model \\[\n\\text{logit}(p) = \\log \\left( \\frac{p}{1-p}\\right) = \\mathbf{x}^T \\boldsymbol{\\beta}\n\\tag{5.3}\\] but that other links function are possible, such as the probit, Equation 3.15, the complementary log-log, Equation 3.16, and the cauchit Equation 3.17.\nThis model can alternatively be written \\[\nY\\ \\sim \\text{B}(m,p), \\quad \\text{where }\np=\\frac{\\exp \\{\\mathbf{x}^T \\boldsymbol{\\beta}\\}}{1+\\exp\\{ \\mathbf{x}^T \\boldsymbol{\\beta}\\}}\n\\tag{5.4}\\] which makes the dependence of \\(Y\\) on \\(\\mathbf{x}\\), and \\(\\boldsymbol{\\beta}\\), more explicit.\nIn general, we might want to consider \\(n\\) independent binomial random variables representing subgroups of the sample with \\(Y_1\\sim \\text{B}(m_1,p_1), \\dots Y_n\\sim \\text{B}(m_n,p_n)\\), see Table 5.1 and hence \\[\n\\mathbf{Y}\\ \\sim \\text{B}(\\mathbf{m},\\mathbf{p}), \\quad \\text{and }\n\\mathbf{p}=\\frac{\\exp \\{X \\boldsymbol{\\beta}\\}}{1+\\exp\\{ X \\boldsymbol{\\beta}\\}},\n\\] with response \\(\\mathbf{Y}=\\{Y_1,\\dots, Y_n\\}\\), \\(\\mathbf{m}=\\{m_1,\\dots, m_n\\}\\), \\(\\mathbf{p}=\\{p_1,\\dots, p_n\\}\\), \\(X\\) being the \\(n \\times p\\) design matrix and \\(\\boldsymbol{\\beta} = \\{\\beta_1,\\dots,\\beta_p\\}\\) the linear predictor parameters.\n\n\nTable 5.1: Linear logistic model\n\n\n\nGroup 1\nGroup 2\n\\(\\dots\\)\nGroup \\(n\\)\n\n\n\n\nSuccesses\n\\(Y_1\\)\n\\(Y_2\\)\n\\(\\dots\\)\n\\(Y_n\\)\n\n\nFailures\n\\(m_1-Y_1\\)\n\\(m_2-Y_2\\)\n\\(\\dots\\)\n\\(m_n-Y_n\\)\n\n\nTotal\n\\(m_1\\)\n\\(m_2\\)\n\\(\\dots\\)\n\\(m_n\\)\n\n\n\n\nThen we can write down the log likelihood of \\(\\boldsymbol{\\beta}\\) using Equation 4.12 and Equation 5.2 as \\[\nl(\\boldsymbol{\\beta}; \\mathbf{y})\n=\n\\sum_{i=1}^n\n\\left\\{\ny_i \\, \\log (p_i) + (m_i-y_i) \\log (1-p_i) + \\log {m_i \\choose y_i}\n\\right\\}\n\\tag{5.5}\\] where \\[\np_i=\\frac{\\exp \\{\\mathbf{x}_i^T \\boldsymbol{\\beta}\\}}{1+\\exp\\{ \\mathbf{x}_i^T \\boldsymbol{\\beta}\\}}.\n\\] We would then use the principle of maximum likelihood to estimate \\(\\boldsymbol{\\beta}\\) \\[\n\\hat{\\boldsymbol{\\beta}} = \\text{arg} \\max_{\\boldsymbol{\\beta}} \\,\nl(\\boldsymbol{\\beta}; \\mathbf{y}).\n\\] Then, the fitted probability are given by \\[\n\\hat{p}_i=\\frac{\\exp \\{\\mathbf{x}_i^T \\hat{\\boldsymbol{\\beta}}\\}}{1+\\exp\\{ \\mathbf{x}_i^T \\hat{\\boldsymbol{\\beta}}\\}}\n\\] and corresponding fitted values \\[\n\\hat y_i = m_i \\, \\hat{p}_i.\n\\]\nThe Pearson residuals for binomial data take the form\n\\[\ne^P_i  =\n\\frac{y_i - m_i\\hat{p}_i}{\\sqrt{m_i \\hat{p}_i (1 - \\hat{p}_i)}} .\n\\] From the general result \\(\\text{Var}(Y_i)=\\phi b''(\\theta)\\) in Proposition 3.1, it can be shown that\\(\\text{Var}(Y_i)=m_i {p}_i (1 - {p}_i)\\). Then, using the estimate of \\(p_i\\) leads to the denominator above.\nFor large \\(m_i\\), the usual Normal approximation to the Binomial means that the Pearson residuals are approximately \\(N(0, 1)\\) distributed.\nIt can be shown that the deviance for the fitted model is \\[\nD =\n2 \\sum_{i=1}^n \\left\\{ y_i \\log \\left( \\frac{y_i} {m_i \\hat{p}_i} \\right) +\n(m_i - y_i) \\log \\left( \\frac{m_i - y_i}{m_i (1 - \\hat{p}_i)} \\right)\n\\right\\},\n\\tag{5.6}\\] which is approximately \\(\\chi^2_{n-r}\\) distributed if the model is correct, where \\(r\\) is the number of degrees of freedom in the model (i.e. the number of columns of the design matrix).\nThis formula can be shown to be equivalent to \\[\nD = 2 \\sum_{j=1}^2 \\sum_{i=1}^n O_{ji} \\log\n\\frac{O_{ji}}{E_{ji}}\n\\] where \\(O_{ji}\\) denotes the observed value and \\(E_{ji}\\) denotes the expected value in cell \\((j,i)\\) of the \\(2 \\times n\\) table of successes and failures:\n\nObserved and expected frequencies in the linear logistic model\n\n\n\nGroup 1\nGroup 2\n\\(\\dots\\)\nGroup \\(n\\)\n\n\n\n\nSuccesses, \\(j=1\\)\n\\(O_{11}\\)\n\\(O_{12}\\)\n\\(\\dots\\)\n\\(O_{1n}\\)\n\n\n\n\\(E_{11}\\)\n\\(E_{12}\\)\n\\(\\dots\\)\n\\(E_{1n}\\)\n\n\nFailures, \\(j=2\\)\n\\(O_{21}\\)\n\\(O_{22}\\)\n\\(\\dots\\)\n\\(O_{2n}\\)\n\n\n\n\\(E_{21}\\)\n\\(E_{22}\\)\n\\(\\dots\\)\n\\(E_{2n}\\)\n\n\n\nAnother goodness-of-fit statistic is the Pearson chi-squared statistic: \\[\nX^2 = \\sum_{j=1}^2 \\sum_{i=1}^n \\frac{(O_{ji} - E_{ji})^2}{E_{ji}}.\n\\] This is asymptotically equivalent to the deviance Equation 5.6 (proof is by Taylor series expansion; omitted). Thus, asymptotically, \\(X^2\\) is also approximately \\(\\chi^2_{n-r}\\) distributed. Both approximations can be poor if the expected frequencies are small, but \\(X^2\\) copes slightly better with this problem. See Dobson, p.136 for more details."
  },
  {
    "objectID": "5_logisticmodel.html#overdispersion",
    "href": "5_logisticmodel.html#overdispersion",
    "title": "5  Modelling proportions",
    "section": "5.3 Overdispersion",
    "text": "5.3 Overdispersion\nExamination of residuals and deviances may indicate that a model is not an adequate fit to the data. One possible reason is overdispersion. This can occur for any error distribution where the variance is linked to the mean — e.g. Binomial, Poisson. In the binomial case, overdispersion is called extra-Binomial variation.\nRecall that if \\(Y_i \\sim \\text{Bin}(m_i, p_i)\\), \\(\\text{Var}(Y_i) = m_i p_i (1 - p_i)\\). Overdispersion occurs if observations which have been modelled by a \\(\\text{Bin}(m_i, \\hat{p}_i)\\) distribution have substantially greater variation than \\(m_i \\hat{p}_i (1 - \\hat{p}_i)\\). This will lead to a value of \\(D\\) substantially greater than the expected value of \\(n - r\\). This can occur if the model is missing appropriate explanatory variables or has the wrong link function, or if the \\(Y_i\\) are not independent.\nOne solution is to include an extra parameter \\(\\tau\\) in the model so that \\(\\text{Var}(Y_i) = \\tau \\times m_i p_i (1 - p_i)\\). For more details, see Section 7.7 of Dobson or Chapter 6 of Collett (1991) Modelling Binary data, Chapman & Hall.\nThe \\(\\texttt{glm}\\) function in R allows for extra-Binomial variation by setting \\(\\texttt{family=quasibinomial()}\\) with the usual link functions available."
  },
  {
    "objectID": "5_logisticmodel.html#application-to-doseresponse-experiments",
    "href": "5_logisticmodel.html#application-to-doseresponse-experiments",
    "title": "5  Modelling proportions",
    "section": "5.4 Application to dose–response experiments",
    "text": "5.4 Application to dose–response experiments\nA variable dose of some reagent is administered to each study subject, and the occurrence of a specific response is recorded. This is a dose–response experiment, one of the first uses of regression models for Bernoulli (or Binomial) responses.\nFor example, the Table 5.2 (including the information from Table 1.1) gives the number of beetles killed \\(y_i\\) and the number not killed \\((m_i-y_i)\\) out of a total number \\(m_i\\) that were exposed to a dose \\(x_i\\) of gaseous carbon disulphide, for \\(n=8\\) dose levels \\(i=1,\\dots,8\\) (Dobson: pp.109 in 1st edn; pp.119 in 2nd edn; pp.127 in 3rd edn). The proportion killed \\(p_i = y_i/m_i\\) at each dose level \\(i\\) is also shown in Table 5.2 and plotted in Figure 5.1.\n\n\nTable 5.2: Numbers of beetles killed by five hours of exposure to 8 different concentrations of gaseous carbon disulphide\n\n\n\n\n\n\n\n\n\nDose\n\\(x_i\\)\nNo. of beetle\n\\(m_i\\)\nNo. killed\n\\(y_i\\)\nNo. not killed\n\\(m_i-y_i\\)\nProportion\n\\(p_i=y_i/m_i\\)\n\n\n\n\n1.6907\n59\n6\n53\n0.10\n\n\n\n\n1.7242\n60\n13\n47\n0.22\n\n\n\n\n1.7552\n62\n18\n44\n0.29\n\n\n\n\n1.7842\n56\n28\n28\n0.50\n\n\n\n\n1.8113\n63\n52\n11\n0.83\n\n\n\n\n1.8369\n59\n53\n6\n0.90\n\n\n\n\n1.8610\n62\n61\n1\n0.98\n\n\n\n\n1.8839\n60\n60\n0\n1.00\n\n\n\n\n\n\n\n\nCode\nbeetle = read.table(\"https://rgaykroyd.github.io/MATH3823/Datasets/beetle.txt\", header=T)\n\ndose = beetle$dose\nmortality = beetle$died/beetle$total\n\nplot(dose, mortality, pch=16,\n     xlim=c(1.65, 1.90), xlab =\"Dose\",\n     ylim=c(-0.1, 1.1),  ylab=\"Mortality\")\nabline(h=c(0,1), lty=2)\n\ny = cbind(beetle$died, beetle$total-beetle$died)\nglm.fit = glm(y ~ dose, family=binomial(link='logit'))\n\noutput.dose = seq(1.6,1.95,0.001)\nfitted = predict(glm.fit, data.frame(dose=output.dose), type=\"response\")\nlines(output.dose, fitted)\n\n\n\n\n\nFigure 5.1: Beetle mortality rates with fitted dose-response curves.\n\n\n\n\nNow Equation 5.4 motivates modelling the beetle data as \\[\nY_i \\sim \\text{B}(m_i,p_i), \\quad \\mbox{for } i=1,\\dots, n=8\n\\] where \\[\n\\eta_i = \\alpha + \\beta x_i\n\\] and so \\[\np_i=\\frac{\\exp\\{\\alpha+\\beta x_i\\}}{(1+\\exp\\{\\alpha+\\beta x_i\\})}.\n\\]\nTo fit this model in R, a matrix with columns containing the numbers killed \\(y_i\\) and the numbers not killed \\(m_i-y_i\\) is first calculated\n\n\nCode\ny = cbind(beetle$died, beetle$total-beetle$died)\n\nhead(y)\n\n\n     [,1] [,2]\n[1,]    6   53\n[2,]   13   47\n[3,]   18   44\n[4,]   28   28\n[5,]   52   11\n[6,]   53    6\n\n\nwhich is then used in the \\(\\texttt{glm}\\) command\n\n\nCode\nglm.fit = glm(y ~ dose, family=binomial)\n\n\nThe model parameter estimates are given by\n\n\nCode\ncoefficients(glm.fit)\n\n\n(Intercept)        dose \n  -60.71745    34.27033 \n\n\nand hence\n\n\n\n\\[\n\\hat p_i = \\frac{\\exp\\{-60.7 + 34.3 x_i\\}}{(1+\\exp\\{ -60.7 + 34.3x_i\\})}.\n\\]\nFurther, the deviance if given by\n\n\nCode\ndeviance(glm.fit)\n\n\n[1] 11.23223\n\n\nor more helpfully a full summary, which contains parameter estimates and deviance values using\n\n\nCode\nsummary(glm.fit)\n\n\n\nCall:\nglm(formula = y ~ dose, family = binomial)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.5941  -0.3944   0.8329   1.2592   1.5940  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -60.717      5.181  -11.72   <2e-16 ***\ndose          34.270      2.912   11.77   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 284.202  on 7  degrees of freedom\nResidual deviance:  11.232  on 6  degrees of freedom\nAIC: 41.43\n\nNumber of Fisher Scoring iterations: 4\n\n\nor the anova table\n\n\nCode\nanova(glm.fit)\n\n\nAnalysis of Deviance Table\n\nModel: binomial, link: logit\n\nResponse: y\n\nTerms added sequentially (first to last)\n\n     Df Deviance Resid. Df Resid. Dev\nNULL                     7    284.202\ndose  1   272.97         6     11.232\n\n\nClearly, many of the results are repeated in the various R output.\nSummary of results:\nFrom the output, we can first test if the Null model, which contains only a constant term, is a good fit to the data using the hypotheses: \\[\nH_0: \\mbox{Null mode is true} \\mbox{ against } H_1: \\mbox{Null mode is false}.\n\\] From the output, note that the Null deviance is \\(D_0=284.202\\) and that this model has \\(r_0=1\\) parameters. Therefore, \\(D_0\\) follows a \\(\\chi^2\\) distribution with \\(n-r_0=8-1=7\\) degrees of freedom and hence the p-value is:\n\n\nCode\npchisq(284.202, 7, lower.tail = FALSE)\n\n\n[1] 1.425247e-57\n\n\nThis means that we reject \\(H_0\\) as the observed value of \\(D\\) is in the upper tail of the \\(\\chi^2\\) distribution.\nIf we had obtained a non-significant result, then we would stop the analysis and conclude that the Null model is an adequate fit.\nNext, consider testing the model which includes the explanatory variable where the the hypotheses are \\[\nH_0: \\mbox{Null model is true} \\mbox{ against } H_1: \\mbox{Model with dose is true}.\n\\] From the output, the deviance of the proposed model is \\(D_1=11.232\\) with \\(r_1=2\\) parameters. The test statistics is then \\(D_0-D_1=284.202-11.232=272.97\\) which follows a \\(\\chi^2\\) distribution with \\(r_1-r_0=2-1=1\\) degrees of freedom. The p-value is then\n\n\nCode\npchisq(272.97, 1, lower.tail = FALSE)\n\n\n[1] 2.556369e-61\n\n\nand we reject \\(H_0\\) in favour of \\(H_1\\) and conclude that dose is important.\nWe can finish the testing by checking if this model is a good fit to the data. That is, \\[\nH_0: \\mbox{The model with dose is true} \\mbox{ against } H_1: \\mbox{The model with dose is false}\n\\] The deviance of this is \\(D_1=11.232\\) with \\(r_1=2\\) parameters which follows a \\(\\chi^2\\) distribution with \\(n-r_1=8-2=6\\) degrees of freedom and has p=value\n\n\nCode\npchisq(11.232, 6, lower.tail = FALSE)\n\n\n[1] 0.08146544\n\n\nwhich means that we accept \\(H_0\\) at the 5% level and conclude that this model is an adequate fit to the data.\nFinally, we can look at the residuals:\n\n\nCode\nfit.resids = residuals(glm.fit, type=\"deviance\")\n\nplot(dose, fit.resids, pch=16)\nabline(h=0, lty=2)\n\n\n\n\n\nThese show a very slight u-shaped pattern and hence it would be work exploring fitting using the other link functions. For this data set, the complementary log-log (cloglog) link has a slightly better fit, see Figure 3.4, but a slight pattern remains.\nBefore finishing, suppose that we wish to predict, by hand, the probability for dose level \\(x_0=1.85\\), say, and the expected number of beetles killed when \\(m_0=60\\) beetles, say, are exposed.\nThe probability is predicted as \\[\n\\hat p_i = \\frac{\\exp\\{-60.7 + 34.3 x_0\\}}{(1+\\exp\\{ -60.7 + 34.3x_0\\})} = 0.936\n\\] and the expected number of beetles \\[\nm_0 \\, \\hat p_i =  56.2\n\\] Finally, suppose that we wish to find the minimum dose which kills 95% of the beetles, that is \\(p_0=0.95\\) which requires us to invert the logistic equation.\nIn general, suppose we wish to find \\(x_p\\) which corresponds to proportion \\(p\\) then \\[\nx_p= \\frac{1}{\\beta} \\left\\{ \\log\\left(\\frac{p}{1-p}\\right)-\\alpha \\right\\}\n\\] and hence here \\(\\hat x_p= 1.856\\)."
  },
  {
    "objectID": "5_logisticmodel.html#exercises",
    "href": "5_logisticmodel.html#exercises",
    "title": "5  Modelling proportions",
    "section": "5.5 Exercises",
    "text": "5.5 Exercises\nTo be added later."
  },
  {
    "objectID": "0_preface.html",
    "href": "0_preface.html",
    "title": "Overview",
    "section": "",
    "text": "Official Module Description"
  },
  {
    "objectID": "0_preface.html#preface",
    "href": "0_preface.html#preface",
    "title": "Overview",
    "section": "Preface",
    "text": "Preface\nThese lecture notes are produced for the University of Leeds module MATH3823 - Generalized Linear Models for the academic year 2022-23. Please note that this material also forms part of the module MATH5824 - Generalized Linear and Additive Models. They are based on those used previously for this module and I am grateful to previous module lecturers for their considerable effort: Lanpeng Ji, Amanda Minter, John Kent, Wally Gilks, and Stuart Barber. This is the first year, however, that they have been produced in accessible format and hence some errors might occur during this conversion process. For information, I am using Quarto (a successor to RMarkdown) from RStudio to produce both the html and PDF, and then GitHub to create the website which can be accessed at rgaykroyd.github.io/MATH3823/. Please note that the PDF versions will only be made available on the University of Leeds Minerva system. Although I am a long-term user of RStudio, I have not previously used Quarto/RMarkdown nor Github and hence please be patient if there are hitches along the way.\nRG Aykroyd, Leeds, November 22, 2022\n\n\n\n\n\n\n\n\nWarning\n\n\n\nStatistical ethics and sensitive data\nPlease note that from time to time we will be using data sets from situations which some might perceive as sensitive. All such data sets will, however, be derived from real-world studies which appear in textbooks or in scientific journals. The daily work of many statisticians involves applying their professional skills in a wide variety of situations and as such it is important to include a range of commonly encountered examples in this module. Whenever possible, sensitive topics will be signposted in advance. If you feel that any examples may be personally upsetting then, if possible, please contact the module lecturer in advance. If you are significantly effected by any of these situations, then you can seek support from the Student Counselling and Wellbeing service."
  },
  {
    "objectID": "0_preface.html#module-summary",
    "href": "0_preface.html#module-summary",
    "title": "Overview",
    "section": "Module summary",
    "text": "Module summary\nLinear regression is a tremendously useful statistical technique but is very limited. Generalised linear models extend linear regression in many ways - allowing us to analyse more complex data sets. In this module we will see how to combine continuous and categorical predictors, analyse binomial response data and model count data."
  },
  {
    "objectID": "0_preface.html#objectives",
    "href": "0_preface.html#objectives",
    "title": "Overview",
    "section": "Objectives",
    "text": "Objectives\nOn completion of this module, students should be able to:\n\ncarry out regression analysis with generalised linear models including the use of link functions;\nunderstand the use of deviance in model selection;\nappreciate the problems caused by overdispersion;\nfit and interpret the special cases of log linear models and logistic regression;\nuse a statistical package with real data to fit these models to data and to write a report giving and interpreting the results."
  },
  {
    "objectID": "0_preface.html#syllabus",
    "href": "0_preface.html#syllabus",
    "title": "Overview",
    "section": "Syllabus",
    "text": "Syllabus\nGeneralised linear model; probit model; logistic regression; log linear models."
  },
  {
    "objectID": "0_preface.html#university-module-catalogue",
    "href": "0_preface.html#university-module-catalogue",
    "title": "Overview",
    "section": "University Module Catalogue",
    "text": "University Module Catalogue\nFor any further details, please see MATH3823 Module Catalogue page"
  },
  {
    "objectID": "1_intro.html#overview",
    "href": "1_intro.html#overview",
    "title": "1  Introduction",
    "section": "1.1 Overview",
    "text": "1.1 Overview\nIn previous modules you have studied linear models with a normally distributed error term, such as simple linear regression, multiple linear regression and ANOVA for normally distributed observations. In this module we will study generalized linear models.\nOutline of the module:\n\nRevision of linear models with normal errors.\nIntroduction to generalized linear models, GLMs.\nLogistic regression models.\nLoglinear models, including contingency tables.\n\n\n\n\n\n\n\nImportant\n\n\n\nThis module will make extensive use of \\(\\mathbf{R}\\) and hence it is very important that you are comfortable with its use. If you need some revision, then material is available on Minerva under RStudio Support.\n\n\nThe purpose of a generalized linear model is to describe the dependence of a response variable \\(y\\) on a set of \\(p\\) explanatory variables \\(\\b{x}=(x_1, x_2, \\ldots, x_p)\\) where, conditionally on \\(\\b{x}\\), observation \\(y\\) has a distribution which is not necessarily normal.\nNote that in these notes we may use lowercase letters, for example \\(y\\) or \\(y_i,\\) to denote both observed values or random variables, which is being considered should be clear from the context.\n\n\n\n\n\n\nImportant\n\n\n\nThis module will make extensive use of many basic ideas from statistics. If you need some revision, then see Appendix A: Basic material on Minerva under Basic Pre-requisite Material."
  },
  {
    "objectID": "1_intro.html#motivating-example",
    "href": "1_intro.html#motivating-example",
    "title": "1  Introduction",
    "section": "1.2 Motivating example",
    "text": "1.2 Motivating example\nTable 1.1 shows data1 on the number of beetles killed by five hours of exposure to 8 different concentrations of gaseous carbon disulphide.\n\n\nTable 1.1: Numbers of beetles killed by five hours of exposure to 8 different concentrations of gaseous carbon disulphide\n\n\n\n\n\n\n\nDose\n\\(x_i\\)\nNo. of beetle\n\\(m_i\\)\nNo. killed\n\\(y_i\\)\n\n\n\n\n1.6907\n59\n6\n\n\n1.7242\n60\n13\n\n\n1.7552\n62\n18\n\n\n1.7842\n56\n28\n\n\n1.8113\n63\n52\n\n\n1.8369\n59\n53\n\n\n1.8610\n62\n61\n\n\n1.8839\n60\n60\n\n\n\n\nFigure 1.1 (a) shows the same data with a linear regression line superimposed. Although this line goes close to the plotted points, we can see some fluctuations around it. More seriously, this is a stupid model: it would predict a mortality rate of greater than 100% at a dose of 1.9 units, and a negative mortality rate at 1.65 units!\n\n\nCode\nbeetle = read.table(\"https://rgaykroyd.github.io/MATH3823/Datasets/beetle.txt\", header=T)\n\ndose = beetle$dose\nmortality = beetle$died/beetle$total\n\nplot(dose, mortality, pch=16,\n     xlim=c(1.65, 1.90), xlab =\"Dose\",\n     ylim=c(-0.1, 1.1),  ylab=\"Mortality\")\nabline(h=c(0,1), lty=2)\n\nlm.fit = lm(mortality ~ dose)\nabline(lm.fit)\n\nplot(dose, mortality, pch=16,\n     xlim=c(1.65, 1.90), xlab =\"Dose\",\n     ylim=c(-0.1, 1.1),  ylab=\"Mortality\")\nabline(h=c(0,1), lty=2)\n\ny = cbind(beetle$died, beetle$total-beetle$died)\nglm.fit = glm(y ~ dose, family=binomial(link='logit'))\n\noutput.dose = seq(1.6,1.95,0.001)\nfitted = predict(glm.fit, data.frame(dose=output.dose), type=\"response\")\nlines(output.dose, fitted)\n\n\n\n\n\n\n\n\n(a) Linear model\n\n\n\n\n\n\n\n(b) Logistic model\n\n\n\n\nFigure 1.1: Beetle mortality rates with fitted dose- response curves.\n\n\n\nA more sensible dose–response relationship for the beetle mortality data might be based on the logistic function (to be defined later), as plotted in Figure 1.1 (b). The resulting curve is a closer, more-sensible, fit. Later in this module we will see how this curve was fitted using maximum likelihood estimation for an appropriate generalized linear model.\nThis is an example of a dose-response experiment which are widely used in medical and pharmaceutical situations.\n\n\n\n\n\n\nWarning\n\n\n\nWarning of potentially sensitive material. For further information on dose-response experiments see, for example, www.britannica.com/science/dose-response-relationship."
  },
  {
    "objectID": "1_intro.html#revision-of-least-squares-estimation",
    "href": "1_intro.html#revision-of-least-squares-estimation",
    "title": "1  Introduction",
    "section": "1.3 Revision of least-squares estimation",
    "text": "1.3 Revision of least-squares estimation\nSuppose that we have \\(n\\) paired data values \\((x_1, y_1),\\dots, (x_n, y_n)\\) and that we believe these are related by a linear model\n\\[\ny_i = \\alpha+\\beta x_i +\\epsilon_i\n\\]\nfor all \\(i\\in \\{1, 2,\\dots,n\\}\\), where \\(\\epsilon_1,\\dots,\\epsilon_n\\) are independent and identically distributed (iid) with \\(\\mbox{E}(\\epsilon_i)=0\\) and \\(\\mbox{Var}(\\epsilon_i)=\\sigma^2\\). The aim will be to find values of the model parameters, \\(\\alpha, \\beta \\text{ and } \\sigma^2\\) using the data. Specifically, we will estimate \\(\\alpha\\) and \\(\\beta\\) using the values which minimize the residual sum of squares (RSS)\n\\[\nRSS(\\alpha, \\beta) = \\sum_{i=1}^n \\left(y_i-(\\alpha+\\beta x_i)\\right)^2.\n\\tag{1.1}\\]\nThis measures how close the data points are around the regression line and hence the resulting estimates, \\(\\hat\\alpha\\) and \\(\\hat\\beta\\), will give us a fitted regression line which is closest to the data.\nIt can be shown that Equation 1.1 takes its minimum when the parameters are given by\n\\[\n\\hat\\alpha = \\bar y -\\hat\\beta\\bar x, \\quad \\mbox{and} \\quad\n\\hat\\beta = \\frac{s_{xy}}{s^2_x}\n\\tag{1.2}\\]\nwhere \\(\\bar x\\) and \\(\\bar y\\) are the sample means,\n\\[\ns_{xy}=\\frac{1}{n-1}\\sum_{i=1}^n (x_i-\\bar x)(y_i-\\bar y)\n\\]\nis the sample covariance and\n\\[\ns^2_x = \\frac{1}{n-1} \\sum_{i=1}^n (x_i-\\bar x)^2\n\\]\nis the sample variance of the \\(x\\) values. It can be shown that these estimators are unbiased, that is \\(\\mbox{E}[\\hat\\alpha]=\\alpha\\) and \\(\\mbox{E}[\\hat\\beta]=\\beta\\) – see Section 1.5.\nThe fitted regression lines is then given by \\(\\hat y = \\hat \\alpha +\\hat \\beta x\\), the fitted values by \\(\\hat y_i = \\hat \\alpha +\\hat \\beta x_i\\), and the model residuals by \\(r_i= \\hat \\epsilon_i= y_i-\\hat y_i\\) for all \\(i\\in \\{1,\\dots,n\\}\\).\nTo complete the model fitting, we also estimate the error variance, \\(\\sigma^2\\), using \\[\n\\hat \\sigma^2 = \\frac{1}{n-2} \\sum _{i=1}^n r_i^2.\n\\tag{1.3}\\]\nNote that, by construction, \\(\\bar r=0\\) and, further, it can be shown that \\(\\hat \\sigma^2\\) is an unbiased estimator of \\(\\sigma^2\\), that is \\(\\mbox{E}[\\hat\\sigma^2]=\\sigma^2\\).\n\n\nCode\nxbar = mean(dose)\nybar = mean(mortality)\ns2x = var(dose)\nsxy = cov(dose, mortality)\n\nbetahat = sxy/s2x\nalphahat = ybar-betahat*xbar\n\ns2hat = sum((mortality-alphahat-betahat*dose)^2)/(length(dose)-2)\n\n\nReturning to the above beetle data example, we have \\(\\hat\\alpha=\\)-8.947843, \\(\\hat\\beta=\\) 5.324937, and \\(\\hat \\sigma^2 =\\) 0.0075151.\nWe will interpret the output later, but in \\(\\b{R}\\), the fitting can be done with a single command with corresponding fitting output from a second command:\n\n\nCode\nlm.fit = lm(mortality ~ dose)\n\nsummary(lm.fit)\n\n\n\nCall:\nlm(formula = mortality ~ dose)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.10816 -0.06063  0.00263  0.05119  0.12818 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -8.9478     0.8717  -10.27 4.99e-05 ***\ndose          5.3249     0.4857   10.96 3.42e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.08669 on 6 degrees of freedom\nMultiple R-squared:  0.9524,    Adjusted R-squared:  0.9445 \nF-statistic: 120.2 on 1 and 6 DF,  p-value: 3.422e-05\n\n\n\n\n\n\n\n\nImportant\n\n\n\nYou should have met \\(\\b{R}\\) output like this in previous statistics modules, but if you need some revision then see Appendix-C: Background to Analysis of Variance on Minerva under Basic Pre-requisite Material."
  },
  {
    "objectID": "1_intro.html#sec-typevariables",
    "href": "1_intro.html#sec-typevariables",
    "title": "1  Introduction",
    "section": "1.4 Types of variables",
    "text": "1.4 Types of variables\nThe way a variable enters a model will depends on its type. The most common five types of variable are:\n\nQuantitative\n\nContinuous: for example, height; weight; duration. Real valued. Note that although recorded data is rounded it is still usually best regarded as continuous.\nCount (discrete): for example, number of children in a family; accidents at a road junction; number of items sold. Non-negative and integer-valued.\n\nQualitative\n\nOrdered categorical (ordinal): for example, severity of illness (Mild/ Moderate/Severe); degree classification (first/ upper-second/ lower-second/ third).\nUnordered categorical (nominal):\n\nDichotomous (binary): two categories: for example sex (M/ F); agreement (Yes/ No); coin toss (Head/ Tail).\nPolytomous (also known as polychotomous): more than two categories: for example blood group (A/ B/ O); eye colour (Brown/ Blue/ Green).\n\n\n\nNote that although dichotomous is clearly a special case of polytomous, making the distinction is usually worthwhile as it often leads to a simplified modelling and testing approach."
  },
  {
    "objectID": "1_intro.html#sec-exercises1",
    "href": "1_intro.html#sec-exercises1",
    "title": "1  Introduction",
    "section": "1.5 Exercises",
    "text": "1.5 Exercises\n\n\n\n\n\n\nImportant\n\n\n\nUnless otherwise stated, data files will be available online at: rgaykroyd.github.io/MATH3823/Datasets/filename.ext, where filename.ext is the stated filename with extension.\n\n\n\n1.1 Consider again the beetle data in Table 1.1. Perform the calculations by hand and then check the answers using \\(\\b{R}\\) – a copy of the data is available in the file beetle.txt. Finally plot the fitted regression line on a scatter plot of the data. [Hint: See the code chunk used to produce Figure 1.1.]\n1.2 Consider the following synthetic data:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(i=1\\)\n\\(i=2\\)\n\\(i=3\\)\n\\(i=4\\)\n\\(i=5\\)\n\\(i=6\\)\n\\(i=7\\)\n\\(i=8\\)\n\n\n\n\n\\(x_i\\)\n-1\n0\n1\n2\n2.5\n3\n4\n6\n\n\n\\(y_i\\)\n-2.8\n-1.1\n7.2\n8.0\n8.9\n9.2\n14.8\n24.7\n\n\n\nPlot the data to check that a linear model is suitable and then fit a linear regression model. Do you think that the fitted model can be reliably used to predict the values of \\(y\\) when \\(x=5\\) and \\(x=10\\)? Justify your answers.\n1.3 Starting from Equation 1.1, derive the estimation equations given in Equation 1.2. Further, show that \\(\\hat\\alpha\\) and \\(\\hat\\beta\\) are unbiased estimators of \\(\\alpha\\) and \\(\\beta\\). [Hint: Check your MATH1712 lecture notes.]\nWhat can be said about \\(\\hat\\sigma^2\\) as an estimator of \\(\\sigma^2\\)? [Hint: There is a careful theoretical proof, but here only an intuitive explanation is expected.]\n1.4 The Brownlee’s Stack Loss Plant Data2 is already available in \\(\\mathbf{R}\\), with background details on the help page, \\(\\texttt{?stackloss}\\). [Hint: You already met this example in MATH1712.]\nAfter plotting all pairs of variables, which of \\(\\texttt{Air.Flow}\\), \\(\\texttt{Water.Temp}\\) and \\(\\texttt{Acid.Conc}\\) do you think could be used to model \\(\\texttt{stack.loss}\\) using a linear regression? Justify your answer.\nPerform a simple linear regression with using \\(\\texttt{stack.loss}\\) as the response variable and your chosen variable as the explanatory variable. Add the fitted regression line to a scatter plot of the data and comment.\n1.5 In an experiment conducted by de Silva et al. in 20203 data was obtained to investigate falling objects and gravity, as first consider by Galileo and Newton. A copy of the data is available in the file physics_from_data.csv.\nRead the data file into \\(\\b{R}\\) and perform a simple linear regression of the maximum Reynolds number as the response variable and, in turn, each of the other variables as the explanatory variable.\nPlot the data and add the corresponding fitted linear models. Which variable do you think helps explain Reynolds number the best? Why do you think this?\n\n\nHere are an infinite number of further numerical examples from maths e.g. (thanks to https://www.mathcentre.ac.uk/):\nFinding the intersercept\nFinding the slope - Part 1\nFinding the slope - Part 2"
  },
  {
    "objectID": "2_linearmodels.html#overview",
    "href": "2_linearmodels.html#overview",
    "title": "2  Essentials of Normal Linear Models",
    "section": "2.1 Overview",
    "text": "2.1 Overview\nIn many fields of application, we might assume the response variable is normally distributed. For example: heights, weights, log prices, etc.\nThe data1 in Table 2.1 record the birth weights of 12 girls and 12 boys and their gestational ages (time from conception to birth).\n\n\nTable 2.1: Gestational ages (in weeks) and birth weights (in grams) for 24 babies (12 girls and 12 boys).\n\n\n\n\n(a) Girls\n\n\nGestational Age\nBirth weight\n\n\n\n\n40\n3317\n\n\n36\n2729\n\n\n40\n2935\n\n\n37\n2754\n\n\n42\n3210\n\n\n39\n2817\n\n\n40\n3126\n\n\n37\n2539\n\n\n36\n2412\n\n\n38\n2991\n\n\n39\n2875\n\n\n40\n3231\n\n\n\n\n\n\n(b) Boys\n\n\nGestational Age\nBirth weight\n\n\n\n\n40\n2968\n\n\n38\n2795\n\n\n40\n3163\n\n\n35\n2925\n\n\n36\n2625\n\n\n37\n2847\n\n\n41\n3292\n\n\n40\n3473\n\n\n37\n2628\n\n\n38\n3176\n\n\n40\n3421\n\n\n38\n3975\n\n\n\n\n\n\nA key question is, can we predict the birth weight of a baby born at a given gestational age using these data. For this we will need to make assumptions about the relationship between birth weight and gestational age, and any associated natural variation – that is we require a model.\nFirst we should explore the data. Figure 2.1 (a) shows a histogram of the birth weights indicating a spread around modal group 2800-3000 grams; Figure 2.1 (b) indicates slightly higher birth weights for the boys than the girls; and Figure 2.1 (c) shows an increasing relationship between weight and age. Together, these suggest that gestational age and sex are likely to be important for predicting weight.\n\n\nCode\nbirthweight = read.table(\"https://rgaykroyd.github.io/MATH3823/Datasets/birthwt-numeric.txt\", header=T)\n\nweight = birthweight$weight\nage = birthweight$age\nsex = birthweight$sex\n\nhist(weight, breaks=6, probability = T, main = \"\", \n     xlab = \"Birth weight (grams)\")\nboxplot(weight~sex, names=c(\"Girl\", \"Boy\"))\n\nplot(age, weight, pch=16, \n     xlab = \"Gestational age (weeks)\",\n     ylab = \"Birth weight (grams)\")\n\n\n\n\n\n\n\n\n(a) Weight distribution\n\n\n\n\n\n\n\n(b) Weight sub-divided by Sex\n\n\n\n\n\n\n\n\n\n(c) Relationship beween variables\n\n\n\n\nFigure 2.1: Birthweight and gestational age for 24 babies.\n\n\n\nBefore considering possible models, Figure 2.2 again shows the relationship between weight and age but this time with the points coloured according to the baby’s sex. This, perhaps, shows the boys to have generally higher weights across the age range than girls.\n\n\nCode\nbirthweight = read.table(\"https://rgaykroyd.github.io/MATH3823/Datasets/birthwt-numeric.txt\", header=T)\n\nweight = birthweight$weight\nage = birthweight$age\nsex = birthweight$sex\n\nplot(age, weight, col=2-sex, pch=15+sex,\n     xlab = \"Gestational age (weeks)\",\n     ylab = \"Birth weight (grams)\")\nlegend(41,2800, c(\"Girl\",\"Boy\"), col = c(2,1), pch=c(15,16))\n\n\n\n\n\nFigure 2.2: Birthweight and gestational age for 12 girls (red squares) and 12 boys (black dots).\n\n\n\n\nOf course, there are very many possible models, but here we will consider the following:\n\n\n\n\n\n\n\n\\(\\texttt{Model 0}:\\)\n\\(\\texttt{Weight}=\\alpha\\)\n\n\n\\(\\texttt{Model 1}:\\)\n\\(\\texttt{Weight}=\\alpha + \\beta.\\texttt{Age}\\)\n\n\n\\(\\texttt{Model 2}:\\)\n\\(\\texttt{Weight}=\\alpha + \\beta.\\texttt{Age}+\\gamma.\\texttt{Sex}\\)\n\n\n\\(\\texttt{Model 3}:\\)\n\\(\\texttt{Weight}=\\alpha + \\beta.\\texttt{Age}+\\gamma.\\texttt{Sex} + \\delta.\\texttt{Age}.\\texttt{Sex}\\)\n\n\n\nIn these models, \\(\\texttt{Weight}\\) is called the response variable (sometimes called the dependent variable) and \\(\\texttt{Age}\\) and \\(\\texttt{Sex}\\) are called the covariates or explanatory variables (sometimes called the predictor or independent variables). Here, \\(\\texttt{Age}\\) is a continuous variable whereas \\(\\texttt{Sex}\\) is coded as a dummy variable taking the value 0 for girls and 1 for boys; it is an example of a factor, in this case with just two levels: Girl and Boy.\nNote that \\(\\texttt{Model 0}\\) is a special case of \\(\\texttt{Model 1}\\) (consider the situation when \\(\\beta=0\\)) and that \\(\\texttt{Model 1}\\) is a special case of \\(\\texttt{Model 2}\\) (consider the situation when \\(\\gamma=0\\)) and finally that \\(\\texttt{Model 2}\\) is a special case of \\(\\texttt{Model 3}\\) (consider the situation when \\(\\delta=0\\)) – such models are called nested.\nIn these models, \\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\) and \\(\\delta\\) are model parameters. Parameter \\(\\alpha\\) is called the intercept term; \\(\\beta\\) is called the main effect of \\(\\texttt{Age}\\); and is interpreted as the effect on birth weight per week of gestational age. Similarly, \\(\\gamma\\) is the main effect of \\(\\texttt{Sex}\\), interpreted as the effect on birth weight of being a boy (because girl is the baseline category).\nParameter \\(\\delta\\) is called the interaction effect between \\(\\texttt{Age}\\) and \\(\\texttt{Sex}\\). Take care when interpreting an interaction effect. Here, it does not mean that age somehow affects sex, or vice-versa. It means that the effect of gestational age on birth weight depends on whether the baby is a boy or a girl.\nThese models can be fitted to the data using (Ordinary) Least Squares to produce the results presented in Figure 2.3.\nWhich model should we use?\n\n\nCode\nbirthweight = read.table(\"https://rgaykroyd.github.io/MATH3823/Datasets/birthwt-numeric.txt\", header=T)\n\nweight = birthweight$weight\nage = birthweight$age\nsex = birthweight$sex\n\n\nplot(age, weight, pch=16, \n     xlab = \"Gestational age (weeks)\",\n     ylab = \"Birth weight (grams)\")\nabline(h=mean(weight))\n\n\nplot(age, weight, pch=16,  \n     xlab = \"Gestational age (weeks)\",\n     ylab = \"Birth weight (grams)\")\nM1.fit = lm(weight~age)\nabline(M1.fit$coefficients[1],M1.fit$coefficients[2])\n\n\nplot(age, weight, pch=15+sex, col=2-sex, \n     xlab = \"Gestational age (weeks)\",\n     ylab = \"Birth weight (grams)\")\nlegend(41,2800, c(\"Girl\",\"Boy\"), col = c(2,1),pch=c(15,16))\n\nM2.fit = lm(weight~age+sex)\n\nabline(M2.fit$coefficients[1],M2.fit$coefficients[2], col=2)\nabline(M2.fit$coefficients[1]+M2.fit$coefficients[3],M2.fit$coefficients[2], col=1)\n\n\nplot(age, weight, pch=15+sex, col=2-sex, \n     xlab = \"Gestational age (weeks)\",\n     ylab = \"Birth weight (grams)\")\nlegend(41,2800, c(\"Girl\",\"Boy\"), col = c(2,1), pch=c(15,16))\n\nM3.fit = lm(weight~age+sex+age*sex)\n\nabline(M3.fit$coefficients[1],M3.fit$coefficients[2], col=2)\nabline(M3.fit$coefficients[1]+M3.fit$coefficients[3],M3.fit$coefficients[2]+M3.fit$coefficients[4], col=1)\n\n\n\n\n\n\n\n\n(a) Model 0\n\n\n\n\n\n\n\n(b) Model 1\n\n\n\n\n\n\n\n\n\n(c) Model 2\n\n\n\n\n\n\n\n(d) Model 3\n\n\n\n\nFigure 2.3: Birthweight and gestational age data with superimposed fitted regression lines from various competing models.\n\n\n\nWe know from previous modules that statistical tests can be used to check the importance of regression coefficients and model parameters, but it is also important to use the graphical results, as in Figure 2.3, to guide us.\n\\(\\texttt{Model 0}\\) says that there is no change in birth weight with gestational age which means that we would use the average birth weight as the prediction whatever the gestational age – this makes no sense. As we can easily see from the scatter plot of the data, the fitted line in this case is clearly inappropriate.\n\\(\\texttt{Model 1}\\) does not take into account whether the baby is a girl or a boy, but does model the relationship between birth weight and gestational age. This does seem to provide a good fit and might be adequate for many purposes. Recall from Figure 2.1 (b) and Figure 2.2, however, that for a given gestational age the boys seem to have a higher birth weight than the girls.\n\\(\\texttt{Model 2}\\) does take the sex of the baby into account by allowing separate intercepts in the fitted lines – this means that the lines are parallel. By eye, there is a clear difference between these two lines but it might not be important.\n\\(\\texttt{Model 3}\\) allows for separate slopes as well as intercepts. There is a slight difference in the slopes, with the birth weight of the girls gradually catching-up as the gestational age increases. It is difficult to see, however, if this will be a general pattern or if it is only true for this data set – especially given the relatively small sample size.\nHere, it is not clear by eye which of the fitted models will be the best and hence we should use a statistical test to help. In particular, we can choose between the models using F-tests.\nLet \\(y_i\\) denote the value of the dependent variable \\(\\texttt{Weight}\\) for individual \\(i=1,\\dots,n\\), and let the four models be indexed by \\(k=0,1,2,3\\).\nLet \\(R_k\\) denote the residual sum of squares (RSS) for Model \\(k:\\)\n\\[\nR_k = \\sum_{i=1}^n (y_i - \\hat{\\mu}_{ki})^2,\n\\tag{2.1}\\]\nwhere \\(\\hat{\\mu}_{ki}\\) is the fitted value for individual \\(i\\) under \\(\\texttt{Model}\\) \\(k\\). Let \\(r_k\\) denote the corresponding residual degrees of freedom for \\(\\texttt{Model}\\) \\(k\\) (the number of observations minus the number of model parameters).\nConsider the following hypotheses: \\[\nH_0: \\texttt{Model } 0 \\text{ is true}; \\hspace{5mm} H_1: \\texttt{Model } 1 \\text{ is true}.\n\\] Under the null hypothesis \\(H_0\\), the difference between \\(R_0\\) and \\(R_1\\) will be purely random, so the between-models mean-square \\((R_0 - R_1)/(r_0 - r_1)\\) should be comparable to the residual mean-square \\(R_1/r_1\\). Thus our test statistic for comparing \\(\\texttt{Model } 1\\) to the simpler \\(\\texttt{Model } 0\\) is:\n\\[\nF_{01} = \\frac{(R_0 - R_1)/(r_0 - r_1)}{R_1/r_1}.\n\\tag{2.2}\\]\nIt can be shown that, under the null hypothesis \\(H_0\\), the statistic \\(F_{01}\\) will have an \\(F\\)-distribution on \\(r_0 - r_1\\) and \\(r_1\\) degrees of freedom, which we write: \\(F_{r_0-r_1, r_1}\\). Under the alternative hypothesis \\(H_1\\), the difference \\(R_0-R_1\\) will tend to be larger than expected under \\(H_0\\), and so the observed value \\(F_{01}\\) will probably lie in the upper tail of the \\(F_{r_0-r_1, r_1}\\) distribution.\nReturning to the birth weight data, we obtain the following output from \\(\\b{R}\\) when we fit \\(\\texttt{Model } 1\\):\n\n\nCode\n# read the data from file into a dataframe called ’birthweight’\nbirthweight = read.table(\"https://rgaykroyd.github.io/MATH3823/Datasets/birthwt.txt\", header=T)\n\n# fit Model 1\nfit1 = lm(weight ~ age, data=birthweight)\n\n# print the parameter estimates from Model 1\ncoefficients(fit1)\n\n\n(Intercept)         age \n -1484.9846    115.5283 \n\n\nCode\n# perform an analysis of variance of Model 1\nanova(fit1)\n\n\nAnalysis of Variance Table\n\nResponse: weight\n          Df  Sum Sq Mean Sq F value   Pr(>F)    \nage        1 1013799 1013799   27.33 3.04e-05 ***\nResiduals 22  816074   37094                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThus we have parameter estimates: \\(\\hat\\alpha = -1484.98\\) and \\(\\hat\\beta = 115.5\\). The Analysis of Variance (ANOVA) table gives: \\(R_0-R_1 = 1013799\\) with \\(r_0-r_1 = 1\\) and \\(R_1 = 816074\\) with \\(r_1 = 22\\).\nIf we wanted \\(R_0\\) and \\(r_0\\) then we can either fit \\(\\texttt{Model 0}\\) or get them by subtraction.\nThe \\(F_{01}\\) statistic, Equation 2.2, is then\n\\[\nF_{01} = \\frac{113799/1}{816074/22} = 27.33,\n\\] which can be read directly from the ANOVA table in the column headed ‘F value’.\nIs \\(F_{01} = 27.33\\) in the upper tail of the \\(F_{1,22}\\) distribution? (See Figure 2.4 and note that 27.33 is very far to the right.) The final column of the ANOVA table tells us that the probability of observing \\(F_{01} > 27.33\\) is only \\(3.04\\times10^5\\) – this is called a p-value. The *** beside this p-value highlights that its value lies between 0 and 0.001. This indicates that we should reject \\(H_0\\) in favour of \\(H_1\\) – there is very strong evidence for the more complicated model. Thus we would conclude that the effect of gestational age is statistically significant in these data.\n\n\nCode\ncurve(df(x,1,22), 0,30, \n      ylab=expression(\"PDF of \"*\"F\"[0][1]))\n\n\n\n\n\nFigure 2.4: Probability density function of \\(F_{01}\\) distribution.\n\n\n\n\nNext, consider the following hypotheses:\n\\[\nH_0: \\texttt{Model } 1 \\text{ is true}; \\hspace{5mm} H_1: \\texttt{Model } 2 \\text{ is true}.\n\\]\nUnder the null hypothesis \\(H_0\\), the difference between \\(R_1\\) and \\(R_2\\) will be purely random, so the between-models mean-square \\((R_1 - R_2)/(r_1 - r_2)\\) should be comparable to the residual mean-square \\(R_2/r_2\\). Thus our test statistic for comparing \\(\\texttt{Model } 2\\) to the simpler \\(\\texttt{Model } 1\\) is:\n\\[\nF_{12} = \\frac{(R_1 - R_2)/(r_1 - r_2)}{R_2/r_2}.\n\\tag{2.3}\\]\nUnder the null hypothesis \\(H_0\\), the statistic \\(F_{12}\\) will have an \\(F\\)-distribution on \\(r_1 - r_2\\) and \\(r_2\\) degrees of freedom, which we write: \\(F_{r_1-r_2, r_2}\\). Under the alternative hypothesis \\(H_1\\), the difference \\(R_1-R_2\\) will tend to be larger than expected under \\(H_0\\), and so the observed value \\(F_{12}\\) will probably lie in the upper tail of the \\(F_{r_1-r_2, r_2}\\) distribution.\nReturning to the birth weight data, we obtain the following output from \\(\\b{R}\\) (where \\(\\texttt{sexM}\\) denotes Boy):\n\n\nCode\n# fit Model 2\nfit2 = lm(weight ~ age + sex, data=birthweight)\n# print the parameter estimates from Model 2\ncoefficients(fit2)\n\n\n(Intercept)         age        sexM \n -1773.3218    120.8943    163.0393 \n\n\nCode\n# perform an analysis of variance on the fitted model\nanova(fit2)\n\n\nAnalysis of Variance Table\n\nResponse: weight\n          Df  Sum Sq Mean Sq F value    Pr(>F)    \nage        1 1013799 1013799 32.3174 1.213e-05 ***\nsex        1  157304  157304  5.0145   0.03609 *  \nResiduals 21  658771   31370                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThus we have parameter estimates: \\(\\hat \\alpha = -1773.3\\), \\(\\hat\\beta = 120.9\\) and \\(\\hat\\gamma = 163.0\\), the latter being the effect of being a boy compared to the baseline category of being a girl.\nThe Analysis of Variance (ANOVA) table gives: \\(R_1-R_2 = 157304\\) with \\(r_1-r_2=1\\), and \\(R_2=658771\\) with \\(r_2=21\\). The \\(F_{12}\\) statistic, Equation 2.3, is then\n\\[\nF_{12} = \\frac{157304/1}{658771/21} = 5.0145,\n\\]\nwhich can be read directly from the ANOVA table in the column headed ‘F value’. Is \\(F_{12} = 5.01\\) in the upper tail of the \\(F_{1,21}\\) distribution?\nThe final column of the ANOVA table tells us that the probability of observing \\(F_{12} > 5.01\\) is only \\(0.03609\\) – this is called a p-value. The * beside this p-value highlights that its value lies between 0.01 and 0.05. This indicates that we should reject \\(H_0\\) in favour of \\(H_1\\) – there is evidence for the more complicated model. Thus we would conclude that the effect of the sex of the baby, after controlling for gestational age, is statistically significant in these data.\nTo complete the analysis, we should now compare \\(\\texttt{Model }2\\) with \\(\\texttt{Model }3\\) – see Exercises."
  },
  {
    "objectID": "2_linearmodels.html#sec-typesnormalvariable",
    "href": "2_linearmodels.html#sec-typesnormalvariable",
    "title": "2  Essentials of Normal Linear Models",
    "section": "2.2 Types of normal linear model",
    "text": "2.2 Types of normal linear model\nHere we consider how normal linear models can be set up for different types of explanatory variable. The dependent variable \\(y\\) is modelled as a linear combination of \\(p\\) explanatory variables \\(\\b{x} =(x_1, x_2,\\ldots, x_p)\\) plus a random error \\(\\epsilon \\sim N(0, \\sigma^2)\\), where ‘~’ means ‘is distributed as’. Several models are of this kind, depending on the number and type of explanatory variables. Table 2.2 lists some types of normal linear models with their explanatory variable types.\n\n\nTable 2.2: Types of normal linear model and their explanatory variable types where indicator function \\(I(x=j)=1\\) if \\(x=j\\) and \\(0\\) otherwise.\n\n\n\n\n\n\n\n\\(p\\)\nExplanatory variables\nModel\n\n\n1\nQuantitative\nSimple linear regression \\(y=\\alpha+\\beta x+\\epsilon\\)\n\n\n>1\nQuantitative\nMultiple linear regression\\(y=\\alpha+\\sum_{i=1}^p\\beta_i x_i+\\epsilon\\)\n\n\n1\nDichotomous (\\(x=1\\) or \\(2\\))\nTwo-sample t-test \\(y=\\alpha+\\delta ~ I(x=2)+\\epsilon\\)\n\n\n1\nPolytomous, \\(k\\) levels \\((x=1,\\ldots,k)\\)\nOne-way ANOVA\\(y=\\alpha+\\sum_{j=1}^k \\delta_j \\ I(x=j)+\\epsilon\\)\n\n\n>1\nQualitative\n\\(p\\)-way ANOVA\n\n\n\n\nFor the two-sample t-test model2, observations in the two groups have means \\(\\alpha+\\beta_1\\) and \\(\\alpha + \\beta_2\\) . Notice, however, that we have three parameters with only two group sample means and hence parameter estimation is not possible. To avoid this identification problem, we either impose a ‘corner’ constraint: \\(\\beta_1=0\\) and then \\(\\beta_2\\) represents the difference in the Group 2 mean relative to a baseline of Group \\(1\\). Alternatively, we may impose a ‘sum-to-zero’ constraint: \\(\\beta_1+ \\beta_2 =0\\), the values \\(\\beta_1=-\\beta_2\\) then give differences in the groups means relative to the overall mean. Table 2.3 shows the effect of the parameter constraint on the group means.\n\n\nTable 2.3: Parameters in the two-sample t-test model after imposing parameter constraint to avoid the identification problem.\n\n\nConstraint\nGroup 1 mean\nGroup 2 mean\n\n\n\n\n\\(\\beta_1=0\\)\n\\(\\alpha\\)\n\\(\\alpha+\\beta_2\\)\n\n\n\\(\\beta_1+\\beta_2=0\\)\n\\(\\alpha-\\beta_2\\)\n\\(\\alpha+\\beta_2\\)\n\n\n\n\nFor the general one-way ANOVA model with \\(k\\) groups, observations in Group \\(j\\) have mean \\(\\alpha + \\delta_j\\) , for \\(j =1, \\ldots, k\\) – that leads to \\(k + 1\\) parameters describing \\(k\\) group means. Again we can impose the ‘corner’ constraint: \\(\\delta_1 = 0\\) and then \\(\\delta_j\\) represents the difference in means between Group \\(j\\) and the baseline Group \\(1\\). Alternatively, we may impose a ‘sum-to-zero’ constraint:\\(\\sum_{j=1}^k \\delta_j =0\\) and again \\((\\delta_1, \\delta_2,\\dots,\\delta_k)\\) then represents an individual group effect relative to the overall data mean."
  },
  {
    "objectID": "2_linearmodels.html#matrix-representation-of-linear-models",
    "href": "2_linearmodels.html#matrix-representation-of-linear-models",
    "title": "2  Essentials of Normal Linear Models",
    "section": "2.3 Matrix representation of linear models",
    "text": "2.3 Matrix representation of linear models\nAll of the models in Table Table 2.2 can be fitted by least squares (OLS). To describe this, a matrix formulation will be most convenient:\n\\[\n\\mathbf{Y} = X\\boldsymbol{\\beta}+\\boldsymbol{\\epsilon}\n\\tag{2.4}\\]\nwhere\n\n\\(\\mathbf{Y}\\) is an \\(n\\times 1\\) vector of observed response values with \\(n\\) being the number of observations.\n\\(X\\) is an \\(n\\times p\\) design matrix, to be discussed below.\n\\(\\boldsymbol{\\beta}\\) is a \\(p\\times 1\\) vector of parameters or coefficients to be estimated.\n\\(\\boldsymbol{\\epsilon}\\) is an \\(n\\times 1\\) vector of independent and identically distributed (IID) random variables, which here \\(\\epsilon \\sim N(0, \\sigma^2)\\) and is called the “error” term."
  },
  {
    "objectID": "2_linearmodels.html#sec-designmatrix",
    "href": "2_linearmodels.html#sec-designmatrix",
    "title": "2  Essentials of Normal Linear Models",
    "section": "2.4 Construction of the design matrix",
    "text": "2.4 Construction of the design matrix\nCreating the design matrix is a key part of the modelling as it describes the important structure of investigation or experiment. The design matrix can be constructed by the following process.\n\nBegin with an \\(X\\) containing only one column: a vector of ones for the overall mean or intercept term (the \\(\\alpha\\) in Table 2.2).\nFor each explanatory variable \\(x_j\\), do the following:\n\nIf a variable \\(x_j\\) is quantitative, add a column to \\(X\\) containing the values of \\(x_j.\\)\nIf \\(x_j\\) is qualitative with \\(k\\) levels, add \\(k\\) “dummy” columns to \\(X\\), taking values 0 and 1, where a 1 in the \\(\\ell\\)th dummy column identifies that the corresponding observation is at level \\(\\ell\\) of factor \\(x_j\\) . For example, suppose we have a factor \\(\\mathbf{x}_j = (M, M, F, M, F)\\) representing the sex of \\(n = 5\\) individuals. This information can be coded into two dummy columns of \\(X\\):\n\n\\[\n\\begin{matrix}\n\\begin{matrix}\nF & M\n\\end{matrix}\\\\\n\\begin{bmatrix}\n0 & 1 \\\\\n0 & 1 \\\\\n1 & 0 \\\\\n0 & 1 \\\\\n1 & 0\n\\end{bmatrix}\n\\end{matrix}\n\\]\nWhen qualitative variables are present, \\(X\\) will be singular – that is, there will be linear dependencies between the columns of \\(X\\). For example, the sum of the two columns above is a vector of ones, the same as the intercept column. We resolve this identification problem by deleting some columns of \\(X\\). This is equivalent to applying the corner constraint \\(\\delta_1 = 0\\) in the one-way ANOVA.\nIn the above example, after removing a column, we get:\n\\[\n\\mathbf{X}=\\begin{bmatrix}\n1 &  1 \\\\\n1 &  1 \\\\\n1 &  0 \\\\\n1 &  1 \\\\\n1 &  0\n\\end{bmatrix}.\n\\]\nEach column of \\(X\\) represents either a quantitative variable, or a level of a qualitative variable. We will use \\(i = 1, \\ldots, n\\) to label the observations (rows of \\(X\\)) and \\(j = 1, \\ldots, p\\) to label the columns of \\(X\\).\n\n\n2.4.1 Example: Simple linear regression\nConsider the simple linear regression model \\(y=\\alpha+\\beta x+\\epsilon\\) with \\(\\epsilon \\sim N(0, \\sigma^2)\\). Given data on \\(n\\) pairs \\((x_i, y_i), i = 1, \\ldots, n\\), we write this as\n\\[\ny_i = \\alpha+\\beta x_i+\\epsilon_i, \\quad \\text{for } i=1,2,\\dots,n,\n\\tag{2.5}\\]\nwhere the \\(\\epsilon_i\\) are IID \\(N(0,\\sigma^2)\\). In matrix form, this becomes\n\\[\n\\mathbf{Y}=X\\boldsymbol{\\beta}+\\boldsymbol{\\epsilon}\n\\tag{2.6}\\] with \\[\n\\mathbf{Y}=\\begin{bmatrix}\ny_1\\\\\n\\vdots\\\\\ny_n\n\\end{bmatrix},\n%\n\\hspace{5mm}\n%\nX=\\begin{bmatrix}\n1 & x_1\\\\\n\\vdots & \\vdots\\\\\n1 & x_n\n\\end{bmatrix},\n%\n\\hspace{5mm}\n%\n\\boldsymbol{\\beta}=\n\\begin{bmatrix}\n\\beta_1\\\\\n\\beta_2\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\alpha\\\\\n\\beta\n\\end{bmatrix},\n%\n\\hspace{5mm}\n%\n\\boldsymbol{\\epsilon}=\n\\begin{bmatrix}\n\\epsilon_1\\\\\n\\vdots\\\\\n\\epsilon_n\n\\end{bmatrix}.\n\\] The \\(i\\)th row of Equation 2.6 has the same meaning as Equation 2.5: \\[\ny_i = 1\\times \\beta_1 + x_i\\times \\beta_2 +\\epsilon_i = \\alpha+\\beta x_i +\\epsilon_i,  \\hspace{2mm} \\text{for } i=1,2,\\dots,n.\n\\]\n\n\n2.4.2 Example: One-way ANOVA\nFor one-way ANOVA with \\(k\\) levels, the model is \\[\ny_i =\\alpha+\\sum_{j=1}^k \\delta_j \\ I(x_i=j)+\\epsilon_i, \\quad \\text{for } i=1, 2, \\dots,n,\n\\] where \\(x_i\\) denotes the group level of individual \\(i\\). So if \\(y_i\\) is from the \\(j\\)th group then \\(y_i \\sim N(\\alpha+\\delta_j, \\sigma^2)\\). Here \\(\\alpha\\) is the intercept and the \\((\\delta_1, \\delta_2, \\dots,\\delta_k)\\) represent the “main effects”.\nWe can store the information about the levels of \\(g\\) in a dummy matrix \\(X^* = (x^*_{ij})\\) where\n\\[\nx^*_{ij} = \\left\\{\n\\begin{array}{cl}\n1, & g_i=j,\\\\\n0, & \\text{otherwise.}\n\\end{array}\n\\right.\n\\]\nThen set \\(X = [1, X^*]\\), where \\(1\\) is an \\(n\\)-vector of \\(1\\)’s. For the male–female example at (1.12), we have \\(n = 5\\) and a sex factor:\n\\[\ng=\\begin{bmatrix}1\\\\ 1 \\\\2\\\\1\\\\2\\end{bmatrix},\n%\n\\hspace{5mm}\n%\nX=\\begin{bmatrix}\n1 & 1 & 0\\\\\n1&1& 0 \\\\ 1& 0 & 1\\\\ 1& 1 & 0 \\\\ 1& 0 & 1\n\\end{bmatrix},\n%\n\\hspace{5mm}\n%\n\\beta=\\begin{bmatrix}\\beta_1\\\\\\beta_2\\\\\\beta_3\\end{bmatrix}\n=\\begin{bmatrix}\\alpha\\\\\\delta_1 \\\\\\delta_2\\end{bmatrix}.\n\\]\nThen the \\(i\\)th row of \\(X\\) becomes \\(\\beta_1 + \\beta_2 = \\alpha + \\delta_1\\) if \\(g_i = 1\\) and \\(\\beta_1 + \\beta_3 = \\alpha + \\delta_2\\) if \\(g_i\\) = 2. That is, the \\(i\\)th row of \\(X\\) is\n\\[\n\\alpha+\\sum_{j=1}^2 \\delta_j I(g_i=j)\n\\] so this model can be written \\(Y=X\\beta+\\epsilon\\). Here, \\(X\\) is singular: its last two columns added together equal its first column. Statistically, the problem is that we are trying to estimate two means (the mean response for Boys and the mean response for Girls) with three parameters (\\(\\alpha\\), \\(\\delta_2\\) and \\(\\delta_2\\)).\nIn practice, we often resolve this aliasing or identification problem by setting one of the parameters to be zero, that is \\(\\delta_1 = 0\\), which corresponds to deleting the second column of \\(X\\))."
  },
  {
    "objectID": "2_linearmodels.html#sec-shorthand",
    "href": "2_linearmodels.html#sec-shorthand",
    "title": "2  Essentials of Normal Linear Models",
    "section": "2.5 Model shorthand notation",
    "text": "2.5 Model shorthand notation\nIn R, a qualitative (categorical) variable is called a factor, and its categories are called levels. For example, variable \\(\\texttt{Sex}\\) in the birth weight data (above) has levels coded “M” for ‘Boy’ and “F” for ‘Girl’. It may not be obvious to R whether a variable is quantitative or qualitative. For example, a qualitative variable called \\(\\texttt{Grade}\\) might have categories 1, 2 and 3. If was included in a model, R would treat it as quantitative unless we declare it to be a factor, which we can do with the command:\n\\(\\texttt{grade = as.factor(grade)}\\)\nA convenient model-specification notation has been developed from which the design matrix \\(X\\) can be constructed. Below, \\(E, F, \\ldots\\) denote generic quantitative (continuous) or qualitative (categorical) variables. Terms in this notation may take the following forms:\n\n\\(1\\) : a column of 1’s to accommodate an intercept term (the \\(\\alpha\\)’s of Table 2.2 ). This is included in the model by default.\n\\(E\\) : variable \\(E\\) is included in the model. The design matrix includes \\(k_E\\) columns for \\(E\\). If \\(E\\) is quantitative, \\(k_E = 1\\). If E is qualitative, \\(k_E\\) is the number of levels of \\(E\\) minus 1.\n\\(E +F\\) : both \\(E\\) and \\(F\\) are included the model. The design matrix includes \\(k_E +k_F\\) columns accordingly.\n\\(E : F\\) (sometimes \\(E \\cdot F\\)) : the model includes an interaction between \\(E\\) and \\(F\\); each column that would be included for \\(E\\) is multiplied by each column for \\(F\\) in turn. The design matrix includes \\(k_E \\times k_F\\) columns accordingly.\n\\(E * F\\) : shorthand for \\(1 + E + F + E : F\\): useful for crossed models where \\(E\\) and \\(F\\) are different factors. For example, \\(E\\) labels age groups; \\(F\\) labels medical conditions.\n\\(E/F\\) : shorthand for \\(1 + E + E : F\\): useful for nested models where \\(F\\) is a factor whose levels have meaning only within levels of factor \\(E\\). For example, \\(E\\) labels different hospitals; \\(F\\) labels wards within hospitals.\n\\(\\text{poly}(E; \\ell)\\) : shorthand for an orthogonal polynomial, wherein \\(x\\) contains a set of mutually orthogonal columns containing polynomials in \\(E\\) of increasing order, from order \\(1\\) through order \\(\\ell\\).\n\\(-E\\) : shorthand for removing a term from the model; for example \\(E * F -E\\) is short for \\(1 + F + E : F\\).\n\\(I()\\) : shorthand for an arithmetical expression (not to be confused with the indicator function defined above). For example, \\(I(E + F)\\) denotes a new quantitative variable constructed by adding together quantitative variables \\(E\\) and \\(F\\). This would cause an error if either \\(E\\) or \\(F\\) has been declared as a factor. What would happen in this example if we omitted the \\(I(\\cdot)\\) notation?\n\nThe notation uses “~” as shorthand for “is modelled by” or “is regressed on”. For example,\n\nWeight is regressed on age-group and sex with no interaction between them: \\[\n\\texttt{Weight} \\sim \\texttt{Age} + \\texttt{Sex}\n\\] as for the birthweight data in Figure 1.2c.\nWell being is regressed on age-group and income-group, where income is thought to affect wellbeing differentially by age: \\[\n\\texttt{Wellbeing} \\sim \\texttt{Age} * \\texttt{Income}\n\\]\nClass of degree is regressed on school of the university and on degree subject within the school: \\[\n\\texttt{DegreeClass} \\sim \\texttt{School/Subject}\n\\]\nYield of wheat is regressed on seed-variety and annual rainfall: \\[\n\\texttt{Yield} \\sim \\texttt{Variety} + \\texttt{poly}(\\texttt{Rainfall}, 2)\n\\]\nProfit is regressed on amount invested: \\[\n\\texttt{Profit}\\sim \\texttt{Investment}- 1\n\\] (no intercept term, that is a regression through the origin)."
  },
  {
    "objectID": "2_linearmodels.html#exercises",
    "href": "2_linearmodels.html#exercises",
    "title": "2  Essentials of Normal Linear Models",
    "section": "2.6 Exercises",
    "text": "2.6 Exercises\n\n\n\n\n\n\nImportant\n\n\n\nPlease note that these questions are currently unchecked. I aim to look at them very soon but a apologize is anything is incomplete or there are errors.\n\n\n2.1. An extra model which could have been considered for the Birth weight data example would be one that says that \\(\\texttt{Weight}\\) is different for girls and boys, but does not depend on gestational age.\nWrite down the equation corresponding to this model. Then, load the birth weight data into RStudio and fit the model. How are the fitted model parameters related to the overall birth weight mean and the mean birth weights of the girls and boys? Is this a good fit to the data? Is Sex statistically significant?\n2.2. In an experiment to investigate Ohm’s Law, \\(V=IR\\) where \\(V\\) is Voltage, I is current and \\(R\\) is resistance of the material, the following data3 were recorded:\n\n\nTable 2.4: Experimental verification of Ohm’s Law\n\n\nVoltage (Volts)\n4\n8\n10\n12\n14\n18\n20\n24\n\n\nCurrent (mAmps)\n11\n24\n30\n36\n40\n53\n58.5\n70\n\n\n\n\nDoes this data support Ohm’s Law? What is the resistance of the material used?\n2.3 In an investigation4 into the effect of eating on pulse rate, 6 men and 6 women were tested before and after a meal, with the following results:\n\n\nTable 2.5: At rest pulse rate before and after a meal for men and women\n\n\nMen\nbefore\n105\n79\n79\n103\n87\n97\n\n\n\nafter\n109\n87\n86\n109\n100\n101\n\n\nWomen\nbefore\n74\n73\n82\n78\n86\n77\n\n\n\nafter\n82\n80\n90\n90\n93\n81\n\n\n\n\nSuggest a suitable model for this situation and write down the corresponding design matrix. Calculate the parameter estimates using the matrix regression estimation equation.\nPerform an appropriate analysis in R to find out if there is evidence to suggest that the change in pulse rate due to a meal is the same for men and women.\n2.4 A laboratory experiment5 was performed into the effect of seasonal floods on the growth of barley seedlings in a incubator, as measured by their height in mm. Three types of barley seed (Goldmarker, Midas, Igri) were used with two watering condition (Normal and Waterlogged). Further, each combination was repeated four times on different shelves in the laboratory incubator (Top, Second, Third and Bottom shelf). The data are available in the file barley.csv\nSuggest a suitable model for this situation. Identify the response and explanatory variables and list the levels for any qualitative variables. Write down the design matrix for each model you consider.\nPerform appropriate analyses to test if each of the following are important: (a) watering condition, (b) type of barley seed, and (c) shelf position.\nIn the analysis, do not include any interactions involving shelf position. If you find a significant interaction between watering condition and type of barley seed, carefully interpret the parameter estimates."
  },
  {
    "objectID": "3_GLM-Theory.html#motivating-examples",
    "href": "3_GLM-Theory.html#motivating-examples",
    "title": "3  GLM Theory",
    "section": "3.1 Motivating examples",
    "text": "3.1 Motivating examples\nWe cannot always assume that the dependent variable \\(Y\\) is normally distributed. For example, for the beetle mortality data in Table 1.1, suppose each beetle subjected to a dose \\(x_i\\) has a probability \\(p_i\\) of being killed. Then, the number of beetles killed \\(Y_i\\) out of a total number \\(m_i\\) at dose-level \\(x_i\\) will have a \\(\\text{Bin}(m_i,p_i)\\) distribution with probability mass function\n\\[\n\\text{Pr}(y_i ;~ p_i,m_i) = \\left(\\begin{array}{c} m_i\\\\ y_i \\end{array} \\right) p_i^{y_i} (1-p_i)^{m_i-y_i}\n\\tag{3.1}\\] where \\(y_i\\) takes values in \\(\\{0,1,\\dots,m_i\\}\\).\nTable 3.1 contains seasonal data on tropical cyclones for 13 seasons. Suppose that, within season \\(i\\), there is a constant probability \\(\\lambda_i dt\\) of a cyclone occurring in any short time-interval \\(dt\\). Then the total number of cyclones \\(Y_i\\) during season \\(i\\) will have a Poisson distribution with mean \\(\\lambda_i\\), that is \\(Y_i\\sim \\text{Po}(\\lambda_i)\\), with probability mass function \\[\n\\text{Pr}(y_i ;~ \\lambda_i) = \\frac{\\lambda_i^{y_i} e^{-\\lambda_i} }{y_i!}\n\\tag{3.2}\\] where \\(y_i\\) takes values in \\(\\{0,1,2,\\dots\\}\\).\n\n\nTable 3.1: Numbers of tropical cyclones in \\(n = 13\\) successive seasons1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeason\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\nNo of cyclones\n6\n5\n4\n6\n6\n3\n12\n7\n4\n2\n6\n7\n4\n\n\n\n\nIn these two examples, we have non-normal data and would like to know whether and how the dependent variable \\(Y_i\\) depends on the covariate \\(x_i\\) or \\(i.\\)\n\n\nCode\nbeetle = read.table(\"https://rgaykroyd.github.io/MATH3823/Datasets/beetle.txt\", header=T)\n\ndose = beetle$dose\nmortality = beetle$died/beetle$total\n\nplot(dose, mortality, pch=16,\n     xlim=c(1.65, 1.90), xlab =\"Dose\",\n     ylim=c(-0.1, 1.1),  ylab=\"Mortality\")\nabline(h=c(0,1), lty=2)\n\nseason = 1:13\ncyclones = c(6,5,4,6,6,3,12,7,4,2,6,7,4)\n\nplot(season, cyclones, pch=16,\n     ylim=c(0,12),\n     xlab=\"Season\", ylab=\"No. of cyclones\")\n\n\n\n\n\n\n\n\n(a) Binomial model\n\n\n\n\n\n\n\n(b) Poisson model\n\n\n\n\nFigure 3.1: Examples of non-normally distributed data.\n\n\n\nGeneralized linear models provide a modelling framework for data analysis in the non-normal setting. We will revisit the beetle mortality and cyclone data sets after describing the structure of a generalized linear model."
  },
  {
    "objectID": "3_GLM-Theory.html#sec-glmstructure",
    "href": "3_GLM-Theory.html#sec-glmstructure",
    "title": "3  GLM Theory",
    "section": "3.2 The GLM structure",
    "text": "3.2 The GLM structure\nA generalized linear model relates a continuous or discrete response variable \\(Y\\) to a set of explanatory variables \\(\\mathbf{x}=(x_1, \\ldots, x_p)\\). The model contains three parts:\nRandom part: The probability (mass or density) function of \\(Y\\) is assumed to belong to the two-parameter exponential family of distributions with parameters \\(\\theta\\) and \\(\\phi:\\)\n\\[\nf(y; \\theta, \\phi) = \\exp \\left\\{ \\frac{y \\theta - b(\\theta)}\n                  {\\phi} + c(y, \\phi) \\right\\},\n\\tag{3.3}\\] where \\(\\phi>0\\). Here, \\(\\theta\\) is called the canonical or natural parameter of the distribution and \\(\\phi\\) is called the scale parameter. We show below that the mean \\(\\mbox{E}[Y]\\) depends only on \\(\\theta\\), and \\(\\mbox{Var}[Y]\\) depends on \\(\\phi\\) and possibly also \\(\\theta\\). Various choices for functions \\(b(\\cdot)\\) and \\(c(\\cdot)\\) produce a wide variety of familiar distributions (see below). Sometimes we may set \\(\\phi=1\\); then Equation 3.3 is called the one-parameter exponential family.\nFurther, note that in some references to generalized linear models (such as Dobson and Barnett, 3rd edn.), \\(\\phi\\) does not appear at all in the exponential family formula Equation 3.3, instead it is absorbed into \\(\\theta\\) and \\(b(\\theta)\\).\nIn this module, we will generally assume that each observation \\(Y_i\\), \\(i=1,\\dots,n\\), is independently drawn from an exponential family where \\(\\theta\\) depends on the covariates. Thus we write \\[\nf(y_i; \\theta_i, \\phi) = \\exp \\left\\{ \\frac{y_i \\theta_i - b(\\theta_i)}   {\\phi} + c(y_i, \\phi) \\right\\}.\n\\] Note the subscripts on both \\(y\\) and \\(\\theta\\), and hence the observations may not be identically distributed.\nSystematic part: This is a linear predictor: \\[\n\\eta = \\sum_{j=1}^p \\beta_j x_j.\n\\tag{3.4}\\] Note that the symbol \\(\\eta\\) is pronounced eta.\nLink function: This is a one-to-one function providing the link between the linear predictor \\(\\eta\\) and the mean \\(\\mu = \\mbox{E}[Y]\\):\n\\[\n\\eta = g(\\mu), \\quad \\mbox{and} \\quad \\mu  = g^{-1}(\\eta) = h(\\eta).\n\\tag{3.5}\\]\nHere, \\(g(\\mu)\\) is called the link function, and \\(h(\\eta)\\) is called the inverse link function.\nWe will now discuss each of these parts in more detail."
  },
  {
    "objectID": "3_GLM-Theory.html#the-random-part-of-a-glm",
    "href": "3_GLM-Theory.html#the-random-part-of-a-glm",
    "title": "3  GLM Theory",
    "section": "3.3 The random part of a GLM",
    "text": "3.3 The random part of a GLM\nWe begin with some examples of exponential family members.\n\nExample: Poisson distribution\nIf \\(Y\\) has a Poisson distribution with parameter \\(\\lambda\\), that is \\(Y \\sim \\text{Po}(\\lambda)\\), then \\(Y\\) takes values in \\(\\{0,1,2,\\dots\\}\\) and has probability mass function: \\[\nf(y) = \\frac{e^{-\\lambda} \\lambda^y} {y!}  \n= \\exp \\left\\{y \\log \\lambda - \\lambda - \\log y! \\right\\},\n\\tag{3.6}\\] which has the form of Equation 3.3 with components as in Table 3.2.\n\n\nTable 3.2: Exponential model components for the Poisson\n\n\n\\(\\theta\\)\n\\(\\phi\\)\n\\(b(\\theta)\\)\n\\(c(y,\\phi)\\)\n\n\n\n\n\\(\\log\\lambda\\)\n\\(1\\)\n\\(\\lambda=e^\\theta\\)\n\\(-\\log y!\\)\n\n\n\n\nFor example, to model the cyclone data in Table 3.1, we might simply assume that the number of cyclones in each season has a Poisson distribution, assuming a constant rate \\(\\lambda\\) across all seasons \\(i\\). That is \\(Y_i \\sim \\text{Po}(\\lambda).\\) The parameter would be simply estimated by the sample mean, \\(\\hat\\lambda=\\bar y=\\) 5.5384615.\n\n\n\nCode\nseason = 1:13\ncyclones = c(6,5,4,6,6,3,12,7,4,2,6,7,4)\nn = length(cyclones)\n\nplot(season, cyclones, pch=16,\n     ylim=c(0,12),\n     xlab=\"Season\", ylab=\"No. of cyclones\")\nabline(h=mean(cyclones), lty=2)\n\nhist(cyclones, breaks=1:13-0.5, main=\"\")\nfitted=n*dpois(0:12,mean(cyclones))\nlines(0:12, fitted, type='h', lwd=3)\n\n\n\n\n\n\n\n\n(a) No. of cyclones against season with mean\n\n\n\n\n\n\n\n(b) Fitted Poisson model assuming constant rate\n\n\n\n\nFigure 3.2: Poisson model fitted to cyclone data.\n\n\n\n\nExample: Binomial distribution\nLet \\(Y\\) have a Binomial distribution, that is \\(Y \\sim \\text{Bin}(m, p)\\), with \\(m\\) fixed. Then \\(Y\\) is discrete, taking values in \\(\\{0,1,\\dots,m\\}\\), and has probability mass function: \\[\nf(y) = {m \\choose y} p^y (1 - p)^{m - y} = {m \\choose y} \\left(\\frac{p}{1-p} \\right)^y (1 - p)^m\n\\] which can be re-written as \\[\nf(y)  = \\exp \\left\\{ y\\ \\mbox{logit } p  + m \\log (1-p) + \\log {m \\choose y}\\right\\},\n\\tag{3.7}\\] which has the form of Equation 3.3 with, \\[\n\\theta=\\text{logit} \\ p = \\log \\left( \\frac{p}{1-p}\\right),\n\\] and with components as in Table 3.3.\n\n\nTable 3.3: Exponential model components for the Binomial\n\n\n\\(\\theta\\)\n\\(\\phi\\)\n\\(b(\\theta)\\)\n\\(c(y,\\phi)\\)\n\n\n\n\n\\(\\mbox{logit }p\\)\n\\(1\\)\n\\(m\\log(1+e^\\theta)\\)\n\\(\\log{m\\choose y}\\)\n\n\n\n\nNote that it can be shown that \\(-m\\log(1-p)=m\\log(1+e^\\theta)\\) – see Exercises.\n\n\n\nCode\nbeetle = read.table(\"https://rgaykroyd.github.io/MATH3823/Datasets/beetle.txt\", header=T)\n\ndose = beetle$dose\nmortality = beetle$died/beetle$total\n\nplot(dose, mortality, pch=16,\n     xlim=c(1.65, 1.90), xlab =\"Dose\",\n     ylim=c(-0.1, 1.1),  ylab=\"Mortality\")\nabline(h=c(0,1), lty=2)\n\ny = cbind(beetle$died, beetle$total-beetle$died)\nglm.fit = glm(y ~ dose, family=binomial(link='logit'))\n\noutput.dose = seq(1.6,1.95,0.001)\nfitted = predict(glm.fit, data.frame(dose=output.dose), type=\"response\")\nlines(output.dose, fitted)\n\n\n\n\n\nFigure 3.3: Binomial model fitted to beetle data.\n\n\n\n\n\nExample: Normal distribution\nLet \\(Y\\) have a Normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\), that is \\(Y\\sim N(0, \\sigma^2)\\). Then \\(Y\\) takes values on the whole real line and has probability density function\n\\[\\begin{align*}\nf(y; \\mu, \\sigma^2) &= \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left\\{ \\frac{-1}{2\\sigma^2} (y - \\mu)^2 \\right\\}, \\notag \\\\\n&= \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left\\{-\\frac{y^2}{2 \\sigma^2} + \\frac{y\\mu}{\\sigma^2} - \\frac{\\mu^2}{2 \\sigma^2}\\right\\}\\\\\n&= \\exp \\left\\{ \\frac{y \\mu - \\mu^2/2}{\\sigma^2} + \\left[\\frac{-y^2}{2\\sigma^2} - \\frac{1}{2} \\log (2 \\pi \\sigma^2)\n\\right]\\right\\},\n\\end{align*}\\] which has the form of Equation 3.3 with components as in Table 3.4.\n\n\nTable 3.4: Exponential model components for the Gaussian\n\n\n\n\n\n\n\n\n\\(\\theta\\)\n\\(\\phi\\)\n\\(b(\\theta)\\)\n\\(c(y,\\phi)\\)\n\n\n\n\n\\(\\mu\\)\n\\(\\sigma^2\\)\n\\(\\theta^2/2\\)\n\\(-\\frac{y^2}{2\\phi} - \\frac{1}{2} \\log (2 \\pi \\phi)\\)\n\n\n\n\nFrom the usual regression point of view, we write \\(y = \\alpha + \\beta x + \\epsilon\\), with \\(\\epsilon \\sim N(0, \\sigma^2)\\). From the point of view of a generalized linear model, we write \\(Y \\sim N(\\mu, \\sigma^2)\\) where \\(\\mu(x) = \\alpha + \\beta x\\)."
  },
  {
    "objectID": "3_GLM-Theory.html#sec-exponential-family",
    "href": "3_GLM-Theory.html#sec-exponential-family",
    "title": "3  GLM Theory",
    "section": "3.4 Moments of exponential-family distributions",
    "text": "3.4 Moments of exponential-family distributions\nIt is straightforward to find the mean and variance of \\(Y\\) in terms of \\(b(\\theta)\\) and \\(\\phi\\). Since we want to explore the dependence of \\(\\mbox{E}[Y]\\) on explanatory variables, this property makes the exponential family very convenient.\n\nProposition 3.1 For random variables in the exponential family: \\[\n\\mbox{E}[Y] = b'(\\theta), \\quad \\mbox{and } \\quad \\mbox{Var}[Y] =  b''(\\theta)\\phi.\n\\tag{3.8}\\]\n\nProof We give the proof for a continuous random variables. For the discrete case, replace all integrals by sums – see Exercises.\nStarting with the simple property that all probability density functions integrate to 1, we have \\[\n1 = \\int \\exp \\left\\{ \\frac{y \\theta - b(\\theta)}{\\phi} + c(y,\\phi)\\right\\} dy\n\\] and then differentiating both sides with respect to \\(\\theta\\) gives \\[\n0 = \\int \\left[\\frac{ y - b'(\\theta)}{\\phi} \\right]\\exp \\left\\{ \\frac{y \\theta - b(\\theta)}{\\phi} + c(y,\\phi)\\right\\}\\ dy.\n\\tag{3.9}\\] Next, using the definition of the exponential family to simplify the equation gives \\[\n0 = \\int \\left[\\frac{ y - b'(\\theta)}{\\phi} \\right] f(y; \\theta)\\ dy\n\\] and expanding the brackets leads to \\[\n0 = \\frac{1}{\\phi} \\left(\\int y f(y; \\theta) dy - b'(\\theta) \\int f(y;\\theta)\\ dy \\right).\n\\] The first integral is simply the expectation of \\(Y\\) and the second is the integral of the probability density function of \\(Y\\), and hence \\[\n0 = \\frac{1}{\\phi} \\left(\\mbox{E}[Y] - b'(\\theta)\\right)\n\\] which implies that \\[\n\\mbox{E}[Y] = b'(\\theta),\n\\tag{3.10}\\] which proves the first part of the proposition.\nDifferentiating Equation 3.9 by parts and then using the definition of the exponential family to simplify again yields \\[\n0 = \\int \\left\\{ -\\frac{b''(\\theta)}{\\phi} + \\left[\\frac{ y - b'(\\theta)}{\\phi} \\right]^2 \\right\\} f(y; \\theta)\\ dy\n\\] and using Equation 3.10 gives, \\[\n0  =  -\\frac{b''(\\theta)}{\\phi} +\\int \\left[\\frac{ y - \\mbox{E}[Y]}{\\phi} \\right]^2  f(y; \\theta)\\ dy\n\\] \\[\n0 = -\\frac{b''(\\theta)}{\\phi} + \\frac{\\mbox{Var}[Y]}{\\phi^2}\n\\] which implies that \\[\n\\mbox{Var}[Y] = \\phi \\ b''(\\theta).\n\\] which proves the second part of the proposition.\nTogether, these two results allow us to write down the expectation and variance for any random variable once we have shown that it is a member of the exponential family.\n\nExample: Poisson and normal distribution moments\n\n\nTable 3.5: Summary of moment calculations via exponential family properties \n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\theta\\)\n\\(b(\\theta)\\)\n\\(\\phi\\)\n\\(\\mbox{E}[Y]=b'(\\theta)\\)\n\\(b''(\\theta)\\)\n\\(\\mbox{Var}[Y]=b''(\\theta)\\phi\\)\n\n\n\n\nPoisson, \\(Po(\\lambda)\\)\n\\(\\log \\lambda\\)\n\\(e^\\theta\\)\n\\(1\\)\n\\(e^\\theta=\\lambda\\)\n\\(e^\\theta\\)\n\\(e^\\theta\\times 1=\\lambda\\)\n\n\nNormal, \\(N(\\mu,\\sigma^2)\\)\n\\(\\mu\\)\n\\(\\theta^2/2\\)\n\\(\\sigma^2\\)\n\\(\\theta=\\mu\\)\n\\(1\\)\n\\(1\\times \\sigma^2=\\sigma^2\\)"
  },
  {
    "objectID": "3_GLM-Theory.html#sec-systematic",
    "href": "3_GLM-Theory.html#sec-systematic",
    "title": "3  GLM Theory",
    "section": "3.5 The systematic part of the model",
    "text": "3.5 The systematic part of the model\nThe second part of the generalized linear model, the linear predictor, is given in as \\(\\eta = \\sum_{j=1}^p \\beta_j x_j\\), where \\(x_j\\) is the \\(j\\)th explanatory variable (with \\(x_1=1\\) for the intercept). Now, for each observation \\(y_i,\\ i=1,\\dots,n\\), the explanatory variables may differ. To make explicit this dependence on \\(i\\), we write: \\[\n\\eta_i = \\sum_{j=1}^p \\beta_j x_{ij},\n\\tag{3.11}\\] where \\(x_{ij}\\) is the value of the \\(j\\)th explanatory variable on individual \\(i\\) (with \\(x_{i1}=1\\)). Rewriting this in matrix notation: \\[\n\\eta = X \\beta,\n\\tag{3.12}\\] where now \\(\\boldsymbol{\\eta} = (\\eta_1,\\dots,\\eta_n)\\) is a vector of linear predictor variables, \\(\\boldsymbol{\\beta} = (\\beta_1,\\dots,\\beta_p)\\) is a vector of regression parameters, and \\(X\\) is the \\(n\\times p\\) design matrix.\nRecall from Section 1.4 and Section 2.2 that we are concerned with two kinds of explanatory variable:\n\nQuantitative — for example, \\(x \\in (-\\infty, \\infty)\\) etc.\nQualitative — for example, \\(x \\in \\{A, B, C\\}\\) etc.\n\nAs discussed in Section 2.4, each quantitative variable is represented in \\(X\\) by an \\(n \\times 1\\) column vector. Each qualitative variable, with \\(k+1\\) levels, say, is represented by a dummy \\(n \\times k\\) matrix (one column, usually the first, being dropped to avoid identification problems) of 0’s and 1’s ."
  },
  {
    "objectID": "3_GLM-Theory.html#the-link-function",
    "href": "3_GLM-Theory.html#the-link-function",
    "title": "3  GLM Theory",
    "section": "3.6 The link function",
    "text": "3.6 The link function\nIn Section 3.2 we saw that the random part of an observation, \\(y\\), might be described by a member of the exponential family. We also saw, that the systematic part of \\(y\\) might be described using a linear predictor, \\(\\eta,\\) of the explanatory variables. Further, we introduced the notion of a link function \\(\\eta = g(\\mu)\\) to bring these two parts together, where \\(\\mu\\) is the mean of \\(y\\).\nOccasionally, the choice of link function \\(g(\\mu)\\) is motivated by theory underlying the data at hand. For example, in a dose–response setting, the appropriate model might be motivated by the solution to a set of partial differential equations describing the flow through the body of a dose of a drug.\nWhen there is no compelling underlying theory, however, we typically choose a link function that will transform a restricted range of the dependent variable onto the whole real line. For example, when observations are typically positive, so we have \\(\\mu>0\\), we might choose the logarithmic link: \\[\ng(\\mu) = \\log(\\mu).\n\\tag{3.13}\\]\nWhen observations are binomial counts from \\(B(m,p), \\ 0 < p < 1\\), with mean \\(\\mu = mp\\), we might choose the logit link from \\[\n\\eta = g(\\mu) = \\text{logit}(\\mu/m)= \\text{logit}(p) = \\log\\{p/(1-p)\\}\n\\tag{3.14}\\] or the probit link which is the inverse of the cumulative distribution function of the \\(N(0,1)\\) distribution: \\[\n\\eta = g(\\mu) = \\Phi^{-1}(\\mu/m) = \\Phi^{-1}(p),\n\\tag{3.15}\\] or the complementary log-log (cloglog) link: \\[\n\\eta = g(\\mu) = \\log(-\\log(1-\\mu/m))= \\log(-\\log(1-p)),\n\\tag{3.16}\\] or the cauchit link which is the inverse of the cumulative distribution function of the Cauchy (\\(t_1\\)) distribution: \\[\n\\eta = g(\\mu) = \\tan(\\pi(\\mu/m-\\tfrac{1}{2})) = \\tan(\\pi(p-\\tfrac{1}{2})).\n\\tag{3.17}\\] Figure 3.4 shows these link functions for proportions fitted to the beetle mortality data. This demonstrates that the logit and probit links are very similar, that the complementary log-log link fits these data slightly better in the extremes, but that the cauchit link fits these data quite poorly in the extremes.\n\n\nCode\npar(mar=c(3.5,3.5,0,0), mgp=c(2.5,0.75,0))\nbeetle = read.table(\"data/beetle.txt\", header=T)\n\ndose = beetle$dose\nmortality = beetle$died/beetle$total\nym = cbind(beetle$died,beetle$total-beetle$died)\n\nplot(dose, mortality, pch=16,\n     xlim=c(1.65, 1.90), xlab =\"Dose\",\n     ylim=c(-0.1,1.1), ylab=\"Mortality\")\nabline(h=c(0,1), lty=2)\n\nglm.fit = glm(ym~dose, family=binomial(link='logit'))\ntmp = seq(1.6,1.95,0.001)\nfitted = predict(glm.fit, data.frame(dose=tmp), type=\"response\")\nlines(tmp, fitted)\n\nplot(dose, mortality, pch=16,\n     xlim=c(1.65, 1.90), xlab =\"Dose\",\n     ylim=c(-0.1,1.1), ylab=\"Mortality\")\nabline(h=c(0,1), lty=2)\n\nglm.fit = glm(ym~dose, family=binomial(link='probit'))\ntmp = seq(1.6,1.95,0.001)\nfitted = predict(glm.fit, data.frame(dose=tmp), type=\"response\")\nlines(tmp, fitted)\n\n\nplot(dose, mortality, pch=16,\n     xlim=c(1.65, 1.90), xlab =\"Dose\",\n     ylim=c(-0.1,1.1), ylab=\"Mortality\")\nabline(h=c(0,1), lty=2)\n\nglm.fit = glm(ym~dose, family=binomial(link='cloglog'))\ntmp = seq(1.6,1.95,0.001)\nfitted = predict(glm.fit, data.frame(dose=tmp), type=\"response\")\nlines(tmp, fitted)\n\nplot(dose, mortality, pch=16,\n     xlim=c(1.65, 1.90), xlab =\"Dose\",\n     ylim=c(-0.1,1.1), ylab=\"Mortality\")\nabline(h=c(0,1), lty=2)\n\nglm.fit = glm(ym~dose, family=binomial(link='cauchit'))\ntmp = seq(1.6,1.95,0.001)\nfitted = predict(glm.fit, data.frame(dose=tmp), type=\"response\")\nlines(tmp, fitted)\n\n\n\n\n\n\n\n\n(a) Logit link\n\n\n\n\n\n\n\n(b) Probit link\n\n\n\n\n\n\n\n\n\n(c) cloglog link\n\n\n\n\n\n\n\n(d) Cauchit link\n\n\n\n\nFigure 3.4: Dose–response curves fitted to the beetle mortality data from Table 1.1 with different choices of link function.\n\n\n\nA mathematically and computationally convenient choice of link function \\(g(\\mu)\\) can be constructed by setting: \\[\n\\theta =\\eta,\n\\tag{3.18}\\] where \\(\\theta\\) is the canonical parameter of the exponential family as defined in Equation 3.3. Then, Equation 3.8 shows that the mean \\(\\mu\\) is a function of \\(\\theta\\) and therefore, Equation 3.18 indirectly provides a link between \\(\\mu\\) and \\(\\eta\\). That is, Equation 3.18 implicitly defines a link function \\(\\eta=g(\\mu)\\). But what is the form of this \\(g(\\cdot)\\)?\nFrom Equation 3.8, \\[\n\\mu = b'(\\theta).\n\\] So, provided function \\(b'(\\cdot)\\) has an inverse \\((b')^{-1}(\\cdot)\\), we may write \\[\n\\theta = (b')^{-1}(\\mu).\n\\tag{3.19}\\] Now, from Equation 3.5, \\(g(\\mu) = \\eta\\), so using Equation 3.18: \\[\ng(\\mu) = \\theta = (b')^{-1}(\\mu),\n\\tag{3.20}\\] from Equation 3.19. This makes explicit the \\(g(\\mu)\\) that is implicitly asserted by Equation 3.18. The function produced form Equation 3.20 is called the canonical link function.\n\nProposition 3.2 For the canonical link function, \\[\ng'(\\mu) = 1/b''(\\theta).\n\\]\n\nProof: From Proposition 3.1, \\(\\mu = E[Y]=b'(\\theta)\\), so \\[\n\\frac{\\text{d} \\mu }{\\text{d} \\theta} = b''(\\theta).\n\\] From Equation 3.20, for the canonical link function, we have \\(\\theta = g(\\mu)\\), so \\[\n\\frac{\\text{d} \\theta }{ \\text{d} \\mu} = g'(\\mu).\n\\] Now \\(\\text{d} \\theta /\\text{d} \\mu = \\left(\\text{d} \\mu / \\text{d} \\theta\\right)^{-1}\\) and hence \\[\ng'(\\mu) = 1/b''(\\theta).\n\\] Which proves the proposition.\n\nExample: Poisson canonical link function\nFor the Poisson distribution \\(\\text{Po}(\\lambda)\\), we have from Table 3.2 that \\(b(\\theta) = e^\\theta\\). Therefore, \\[\nb'(\\theta) = e^\\theta,\n\\] so the inverse of function \\(b'(\\cdot)\\) exists and is the inverse of the exponential function, which is the logarithmic function. Then, applying Equation 3.20 \\[\ng(\\mu) = \\log(\\mu)\n\\] Thus the canonical link for the Poisson distribution is \\(\\log\\).\n\n\nExample: Normal canonical link function\nFor the Normal distribution \\(N(\\mu, \\sigma^2)\\), we have from Table 3.4 that \\(b(\\theta) = \\theta^2/2\\). Therefore \\[\nb'(\\theta) = \\theta\n\\] so the inverse of function \\(b'(\\cdot)\\) exists and is the inverse of the identity function, which is the identity function. (The identity function is that which maps a value onto itself.) Then, applying Equation 3.20, \\[\ng(\\mu) = \\mu. \\label{eq:canonical.linkfun.normal}\n\\] Thus the canonical link for the Normal distribution is the identity function.\n\nFor many models, \\(\\mu\\) has a restricted range, but we would like \\(\\eta\\) to have unlimited range. It turns out, for several members of the exponential family, that the canonical link function provides \\(\\eta\\) with unlimited range. However, Table 3.6 shows that this is not always so.\n\n\nTable 3.6: Canonical link functions and their ranges (see McCullagh and Nelder, 2nd Edn., p291 with \\(\\dagger\\)binomial distribution with index \\(m\\) and mean \\(\\mu\\) and \\(\\ddagger\\)gamma distribution with mean \\(\\mu\\) (see Exercises for details).\n\n\n\n\n\n\n\n\n\n\n\\(f(y)\\)\nRange of \\(\\mu\\)\n\\(b(\\theta)\\)\n\\(\\mu=b'(\\theta)\\)\nCanonical link, \\(g(\\mu)\\)\nRange of \\(\\eta\\)\n\n\n\n\nNormal\n\\((-\\infty, \\infty)\\)\n\\(\\frac12 \\theta^2\\)\n\\(\\theta\\)\n\\(\\mu\\)\n\\((-\\infty, \\infty)\\)\n\n\nPoisson\n\\((0,\\infty)\\)\n\\(e^\\theta\\)\n\\(e^\\theta\\)\n\\(\\log\\mu\\)\n\\((-\\infty, \\infty)\\)\n\n\nBinomial\\(\\dagger\\)\n\\((0, m)\\)\n\\(m\\log(1-e^\\theta)\\)\n\\(m/(1+e^{-\\theta})\\)\n\\(\\mbox{logit}(\\mu/m)\\)\n\\((-\\infty, \\infty)\\)\n\n\nGamma\\(\\ddagger\\)\n\\((0,\\infty)\\)\n\\(-\\log (-\\theta)\\)\n\\(-\\theta^{-1}\\)\n\\(-\\mu^{-1}\\)\n\\((-\\infty, 0)\\)\n\n\n\n\nWhy is the canonical link function Equation 3.20 convenient? The assertion Equation 3.18 means that, in the exponential-family formula Equation 3.3, we can simply substitute the linear predictor \\[\n\\eta=\\sum_j \\beta_j x_j\n\\] from Equation 3.4 in place of \\(\\theta\\), to give: \\[\n    f(y; \\mathbf{x}, \\boldsymbol{\\beta},\\phi) = \\exp \\left\\{ \\frac{y \\left[\\sum_j \\beta_j x_j\\right] - b\\left(\\left[\\sum_j \\beta_j x_j\\right]\\right)}\n                                            {\\phi} + c(y, \\phi) \\right\\},\n\\tag{3.21}\\] where \\(\\mathbf{x}=\\{x_{j}, j=1,\\dots,p\\}\\) and \\(\\boldsymbol{\\beta}=\\{\\beta_j, j=1,\\dots,p\\}\\).\nFurther, suppose we have \\(n\\) independent observations, \\(\\{y_i,\\ i=1,\\ldots,n\\}\\). As discussed in Section 3.5, the explanatory variables \\((x_{1},\\ldots,x_{p})\\) will depend on \\(i\\), and so \\(\\eta\\) will also depend on \\(i\\). Therefore, we attach subscript \\(i\\) to \\(y\\) and to each \\(x_j\\), giving: \\[\nf(y_i; \\mathbf{x}_i, \\boldsymbol{\\beta}, \\phi) =\n\\exp \\left\\{ \\frac{y_i \\left[\\sum_j \\beta_j x_{ij}\\right] - b\\left(\\left[\\sum_j \\beta_j x_{ij}\\right]\\right)}{\\phi}\n+ c(y_i, \\phi) \\right\\}.\n\\tag{3.22}\\] where \\(\\mathbf{x}_i=\\{x_{ij}, j=1,\\dots,p\\}\\) and \\(\\boldsymbol{\\beta}=\\{\\beta_j, j=1,\\dots,p\\}\\).\nBy independence, the joint distribution of all observations \\(\\mathbf{y} = \\{y_i,\\ i=1,\\dots,n\\}\\), with design matrix \\(X = \\{x_{ij},\\ \\ i=1,\\dots,n; j=1,\\dots,p\\},\\) is: \\[\nf(\\mathbf{y}; X, \\boldsymbol{\\beta},\\phi) = \\prod_{i=1}^n f(y_i; \\theta_i, \\phi),\n\\] so\n\\[\n\\log f(\\mathbf{y}; X, \\boldsymbol{\\beta},\\phi) =\n\\sum_{i=1}^n \\log f(y_i; \\theta_i, \\phi)\n\\] then substituting in using Equation 3.22 gives \\[\n\\log f(\\mathbf{y}; X, \\boldsymbol{\\beta},\\phi) =\n\\sum_{i=1}^n\n\\left\\{  \\frac{y_i \\left[\\sum_j \\beta_j x_{ij}\\right] - b\\left(\\left[\\sum_j \\beta_j x_{ij}\\right]\\right)}{\\phi} + c(y_i, \\phi)\\right\\}\n\\] and finally simplifying to give \\[\n\\log f(\\mathbf{y}; X, \\boldsymbol{\\beta},\\phi) =\n\\frac{\\sum_j \\beta_j S_j - \\sum_i b\\left(\\left[\\sum_j \\beta_j x_{ij}\\right]\\right)}\n{\\phi} + \\sum_i c(y_i, \\phi)\n\\tag{3.23}\\] where \\[\nS_j = \\sum_{i=1}^n  y_i  x_{ij}.\n\\] Thus, in the log-likelihood Equation 3.23, it is only the first term that involves both the observations \\(\\mathbf{y}=\\{y_i,\\ i=1,\\dots,n\\}\\) and the parameters \\(\\boldsymbol{\\beta}=\\{\\beta_j, j=1,\\dots,p\\}\\), and this term depends on the observations only through the statistics \\(\\mathbf{S}=\\{S_j, j=1,\\dots,p\\}\\) – these are called sufficient statistics, and their appearance in Equation 3.23 confers both theoretical and practical advantages."
  },
  {
    "objectID": "3_GLM-Theory.html#exercises",
    "href": "3_GLM-Theory.html#exercises",
    "title": "3  GLM Theory",
    "section": "3.7 Exercises",
    "text": "3.7 Exercises\n3.1 Consider the beetle data again, see Table 1.1, but suppose that we had only been given the \\(y\\) values, that is the number killed, and misinformed that each came from a sample of size \\(62\\). Further, suppose that we did not know that different doses had been used. That is, we where given data: \\(\\mathbf{y}=\\{6, 13, 18, 28, 52, 53, 61, 60\\}\\) and led to believe that the model \\(Y\\sim \\mbox{Bin}(62,p)\\) was appropriate. Use the given data to estimate \\(p\\). Then, calculate the fitted probabilities and superimpose them on a histogram of the data. [Hint: see the code chunk used to create Figure 3.2.]\n3.2 Verify that, in general, if \\(q = 1/(1+e^{-x})\\) then \\(x = \\log(q/(1-q))\\) and then for the binomial distribution, \\(Y\\sim \\mbox{Bin}(m,p)\\), show that \\[\n-m\\log(1-p)=m\\log(1+e^\\theta)\n\\] where \\(\\theta=\\mbox{logit }p = \\log(p/(1-p))\\).\n3.3 Suppose that \\(Y\\) has a gamma distribution with parameters \\(\\alpha\\) and \\(\\beta\\), that is \\(Y\\sim \\mbox{Gamma}(\\alpha, \\beta)\\), with probability density function \\[\nf(x;\\alpha, \\beta) = \\frac{\\beta^\\alpha x^{\\alpha-1} e^{-\\beta x}}{\\Gamma(\\alpha)} \\quad \\qquad x > 0; \\alpha,\\beta>0.\n\\] Write this in the form of the exponential family and clearly identify \\(\\theta\\), \\(\\phi\\), \\(b(\\theta)\\) and \\(c(y,\\phi)\\) – as in Table 3.2 and Table 3.3.\n3.4 Express the geometric distribution, \\(Y\\sim\\mbox{Geom}(p), 0 < p < 1\\), with probability mass function \\[\nf(y) = (1 - p)^{y-1} p; \\hspace{1cm} y = 1, 2, 3 \\ldots\n\\] as an exponential family distribution, possibly with a scale parameter.\n3.5 Prove Proposition 3.1 assuming that \\(Y\\) follows a discrete distribution. Verify the results for the Poisson in Table 3.5 and then derive similar results for the binomial, \\(Y\\sim \\mbox{Bin}(n,p)\\). From these results, what are the mean and variance of \\(Y\\).\n3.6 Use the properties of exponential families to find the expectation and variance of each of the geometric and gamma distributions defined above.\n[Hint for the gamma, treat \\(1/\\alpha\\) as a scale parameter and let \\(\\theta\\) be a suitable function of \\(\\lambda\\) and \\(\\alpha\\).]\n3.7 Using Proposition 3.2, verify the canonical link function, \\(g(\\mu)\\), for the binomial and gamma distributions shown in Table 3.6."
  },
  {
    "objectID": "4_GLM-Fitting.html#sec-mleiid",
    "href": "4_GLM-Fitting.html#sec-mleiid",
    "title": "4  GLM Estimation",
    "section": "4.1 The identically distributed case",
    "text": "4.1 The identically distributed case\n\n4.1.1 Maximum likelihood estimation\nSuppose we have \\(n\\) independent and identically distributed (i.i.d.) observations \\(y_i,\\ i=1,\\ldots,n\\), where each \\(y_i\\) is sampled from the same exponential family density \\[\nf(y_i; \\theta,\\phi) = \\exp \\left\\{ \\frac{y_i\\theta  - b(\\theta)}{\\phi} + c(y_i, \\phi) \\right\\},\n\\tag{4.1}\\] for \\(i=1,\\dots,n.\\) In this case, the canonical parameter \\(\\theta\\) does not depend on \\(i\\).\nBy independence, the joint distribution of all the observations \\(\\mathbf{y} = \\{y_i,\\ i=1,\\dots,n\\}\\) is: \\[\nf(\\mathbf{y}; \\theta,\\phi) = \\prod_{i=1}^n f(y_i; \\theta, \\phi).\n\\] So, taking logs and then substituting for the probability function using the exponential family form, Equation 3.3, gives \\[\n\\log f(\\mathbf{y}; \\theta,\\phi) = \\sum_{i=1}^n \\log f(y_i; \\theta, \\phi)\n= \\sum_{i=1}^n \\left[\\frac{y_i\\theta  - b(\\theta)}{\\phi} + c(y_i, \\phi)\\right].\n\\] Regarding the observations \\(\\mathbf{y}\\) as constants (which they are, once we have them) and the scale parameter \\(\\phi\\) as a fixed nuisance parameter (whose value we may not know), the log-likelihood as a function of the parameter \\(\\theta\\) of interest is: \\[\nl(\\theta; \\mathbf{y},\\phi)  = n\\left( \\frac{\\bar{y}\\, \\theta  -  b(\\theta)}{\\phi}\\right) + \\mbox{constant},\n\\tag{4.2}\\] where \\(\\bar{y}= \\sum y_i/n.\\)\nWe estimate \\(\\theta\\) by maximizing the log likelihood – i.e. given the data \\(\\mathbf{y}\\), we estimate the value of \\(\\theta\\) to be that value for which the likelihood, and hence the log-likelihood, is greatest.\nWe maximize the log-likelihood by differentiating it and setting it to zero: \\[\n\\frac{d l(\\theta; \\mathbf{y},\\phi)}{d \\theta}  \n= n \\left(\\frac{\\bar{y} -  b'(\\theta)}{\\phi}\\right)\n\\] and hence the MLE for \\(\\theta\\), which we denote \\(\\hat\\theta\\), satisfies \\[\nb'(\\hat\\theta) = \\bar{y}\n\\tag{4.3}\\] and hence \\[\n\\hat\\theta =  (b')^{-1}(\\bar{y}).\n\\tag{4.4}\\]\nFurther, we showed in Proposition 3.1 that \\(\\mbox{E}[Y] = \\mu =b'(\\theta)\\) and if we let \\(\\hat\\mu\\) denote the MLE of \\(\\mu\\), then \\(\\hat\\mu = b'(\\hat{\\theta})\\)1, hence we have \\(\\hat\\mu = \\bar{y}\\). So we find that \\(\\hat\\theta\\) is the value of \\(\\theta\\) for which the theoretical mean \\(\\hat\\mu = b'(\\hat\\theta)\\) matches the sample mean \\(\\bar{y}\\).\n\nExample: MLE of the Poisson distribution\nFor the Poisson distribution, \\(\\text{Po}(\\lambda)\\), we have found that \\(b(\\theta) = e^\\theta\\) and therefore \\(b'(\\theta) = e^\\theta\\). Hence, the MLE of natural parameter \\(\\theta\\) is found as the solution of \\(b'(\\hat\\theta) = e^{\\hat\\theta} = \\bar{y}\\), that is \\(\\hat\\theta = \\log(\\bar{y})\\).\n\n\n\n4.1.2 Estimation accuracy\nFor our i.i.d. sample \\(y_i,\\ i=1,\\ldots,n\\), we have \\(b'(\\hat\\theta) = \\hat\\mu = \\bar y\\). Let \\(\\theta_0\\) be the true value of \\(\\theta\\) with corresponding mean \\(\\mu_0\\), i.e. \\[\nb'(\\theta_0) = \\mu_0.  \n\\tag{4.5}\\] How accurate is \\(\\hat\\theta\\)? We know that \\[\n\\mbox{E}[\\bar Y] = \\mbox{E}\\left[\\frac{1}{n}\\sum_{i=1}^n Y_i\\right]\n= \\frac{1}{n} \\sum_{i=1}^n \\mbox{E}[Y_i]\n= \\mu_0  \n= b'(\\theta_0),\n\\tag{4.6}\\] using Equation 4.5, and \\[\n\\mbox{Var}[\\bar Y] = \\mbox{Var}\\left[\\frac{1}{n} \\sum_{i=1}^n Y_i\\right]\n= \\frac{1}{n^2}  \\sum_{i=1}^n \\mbox{Var}[Y_i]\n\\] because the observations are independent. Then, using the result Equation 3.8 \\[\n\\mbox{Var}[\\bar Y]= \\frac{1}{n} \\ b''(\\theta_0) \\phi.\n\\tag{4.7}\\]\nWe can use Taylor’s theorem to expand \\(b'(\\hat\\theta)\\) about \\(\\theta_0\\): \\[\n\\bar y = b'(\\hat\\theta) \\approx  b'(\\theta_0) + (\\hat\\theta - \\theta_0) b''(\\theta_0),\n\\] which implies that \\[\n(\\hat\\theta - \\theta_0) \\approx  b''(\\theta_0)^{-1}\\{b'(\\hat\\theta) - b'(\\theta_0)\\}  \n= b''(\\theta_0)^{-1}(\\bar Y - \\mu_0),\n\\tag{4.8}\\] using Equation 4.3 and Equation 4.5. We can use Equation 4.8 to get approximations to the mean and variance of \\(\\hat\\theta\\): \\[\n\\mbox{E}[\\hat\\theta - \\theta_0]  \\approx   b''(\\theta_0)^{-1} \\mbox{E}[\\bar Y - \\mu_0]\n= 0,\n\\] using Equation 4.6, so \\[\n\\mbox{E}[\\hat\\theta] \\approx \\theta_0,\n\\tag{4.9}\\] and \\[\n\\mbox{Var}(\\hat\\theta) \\approx \\mbox{E}\\left[(\\hat\\theta - \\theta_0)^2\\right]\n\\] using Equation 4.9, \\[\n\\mbox{Var}(\\hat\\theta) \\approx \\mbox{E}\\left[\\left(b''(\\theta_0)^{-1}(\\bar Y - \\mu_0)\\right)^2\\right]\n\\] using Equation 4.8, \\[\n\\mbox{Var}(\\hat\\theta)\\approx  \\left(b''(\\theta_0)\\right)^{-2} \\mbox{Var}[\\bar Y]  \n\\] using Equation 4.6, \\[\n\\mbox{Var}(\\hat\\theta) = \\frac{\\phi}{n \\ b''(\\theta_0)}\n\\tag{4.10}\\] using Equation 4.7.\nThus we see that the first two derivatives of \\(b(\\theta)\\) play a key role in inference.\n\nExample: Accuracy for the Poisson distribution\nFor the Poisson distribution, \\(\\text{Po}(\\lambda)\\), we have found that \\(\\hat\\theta = \\log(\\bar{Y})\\). Using Equation 4.9 we know that \\(\\hat\\theta\\) is, at least, approximately unbiased. Then, using that \\(\\phi=1\\) (Table 3.2), \\(b'(\\theta)=e^{\\theta}\\) (Table 3.6) hence \\(b''(\\theta)=e^{\\theta}\\), and using Equation 4.10, leads to the result \\[\n\\mbox{Var}(\\hat\\theta) = \\frac{\\phi}{n \\ b''(\\theta_0)}\n=\n\\frac{1}{n e^{\\theta_0}}.\n\\]"
  },
  {
    "objectID": "4_GLM-Fitting.html#sec-mlegeneral",
    "href": "4_GLM-Fitting.html#sec-mlegeneral",
    "title": "4  GLM Estimation",
    "section": "4.2 The general case",
    "text": "4.2 The general case\n\n4.2.1 MLE Estimation\nSuppose that now the \\(n\\) independent observations \\(\\{y_i,\\ i=1,\\dots,n\\}\\) are not identically distributed. They are, however, sampled from the same exponential family density but with differing parameters, that is \\[\nf(y_i; \\theta_i,\\phi) = \\exp \\left\\{ \\frac{y_i\\theta_i  - b(\\theta_i)}{\\phi} + c(y_i, \\phi) \\right\\},\n\\] for \\(i=1,\\dots,n.\\) In this case, the canonical parameter does depend on \\(i\\) – but we assume that the scale parameter \\(\\phi\\) does not – and let \\(\\boldsymbol{\\theta} = \\{\\theta_i,\\ i=1,\\dots,n\\}\\).\nIn most applications, we are not interested in estimation of \\(\\boldsymbol{\\theta}\\) but instead we are interested in the linear predictor parameters \\(\\boldsymbol{\\beta} =\\{\\beta_1,\\dots,\\beta_p\\}\\). Note, however, that each \\(\\theta_i\\) will depend on all \\(\\beta_1,\\dots,\\beta_p\\). This is most obvious for the canonical parameter case where a convenient choice of link function is obtained using \\(\\boldsymbol{\\theta} = \\eta = X\\boldsymbol{\\beta}\\), hence \\(\\theta_i= \\mathbf{x}_i^T \\boldsymbol{\\beta} =\\sum_{j=1}^p x_{ij}\\beta_{j}\\) and each \\(\\theta_i\\) clearly depends on all \\(\\beta_1,\\dots,\\beta_p\\).\nThe principle of maximum likelihood will be used to estimate the model parameters \\(\\boldsymbol{\\beta}\\). Using the independence of \\(y_i, i=1,\\dots,n\\), given the parameters \\(\\boldsymbol{\\beta}\\), the likelihood function is: \\[\nL(\\boldsymbol{\\beta}; \\mathbf{y}, \\phi)\n= f(\\mathbf{y}; X, \\boldsymbol{\\beta},\\phi)\n= \\prod_{i=1}^n f(y_i; \\mathbf{x}_i, \\boldsymbol{\\beta}, \\phi)\n\\tag{4.11}\\] and the log-likelihood by \\[\nl (\\boldsymbol{\\beta}; \\mathbf{y}, \\phi)\n= \\log L(\\boldsymbol{\\beta}; \\mathbf{y}, \\phi)\n= \\sum_{i=1}^n \\log f(y_i;\n\\mathbf{x}_i,\n\\boldsymbol{\\beta}, \\phi).\n\\tag{4.12}\\] Then we wish to find the value of \\(\\boldsymbol{\\beta}\\) which maximizes the log-likelihood function, \\[\n\\hat{\\boldsymbol{\\beta}} =\n\\max_{\\boldsymbol{\\beta}}\nl (\\boldsymbol{\\beta}; \\mathbf{y}, \\phi).\n\\] For generalized linear models there is usually no closed-form expression for the MLE \\(\\hat{\\boldsymbol{\\beta}}.\\) Instead, an iterative approach based on the Newton–Raphson algorithm is usually adopted.\nFor the normal linear regression model, however, that is where the exponential family is Gaussian and the link function \\(g(\\mu)\\) is the identity function, we have the familiar closed-form expression \\[\n\\hat{\\boldsymbol{\\beta}} = \\left(X^T X\\right)^{-1} X^T \\mathbf{y}.\n\\tag{4.13}\\]\n\n\n4.2.2 The score function and Fisher information\nWe define the score function: \\[\nU(\\boldsymbol{\\beta}) = \\frac{\\partial l(\\boldsymbol{\\beta}; \\mathbf{y},\\phi)} {\\partial \\boldsymbol{\\beta}},\n\\tag{4.14}\\] which is a \\(p \\times 1\\) vector. We define the observed Fisher information: \\[\n{\\cal I}(\\boldsymbol{\\beta}) = - \\frac{\\partial U(\\boldsymbol{\\beta})} {\\partial \\boldsymbol{\\beta}^T}\n          = -\\frac{\\partial^2 l(\\boldsymbol{\\beta}; \\mathbf{y},\\phi)} {\\partial \\boldsymbol{\\beta} \\, \\partial \\boldsymbol{\\beta}^T},\n\\tag{4.15}\\] which is a \\(p \\times p\\) matrix whose \\((j,k)\\)th element is: \\(-\\frac{\\partial^2 l(\\boldsymbol{\\beta}; \\mathbf{y},\\phi)} {\\partial \\beta_j\\partial \\beta_k}\\). We also define the expected Fisher information: \\[\n{\\cal J}(\\boldsymbol{\\beta}) = \\mbox{E}\\left[ -\\frac{\\partial^2 l(\\boldsymbol{\\beta}; \\mathbf{y},\\phi)} {\\partial \\boldsymbol{\\beta} \\, \\partial \\boldsymbol{\\beta}^T} \\right],\n\\tag{4.16}\\] which is also a \\(p \\times p\\) matrix.\n\nProposition 4.1 With definitions Equation 4.14, Equation 4.15, Equation 4.16 above,\n\n\\(\\mbox{E}\\left[U(\\boldsymbol{\\beta})\\right] = 0\\),\n\\({\\cal J}(\\boldsymbol{\\beta}) = \\mbox{E}\\left[U(\\boldsymbol{\\beta}) U^T(\\boldsymbol{\\beta})\\right]\\).\n\n\nProof We give the proof for continuous random variables. For the discrete case, replace integration by sums – see Exercises.\nTo start the proof, notice that we can re-write the joint density of the data given the parameters as \\[\nf(\\mathbf{y}; X, \\boldsymbol{\\beta},\\phi) = L(\\boldsymbol{\\beta}; \\mathbf{y}, \\phi) =\\exp\\left\\{ l(\\boldsymbol{\\beta}; \\mathbf{y},\\phi)\\right\\}\n\\] and then \\[\n1 = \\int f(\\mathbf{y}; X \\boldsymbol{\\beta},\\phi) d\\mathbf{y}\n  = \\int \\exp\\{ l(\\boldsymbol{\\beta}; \\mathbf{y},\\phi)\\} d\\mathbf{y},\n\\] where \\(d\\mathbf{y}=dy_1\\cdots dy_n\\). Differentiating this with respect to \\(\\boldsymbol{\\beta}=(\\beta_1,\\dots,\\beta_p)^T\\) gives: \\[\\begin{align*}\n0 & = \\int \\frac{\\partial l(\\boldsymbol{\\beta}; \\mathbf{y},\\phi)} {\\partial \\boldsymbol{\\beta}}\n      \\exp\\{ l(\\boldsymbol{\\beta}; \\mathbf{y},\\phi)\\} d\\mathbf{y}  \\\\\n& = \\int U(\\boldsymbol{\\beta}) f(\\mathbf{y}; X, \\boldsymbol{\\beta},\\phi) d\\mathbf{y}  \\tag{$\\star$} \\\\\n&= \\mbox{E}\\left[U(\\boldsymbol{\\beta})\\right].\n\\end{align*}\\] Proving the first part.\nNext, differentiating \\((\\star)\\) by parts with respect to  \\(\\boldsymbol{\\beta}^T\\), \\[\\begin{align*}\n0 & = \\int \\frac{\\partial U(\\boldsymbol{\\beta})} {\\partial \\boldsymbol{\\beta}^T}\n      f(\\mathbf{y}; X, \\boldsymbol{\\beta},\\phi)  + \\frac{\\partial l(\\boldsymbol{\\beta}; \\mathbf{y},\\phi)}\n      {\\partial \\boldsymbol{\\beta}} \\frac{\\partial l(\\boldsymbol{\\beta}; \\mathbf{y},\\phi)}\n      {\\partial \\boldsymbol{\\beta}^T} f(\\mathbf{y}; X, \\boldsymbol{\\beta},\\phi) d\\mathbf{y} \\\\[2mm]\n& = \\int - {\\cal I}(\\boldsymbol{\\beta}) f(\\mathbf{y}; X, \\boldsymbol{\\beta},\\phi) d\\mathbf{y}\n    + \\int U(\\boldsymbol{\\beta}) U^T(\\boldsymbol{\\beta}) f(\\mathbf{y}; X, \\boldsymbol{\\beta},\\phi) d\\mathbf{y} \\\\[2mm]\n& = - {\\cal J}(\\boldsymbol{\\beta}) + \\mbox{E}\\left[U(\\boldsymbol{\\beta}) U^T(\\boldsymbol{\\beta})\\right].\n\\end{align*}\\] proving the second part.\n\nProposition 4.2 Under some regularity conditions, the MLE \\(\\hat{\\boldsymbol{\\beta}}\\) of \\(\\boldsymbol{\\beta}\\) has the following asymptotic properties:\n\n\\(\\mbox{E}(\\hat{\\boldsymbol{\\beta}}) = \\boldsymbol{\\beta}\\); i.e. \\(\\hat{\\boldsymbol{\\beta}}\\) is unbiased for \\(\\boldsymbol{\\beta}\\).\n\\(\\mbox{Var}(\\hat{\\boldsymbol{\\beta}}) = {\\cal J}^{-1}(\\boldsymbol{\\beta})\\).\n\\(\\hat{\\boldsymbol{\\beta}}\\) follows a \\(p\\)-dimensional Normal distribution.\n\nCombining these three statements we have that asymptotically \\[\n\\hat{\\boldsymbol{\\beta}} \\sim N_p (\\boldsymbol{\\beta}, {\\cal J}^{-1}(\\boldsymbol{\\beta})).\n\\tag{4.17}\\]\n\nProof: Omitted. Similar to the proof using Taylor’s theorem in Section 4.1.\nFrom Equation 4.17, variances \\(\\text{Var}(\\hat{\\beta}_{k})\\), standard errors \\(\\text{se}(\\hat{\\beta}_{k})\\) and correlations between parameter estimates \\(\\text{Corr}(\\hat{\\beta}_{k},\\hat{\\beta}_{h})\\) can be estimated.\n\n\n4.2.3 The saturated case\nAgain we assume the observations \\(y_i,\\ i=1,\\dots,n\\) are independent but now we assume that \\(y_i\\) is sampled from an exponential family probability function with canonical parameter \\(\\theta_i\\), \\[\nf(y_i; \\theta_i,\\phi) = \\exp \\left\\{ \\frac{y_i\\theta_i  - b(\\theta_i)}{\\phi} + c(y_i, \\phi) \\right\\},\n\\] for \\(i=1,\\dots,n.\\) We can form the log-likelihood in the usual way to give \\[\nl (\\boldsymbol{\\theta}; \\mathbf{y}, \\phi) = \\sum_{i=1}^n \\log f(y_i; \\theta_i, \\phi)\n= \\sum_{i=1}^n \\left[\\frac{y_i\\theta_i  - b(\\theta_i)}{\\phi} + c(y_i, \\phi)\\right]\n\\] and so we find the the MLE of \\(\\boldsymbol{\\theta}\\) using \\[\n\\hat \\theta_i = (b')^{-1}(y_i),\n\\quad i=1,\\dots,n.\n\\] Note that each parameter is only a function of the corresponding observation.\nFurther, from Proposition 3.1, \\(\\mbox{E}[Y_i] = \\mu_i =b'(\\theta_i)\\) and if we again let \\(\\hat\\mu_i\\) denote the MLE of \\(\\mu_i\\), then \\(\\hat\\mu_i = b'(\\hat{\\theta_i})\\), hence we have \\(\\hat\\mu_i = y_i\\). Thus we see that, under the saturated model, the mean of the distribution of \\(y_i\\) is estimated to be equal to \\(y_i\\) itself. That is, the data are fitted exactly by the model. Of course this model is quite useless for explanation or prediction, since it misinterprets random variation as systematic variation. Nevertheless, the saturated model is useful as a benchmark for comparing models, as we will see later.\nIt is worth noting that the same situation can occur even when modelling in terms of the regression parameters \\(\\beta_j,\\ j=1,\\dots,p\\). When \\(p\\geq n\\), and if the covariates are linearly independent, each \\(\\theta_i\\) can take on any value independently of the others and so estimating the \\(\\beta_j\\)’s is equivalent to estimating the \\(\\theta_i\\)’s. That is we are also considering the saturated or full model. This highlights the danger of putting too many covariates into the model. There is a big literature on how to deal with more parameters then data using techniques of regularized regression."
  },
  {
    "objectID": "4_GLM-Fitting.html#sec-deviance",
    "href": "4_GLM-Fitting.html#sec-deviance",
    "title": "4  GLM Estimation",
    "section": "4.3 Model deviance",
    "text": "4.3 Model deviance\nThe deviance is a quantity we use to assess the fit of a model to the data. Let \\(M\\) be a model of interest with fitted parameters \\(\\hat{\\boldsymbol\\theta}\\) and corresponding fitted values \\(\\hat{\\boldsymbol\\mu}\\). Also consider the saturated model with fitted parameters \\(\\tilde{\\boldsymbol\\theta}\\) and fitted values \\(\\tilde{\\boldsymbol\\mu}\\).\nThe deviance of model \\(M\\) is defined as twice the difference between the log-likelihood of the saturated model, \\(l(\\tilde{\\boldsymbol\\theta};\\mathbf{y},\\phi)\\), and the log-likelihood of model \\(M\\), \\(l(\\hat{\\boldsymbol\\theta}; \\mathbf{y},\\phi)\\), multiplied by \\(\\phi\\), \\[\\begin{eqnarray}\nD\n& = & 2 \\phi \\left\\{ l(\\tilde{\\boldsymbol\\theta};\\mathbf{y},\\phi) - l(\\hat{\\boldsymbol\\theta}; \\mathbf{y},\\phi) \\right\\} \\label{eq:deviance}\\\\[4mm]\n& = &  \\left\\{ \\begin{array}{ll}\n    \\sum_{i=1}^n  (y_i - \\hat\\mu_i)^2\n   = \\mbox{Residual sum of squares} &  \\mbox{ Normal} \\\\[3mm]\n   2 \\sum_{i=1}^n \\left\\{ y_i \\log \\left( \\frac{y_i}{\\hat\\mu_i} \\right)\n   + (m_i - y_i) \\log \\left( \\frac{m_i - y_i}{m_i - \\hat\\mu_i} \\right) \\right\\}\n   &  \\mbox{ Binomial} \\\\[3mm]\n   2 \\sum_{i=1}^n \\left\\{ y_i \\log \\left( \\frac{y_i}{\\hat\\mu_i} \\right)\n   - y_i + \\hat\\mu_i \\right\\} &  \\mbox{ Poisson} \\\\ \\end{array} \\right.\\notag\n\\end{eqnarray}\\] Note that Dobson, and others, call \\(D^*= D/\\phi=2 \\{ l(\\tilde{\\theta};y,\\phi) - l(\\hat{\\theta}; y,\\phi)\\}\\) the scaled deviance.\nWe now consider two situations:\nScale parameter \\(\\phi\\) known. For some data-types (e.g. Poisson, Binomial), we know \\(\\phi=1\\). Consider two nested models \\(M_1\\) and \\(M_2\\) with \\(r_1\\) and \\(r_2\\) parameters respectively where the parameters in \\(M_1\\) are a subset of those in \\(M_2\\) and hence \\(r_1 < r_2\\). Further, let \\(D_1\\) and \\(D_2\\) be the deviances of model \\(M_1\\) and \\(M_2\\) respectively.\nThen, asymptotically,\n\nthe log likelihood-ratio statistic \\(D_1 - D_2 \\sim \\chi^2_{r_2 - r_1}\\) can be used to test the importance of the extra parameters in \\(M_2\\) not included in \\(M_1\\);\na goodness-of-fit test for \\(M_2\\) can be done based on \\(D_2 \\sim \\chi^2_{n - r_2}\\).\n\nThe quality of the approximations involved depends on there being a large amount of information, for example, large counts for Binomial and Poisson data, or a large sample size for Normal data.\nScale parameter \\(\\phi\\) unknown. For some data-types (e.g. Normal, Gamma), \\(\\phi\\) is not known (typically \\(\\phi = \\sigma^2\\)). We must find a model \\(M_3\\) big enough to be believed, then estimate \\(\\phi\\) by the residual mean square: \\[\n    \\hat{\\phi} = \\frac{D_3}{n - r_3}.\n     \\tag{4.18}\\] Then test \\(M_1\\) against \\(M_2\\) using \\[\n    \\mbox{F}=\\frac{(D_1 - D_2) / (r_2 - r_1)}{\\hat{\\phi}} = \\frac{(D_1 - D_2) / (r_2 - r_1)}{D_3/(n-r_3)}\n     \\tag{4.19}\\] with \\[\n    \\mbox{F} \\sim F_{r_2 - r_1, n - r_3}.\n     \\tag{4.20}\\] So if the observed value of the statistic Equation 4.19 was within the upper (say 5%) tail of the \\(F\\)-distribution Equation 4.20, we would infer that Model \\(M_2\\) is better than Model \\(M_1\\)."
  },
  {
    "objectID": "4_GLM-Fitting.html#model-residuals",
    "href": "4_GLM-Fitting.html#model-residuals",
    "title": "4  GLM Estimation",
    "section": "4.4 Model residuals",
    "text": "4.4 Model residuals\nConsider a generalized linear model with observed values \\(y_i, i=1,\\dots,n\\) and fitted values \\(\\hat\\mu_i\\). Then the raw or response residuals are defined by \\[\ne_i^\\text{raw} = y_i - \\hat\\mu_i.\n\\]\nMore useful are the standardized or Pearson residuals defined by \\[\ne_i^\\text{std} =\ne_i^\\text{P} = \\frac{y_i - \\hat\\mu_i}{\\sqrt{ b''(\\theta_i)}}.\n\\] Recall from Equation 3.8 that \\(\\text{Var}(Y_i) = \\phi \\, b''(\\theta_i)\\).\nDeviance residuals are defined so that the sum of squared deviance residuals equals the total deviance. Thus we set \\[\ne_i^\\text{dev} = \\text{sign}(y_i - \\hat\\mu_i) \\sqrt{d_i},\n\\] where \\(d_i\\) is the contribution of observation \\(i\\) to the deviance, \\(D\\). For example, when \\(y_i\\) has a Poisson distribution with estimated mean \\(\\hat{\\mu}_i\\), we have \\[\ne_i^\\text{dev} = \\text{sign}(y_i - \\hat\\mu_i) \\sqrt{2\\left[y_i \\log\\left(\\frac{y_i}{\\hat{\\mu}_i}\\right) - y_i + \\hat{\\mu}_i\\right]}.\n\\]\nResiduals are useful for assessing the overall fit of a model to the data, and for identifying where the model might need to be improved."
  },
  {
    "objectID": "4_GLM-Fitting.html#fitting-generalized-linear-models-in-r",
    "href": "4_GLM-Fitting.html#fitting-generalized-linear-models-in-r",
    "title": "4  GLM Estimation",
    "section": "4.5 Fitting generalized linear models in R",
    "text": "4.5 Fitting generalized linear models in R\n\n4.5.1 GLM-related R commands\nThe function used to fit a generalized linear model in \\(\\mathbf R\\) is \\[\\texttt{glm(formula, family)}.\\]\nLet \\(\\texttt{x,y,z,a,b,c,}\\) \\(\\dots\\) be a set of vectors all of the same length \\(n\\) (perhaps read in from a data file using the \\(\\texttt{read.table}\\) and \\(\\texttt{attach}\\) commands). If \\(\\texttt{a,b,c}\\) are qualitative variables, then they first need to be declared as factors by \\(\\texttt{a = as.factor(a)}\\), etc.\nThe \\(\\texttt{formula}\\) argument of \\(\\texttt{glm}\\) specifies the required model in compact notation, e.g. \\(\\texttt{y} \\sim \\texttt{x*a}\\) or \\(\\texttt{y} \\sim \\texttt{x + z*a}\\) where \\(\\sim, \\texttt{+,*}\\) have the same meaning as in Section 2.5.\nThe \\(\\texttt{family}\\) argument specifies which exponential family is to be used. We shall use \\(\\texttt{gaussian, poisson}\\) and \\(\\texttt{binomial}\\); \\(\\texttt{gaussian}\\) is the default. Other options are available; see \\(\\texttt{help(family)}\\) for further information.\nAlong with the family, a link function can be specified. The possible choices are:\n\n\\(\\texttt{gaussian} - \\texttt{\"identity\"}\\) (default)\n\\(\\texttt{poisson} - \\texttt{\"log\"}\\) (default), \\(\\texttt{\"sqrt\"}\\), \\(\\texttt{\"identity\"}\\)\n\\(\\texttt{binomial} - \\texttt{\"logit\"}\\) (default), \\(\\texttt{\"probit\"}\\), \\(\\texttt{\"cloglog\"}\\).\n\nR assumes the default options unless we state otherwise. For example,\n\\(\\texttt{glm(y}\\sim\\texttt{a+b)}\\) # Gaussian errors, identity link\n\\(\\texttt{glm(y}\\sim a+b\\texttt{, poisson)}\\) # Poisson errors, log link\n\\(\\texttt{glm(y}\\sim\\texttt{a+b, poisson(\"sqrt\"))}\\) # Poisson errors, sqrt link\nNote that for the binomial case, the response variable should be an \\(n \\times 2\\) matrix \\(\\texttt{ym}\\), say, not a vector, where the first column contains the numbers of successes and the second column the numbers of failures, for example:\n\\(\\texttt{glm(ym}\\sim\\texttt{ a+b, binomial)}\\) # binomial errors, logit link\nTo extract information about a fitted generalized linear model, it is best to store the result of \\(\\texttt{glm}\\) as a variable and then to use the following functions:\n\nTo fit a GLM and store the result in \\(\\texttt{y.glm}\\) (for example):\n\\(\\texttt{y.glm = glm(y}\\sim\\texttt{a*b, poisson(\"sqrt\"))}\\)\nTo print various pieces of information including deviance residuals, parameter estimates and standard errors, deviances, and (if specified) correlations of parameter estimates:\n\\(\\texttt{summary(y.glm, correlation=T)}\\)\nTo print the anova table of the fitted model:\n\\(\\texttt{anova(y.glm)}\\)\nTo print the deviance of the fitted model:\n\\(\\texttt{deviance(y.glm)}\\)\nTo print the residual degrees of freedom of the fitted model:\n\\(\\texttt{df.residual(y.glm)}\\)\nTo print the vector of fitted values under the fitted model:\n\\(\\texttt{fitted.values(y.glm)}\\)\nTo print the residuals from the fitted model:\n\\(\\texttt{residuals(y.glm, type)}\\)\nNote: \\(\\texttt{type}\\) should be \\(\\texttt{\"deviance\"}\\) (default), \\(\\texttt{\"pearson\"}\\), or \\(\\texttt{\"response\"}\\)\nTo print the parameter estimates from the fitted model:\n\\(\\texttt{coefficients(y.glm)}\\)\nTo print the design matrix for a specified model formula: \\(\\texttt{model.matrix(y}\\sim\\texttt{a*b)}\\)\n\nThe functions \\(\\texttt{summary}\\), \\(\\texttt{anova}\\), and possibly \\(\\texttt{model.matrix}\\) are the most useful for printing out information about the fitted model. The results of the other functions can be saved as variables for further computation, if desired.\n\n\n4.5.2 Example of fitting Poisson GLM in R\nHere is a toy example of R commands for modelling a response in terms of two qualitative explanatory variables (that is factors). The model assumes the data are Poisson-distributed and uses the logarithmic link function.\n\n\nCode\ny = c(1, 2, 4, 7, 8, 10, 10, 7, 10, 2, 8, 16)\na = rep(1:4,times=3)\nb = rep(1:3,each=4)\na = as.factor(a)\nb = as.factor(b)\n\ny.glm = glm(y ~ a+b, poisson)\n\nsummary(y.glm, correlation=T)\n\n\n\nCall:\nglm(formula = y ~ a + b, family = poisson)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.8740  -0.6834   0.1287   0.7151   1.5956  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   1.1408     0.3351   3.404 0.000663 ***\na2           -0.3054     0.3522  -0.867 0.385934    \na3            0.1466     0.3132   0.468 0.639712    \na4            0.4568     0.2932   1.558 0.119269    \nb2            0.9163     0.3162   2.898 0.003761 ** \nb3            0.9445     0.3150   2.999 0.002712 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 31.725  on 11  degrees of freedom\nResidual deviance: 13.150  on  6  degrees of freedom\nAIC: 68.227\n\nNumber of Fisher Scoring iterations: 5\n\nCorrelation of Coefficients:\n   (Intercept) a2    a3    a4    b2   \na2 -0.45                              \na3 -0.50        0.48                  \na4 -0.54        0.51  0.57            \nb2 -0.67        0.00  0.00  0.00      \nb3 -0.68        0.00  0.00  0.00  0.72\n\n\nCode\nanova(y.glm)\n\n\nAnalysis of Deviance Table\n\nModel: poisson, link: log\n\nResponse: y\n\nTerms added sequentially (first to last)\n\n     Df Deviance Resid. Df Resid. Dev\nNULL                    11     31.725\na     3   6.2793         8     25.445\nb     2  12.2947         6     13.150\n\n\nCode\ndeviance(y.glm)\n\n\n[1] 13.15047\n\n\nCode\ndf.residual(y.glm)\n\n\n[1] 6\n\n\nCode\nfitted.values(y.glm)\n\n\n        1         2         3         4         5         6         7         8 \n 3.129412  2.305882  3.623529  4.941176  7.823529  5.764706  9.058824 12.352941 \n        9        10        11        12 \n 8.047059  5.929412  9.317647 12.705882"
  },
  {
    "objectID": "4_GLM-Fitting.html#exercises",
    "href": "4_GLM-Fitting.html#exercises",
    "title": "4  GLM Estimation",
    "section": "4.6 Exercises",
    "text": "4.6 Exercises\n4.1 Use Equation 4.4 to obtain estimation equations for the natural parameter \\(\\theta\\), based on a sample \\(\\mathbf{y}=\\{y_1,\\dots, y_n\\}\\), for each of the following situations:\n\nthe binomial, \\(Y\\sim\\mbox{Bin}(n,p)\\),\nthe geometric, \\(Y\\sim\\mbox{Ge}(p)\\),\nthe exponential, \\(Y\\sim\\mbox{Exp}(\\lambda)\\).\n\nAre these estimators unbiased for \\(\\theta\\)? What is the variance of the estimator in each case?\n4.2 For a sample of size \\(n\\) from the normal distribution, \\(Y\\sim N(\\mu, \\sigma^2)\\), how do the results produced using Equation 4.9 and Equation 4.10 compare with the familiar results \\(\\hat\\mu =\\bar y\\), \\(\\text{E}[\\hat\\mu]=\\mu\\), and \\(\\text{Var}[\\hat\\mu] =\\sigma^2/n\\)?\n4.3 For the normal linear regression model, \\(\\mathbf{Y}=X\\boldsymbol{\\beta}+\\boldsymbol{\\epsilon}\\) where \\(\\boldsymbol{\\epsilon}\\sim N_n(0,\\sigma^2 I_n)\\) use the principle of maximum likelihood to show that the MLE has the closed form given in Equation 4.13.\nFurther questions to be added later"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH3823 Generalized Linear Models",
    "section": "",
    "text": "\\[\n\\def\\b#1{\\mathbf{#1}}\n\\]\n\n\nWeekly schedule\n\n\n\n\n\n\nCoursework Practical Sessions (20 - 24 March)\n\n\n\n\nCoursework for this module involves a single written report worth 20% of the module grade. This will mainly involve investigating different models using R and interpreting the results.\nTasks are expected to be handed out on 14 March with hand-in deadline expect to be 31 March.\n\n\n\n\n\n\n\n\n\nWeek 6 (6 - 10 March)\n\n\n\n\nBefore next Lecture: Re-read Chapter 4: Sections 4.1-4.4.\nLecture on Tuesday: Complete Chapter 4: with Section 4.5 Fitting GLMs in R\nLecture on Thursday: Start Chapter 5: Logistic Regression with Sections 5.1 & 5.2.\nWeekly feedback: Relevant exercise questions from last year (solutions online).\n\n\n\n\n\n\n\n\n\nWeek 5 (27 February - 3 March)\n\n\n\n\nBefore next Lecture: Re-read Chapter 4: Section 4.1.\nLecture on Tuesday: Cancelled due to illness. Please read Chapter 4: Section 4.2.\nLecture on Thursday: Chapter 4: Sections 4.3, 4.4 & 4.5.\nWeekly feedback: Start Exercises in Chapter 4.\n\n\n\n\n\n\n\n\n\nWeek 4 (20 - 24 February)\n\n\n\n\nBefore next Lecture: Be confident with all material up to, and including, Section 3.4 Moments of exponential-family distributions.\nLecture on Tuesday: Chapter 3: Sections 3.5 & 3.6\nLecture on Thursday: Start Chapter 4 by covering Section 4.1.\nWeekly feedback: Complete Exercises in Chapter 3.\n\n\n\n\n\n\n\n\n\nWeek 3 (13 - 17 February)\n\n\n\n\nBefore next Lecture: Be confident with material in Chapter 2: Essentials of Normal Linear Models.\nLecture on Tuesday: Cancelled due to UCU strike. Instead, self-study Chapter 3: Sections 3.1 & 3.2.\nLecture on Thursday: Cancelled due to UCU strike. Instead, self-study Chapter 3: Sections 3.3 & 3.4.\nBefore next Lecture: Complete questions and check solutions, including video(s), for all Exercises in Chapters 1 and 2. Start Exercises in Chapter 3.\n\n\n\n\n\n\n\n\n\nWeek 2 (6 - 10 February)\n\n\n\n\nBefore next Lecture: Please re-read Section 2.1: Overview and read Section 2.2: Types of normal linear model.\nLecture on Tuesday: We will briefly cover all remaining material in Chapter 2: Essentials of Normal Linear Models.\nBefore next Lecture: Please re-read Chapter 2 carefully.\nLecture on Thursday: Cancelled due to UCU strike.\nWeekly feedback: Self-study the Exercises in Section 2.6 – solutions to be posted during Week 3.\n\n\n\n\n\n\n\n\n\nWeek 1 (30 January - 3 February)\n\n\n\n\nBefore next Lecture: Please read the Overview.\nLecture on Tuesday: We will briefly cover all material in Chapter 1: Introduction.\nBefore next Lecture: Please re-read Chapter 1 carefully.\nLecture on Thursday: Start Chapter 2: Essentials of Normal Linear Models with Section 2.1: Overview.\nWeekly feedback: Self-study the Exercises in Section 1.5 – solutions to be posted during Week 1."
  }
]