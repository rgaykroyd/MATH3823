[
  {
    "objectID": "6_loglinearmodel.html#overview",
    "href": "6_loglinearmodel.html#overview",
    "title": "6  Loglinear models",
    "section": "6.1 Overview",
    "text": "6.1 Overview\nIn this chapter, we deal with data sets in which the response variable \\(y\\) is a count and the explanatory variables are all factors – i.e. qualitative variables. Initially we assume \\(y\\) has a Poisson distribution.\nSee Chapter 9 of Dobson and Barnett (2008). See also Agresti (1996) An introduction to categorical data analysis.\nWe assume a generalized linear model with\n\nresponses (counts) having independent Poisson distributions;\na logarithmic link function (hence the name log-linear model).\n\nConsider, for example, the two-way contingency table in Table 6.1}:\n\n\nTable 6.1: A two-way contingency table with \\(k_1\\) rows and \\(k_2\\) columns, where each entry \\(y_{ij}\\) is a count.\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n\\(\\dots\\)\n\\(j\\)\n\\(\\dots\\)\n\\(k_2\\)\nTotal\n\n\n\n\n1\n\\(y_{11}\\)\n\\(y_{12}\\)\n\\(\\dots\\)\n\\(y_{1j}\\)\n\\(\\dots\\)\n\\(y_{1k_2}\\)\n\\(y_{1+}\\)\n\n\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\n\n\\(i\\)\n\\(y_{i1}\\)\n\\(y_{12}\\)\n\\(\\dots\\)\n\\(y_{1j}\\)\n\\(\\dots\\)\n\\(y_{ik_2}\\)\n\\(y_{i+}\\)\n\n\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\n\n\\(k_1\\)\n\\(y_{k_11}\\)\n\\(y_{k_12}\\)\n\\(\\dots\\)\n\\(y_{1j}\\)\n\\(\\dots\\)\n\\(y_{k_2k_2}\\)\n\\(y_{1k_1}\\)\n\n\nTotal\n\\(y_{+1}\\)\n\\(y_{+2}\\)\n\\(\\dots\\)\n\\(y_{1j}\\)\n\\(\\dots\\)\n\\(y_{+k_2}\\)\n\\(y_{++}\\)\n\n\n\n\nIn row \\(i\\) and column \\(j\\) of Table 6.1} we assume \\[\nY_{ij} \\sim \\text{Po}(\\lambda_{ij})\n\\] where \\[\n\\log \\lambda_{ij} = \\mu + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij}.\n\\tag{6.1}\\] Here \\(\\mu\\) is called the main effect; \\(\\alpha_i\\) is a row effect; \\(\\beta_j\\) is a column effect; and \\((\\alpha \\beta)_{ij}\\) denotes an interaction effect parameter. Some or all of these effects must be present in the model, with constraints to ensure that the model is identifiable – this is sometimes refered ot as the identifiability or aliasing problem. Generally, R function automatically sets the first level of each effect to zero to achieve model identification. Thus, for the two-way model, \\[\n\\alpha_1 = 0, \\quad \\beta_1 = 0, \\quad\n(\\alpha \\beta)_{11}=(\\alpha \\beta)_{1j}=(\\alpha \\beta)_{k_1}=0.\n\\]"
  },
  {
    "objectID": "6_loglinearmodel.html#motivating-examples",
    "href": "6_loglinearmodel.html#motivating-examples",
    "title": "6  Loglinear models",
    "section": "6.2 Motivating examples",
    "text": "6.2 Motivating examples\n\n6.2.1 Malignant melanoma\nThe data in Table 6.2 are from a study of 400 patients with malignant melanoma, a particular form of skin cancer (see Dobson and Barnett, p.172). For each tumour, its type and site were recorded. The data in Table 6.2 comprise the numbers of tumours \\(y\\) in each combination of site and tumour-type. This is a two-way contingency table. We want to know how melanoma frequency depends on site and type.\n\n\nTable 6.2: Melanoma counts by type and site.\n\n\nType\nHead\nTrunk\nExtremities\nTotal\n\n\n\n\nHutchinson’s melanotic freckle\n22\n2\n19\n34\n\n\nSuperficial spreading melanoma\n16\n54\n115\n185\n\n\nNodular\n19\n33\n73\n125\n\n\nIndeterminate\n11\n17\n28\n56\n\n\nTotal\n68\n106\n226\n400\n\n\n\n\n\n\nTable 6.3: Percentages across columns within rows for melanoma data.\n\n\nType\nHead\nTrunk\nExtremities\nTotal\n\n\n\n\nHutchinson’s melanotic freckle\n64.7\n5.9\n29.4\n100\n\n\nSuperficial spreading melanoma\n8.6\n29.2\n62.2\n100\n\n\nNodular\n15.2\n26.4\n58.4\n100\n\n\nIndeterminate\n19.6\n30.4\n50.0\n100\n\n\nTotal\n17.0\n26.5\n56.5\n100\n\n\n\n\n\n\nTable 6.4: Percentages across rows within columns for melanoma data.\n\n\n\n\n\n\n\n\n\nType\nHead\nTrunk\nExtremities\nTotal\n\n\n\n\nHutchinson’s melanotic freckle\n32.4\n1.9\n4.4\n8.5\n\n\nSuperficial spreading melanoma\n23.5\n50.9\n50.9\n46.3\n\n\nNodular\n27.9\n31.1\n32.3\n31.3\n\n\nIndeterminate\n16.2\n16.0\n12.4\n14.0\n\n\nTotal\n100.0\n99.9\n100.0\n100.0\n\n\n\n\nAlthough we have two factors, and , that we may use as predictors, standard ANOVA regression methods are inappropriate here as the dependent variable is not continuous but is instead a count. We will use log-linear regression, a type of generalized linear model, to analyse these data.\nTable 6.3 shows row and Table 6.4 column percentages for these data. For example, 15.2% of nodular melanomas occurred in the head and neck, 26.4% in the trunk, and 58.4% in the extremities. Compare this to the equivalent figures for Hutchinson’s melanotic freckles: 64.7%, 5.9%, and 29.4% - strikingly different. So different types of melanomas are more likely to occur in different locations.\n\n\n6.2.2 Flu vaccine\nThe data in Table 6.5 are from a randomized controlled trial in which 73 patients were randomized into two groups (see Dobson and Barnett, 2008, p.173). The treatment group was given a flu vaccine, while the control group was given a placebo. Levels of an antibody (HIA) were measured after 6 weeks and classified into three groups: Low, Moderate, and High.\n\n\nTable 6.5: Antibody responses to flu vaccine from a randomized controlled trial.\n\n\nGroup\nLow\nModerate\nHigh\n\n\n\n\nPlacebo\n25\n8\n5\n\n\nVaccine\n6\n18\n11\n\n\nTotal\n31\n26\n16\n\n\n\n\nIs the pattern of response the same for each treatment group? The percentages in Table 6.6 suggest not - row percentages indicate lower responses in the placebo group.\n\n\nTable 6.6: Percentages across rows within columns for antibody responses to flu vaccine.\n\n\nGroup\nLow\nModerate\nHigh\nTotal\n\n\n\n\nPlacebo\n65.8\n21.1\n13.2\n100\n\n\nVaccine\n17.1\n51.4\n31.4\n100\n\n\nTotal\n42.4\n35.6\n22.0\n100\n\n\n\n\n\n\nTable 6.7: Percentages across rows within columns for antibody responses to flu vaccine.\n\n\nGroup\nLow\nModerate\nHigh\n\n\n\n\nPlacebo\n80.6\n30.8\n31.2\n\n\nVaccine\n19.4\n69.2\n68.8\n\n\nTotal\n100\n100\n100"
  },
  {
    "objectID": "6_loglinearmodel.html#maximum-likelihood-estimation",
    "href": "6_loglinearmodel.html#maximum-likelihood-estimation",
    "title": "6  Loglinear models",
    "section": "6.3 Maximum likelihood estimation",
    "text": "6.3 Maximum likelihood estimation\nRecall that for each cell \\(Y_{ij} \\sim \\text{Po}(\\lambda_{ij})\\) so \\(\\mbox{E}[Y_{ij}]= \\lambda_{ij}\\). However, we estimate \\(\\hat{\\lambda}_{ij} = y_{ij}\\) only for the saturated model given by Equation 6.1. In general, for non-saturated models, the estimate \\(\\hat\\lambda_{ij} \\ne y_{ij}\\).\nConsider the independence model for a 2-way table, that is where \\((\\alpha \\beta)_{ij} = 0\\) for all \\(i,j\\). Here, \\[\n\\log \\lambda_{ij} = \\mu +\\alpha_i + \\beta_j \\tag{6.2}\\] and \\(y_{ij}\\) is the observed count for cell \\((i,j)\\), so the likelihood is given by \\[\nL(\\lambda; y)\n=  \\prod_{i,j} \\frac{e^{-\\lambda_{ij}} \\lambda_{ij}^{y_{ij}}}{y_{ij}!}\n\\] and the log-likelihood is \\[\nl(\\lambda; y)\n=  \\sum_{i,j} \\left\\{ y_{ij} \\log \\lambda_{ij} - \\lambda_{ij} -\n    \\log y_{ij}!\\right\\}.\n\\] Next, using Equation 6.2, gives \\[\nl(\\lambda; y) =  \\sum_{i,j} \\left\\{ y_{ij} (\\mu + \\alpha_i + \\beta_j)\n    - \\exp(\\mu + \\alpha_i + \\beta_j)  - \\log y_{ij}! \\right\\}\n\\] which can be re-written as \\[\nl(\\lambda; y)   =  \\mu y_{++} + \\sum_i \\alpha_i y_{i+} + \\sum_j \\beta_j y_{+ j}\n    - e^{\\mu} \\left(\\sum_i e^{\\alpha_i}\\right) \\left(\\sum_j e^{\\beta_j}\n    \\right) - \\sum_{ij} \\log y_{ij}!.\n\\tag{6.3}\\]\nThe maximum likelihood estimates of the model parameters are obtained in the usual way. Differentiating Equation 6.3 with respect to \\(\\mu\\) and setting the result to zero gives, at the MLE, \\[\ny_{++} = e^{\\hat{\\mu}} \\left(\\sum_i e^{\\hat{\\alpha}_i}\\right)\n\\left(\\sum_j e^{\\hat{\\beta}_j}\\right).\n\\]\nDifferentiating Equation 6.3 with respect to \\(\\alpha_i\\) (where \\(i \\ne 1\\) because \\(\\alpha_1=0\\) to avoid an identifiability problem) and setting the result to zero gives, at the MLE, \\[\ny_{i+} = e^{\\widehat{\\mu}}  e^{\\widehat{\\alpha}_i} \\left(\\sum_j e^{\\widehat{\\beta}_j}\\right).  \\notag \\\\\n% = \\sum_j \\widehat{\\lambda_{ij}} = \\widehat{\\lambda}_{i+} ~~~~(i \\not= 1).\n\\] Differentiating Equation 6.3 with respect to \\(\\beta_j\\) (where \\(j \\not= 1\\) because \\(\\beta_1=0\\)) and setting the result to zero gives, at the MLE, \\[\ny_{+j} = e^{\\widehat{\\mu}}  e^{\\widehat{\\beta}_j} \\left(\\sum_i e^{\\widehat{\\alpha}_i}\\right).\n\\] Then \\[\n\\frac{y_{i+}\\ y_{+j}}{y_{++}} = e^{\\widehat{\\mu}+\\widehat{\\alpha}_i+\\widehat{\\beta}_j}  \n= \\widehat{\\lambda}_{ij}.  \n\\tag{6.4}\\] It follows that \\[\n\\widehat{\\lambda}_{i+} = y_{i+} \\quad\n\\widehat{\\lambda}_{+j} = y_{+j} \\quad  \n\\widehat{\\lambda}_{++} = y_{++}.  \n\\]\nThus, the total fitted count in row \\(i\\) is identical to the total observed count in row \\(i\\). Further, the total fitted count in column \\(j\\) is equal to the total observed count in column \\(j\\) and the total fitted count equals the total observed count."
  },
  {
    "objectID": "6_loglinearmodel.html#model-fitting-in-r",
    "href": "6_loglinearmodel.html#model-fitting-in-r",
    "title": "6  Loglinear models",
    "section": "6.4 Model fitting in R",
    "text": "6.4 Model fitting in R\nConsider an analysis of Melanoma data introduced in Section 6.2.1.\nTo test the independence of \\(\\texttt{Type}\\) and \\(\\texttt{Size}\\), we fit the model \\[\n\\text{Count} \\sim \\text{Type} + \\text{Site}\n\\] assuming Poisson counts and a logarithmic link function, using the commands:\n\n\nCode\nmelanoma = read.table(\"https://rgaykroyd.github.io/MATH3823/Datasets/melanoma.txt\", header=TRUE)\nattach(melanoma)\n\ntype.F = as.factor(type)\nsite.F = as.factor(site)\n\nglm1 = glm(count ~ type.F + site.F, family='poisson')\nsummary(glm1)\n\n\n\nCall:\nglm(formula = count ~ type.F + site.F, family = \"poisson\")\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-3.0453  -1.0741   0.1297   0.5857   5.1354  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   1.7544     0.2040   8.600  < 2e-16 ***\ntype.F2       1.6940     0.1866   9.079  < 2e-16 ***\ntype.F3       1.3020     0.1934   6.731 1.68e-11 ***\ntype.F4       0.4990     0.2174   2.295  0.02173 *  \nsite.F2       0.4439     0.1554   2.857  0.00427 ** \nsite.F3       1.2010     0.1383   8.683  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 295.203  on 11  degrees of freedom\nResidual deviance:  51.795  on  6  degrees of freedom\nAIC: 122.91\n\nNumber of Fisher Scoring iterations: 5\n\n\nWe see that the residual deviance for this model is \\(51.795\\) on \\(6\\) degrees of freedom. If the model is true, the residual deviance will have an approximate \\(\\chi^2\\) distribution on \\(6\\) degrees of freedom. If the model is not true, the residual deviance will probably be too large to correspond to this distribution. Thus we calculate the \\(p\\)-value in the upper tail of the \\(\\chi^2_6\\) distribution, which can be computed using the command:\n\n\nCode\npchisq(51.795,6,lower.tail=F)\n\n\n[1] 2.050465e-09\n\n\nThis strongly indicates that the independence model is inadequate. Therefore, unless we can spot alternative simplifications, we will have to use the saturated model.\nWe can look at residuals from the model see where the departures from independence occur. The largest residual is for Hutchinson’s freckle on the head and neck, which occurs more often than would be expected under independence.\nFor the saturated model, \\(\\widehat{\\lambda}_{ij} = y_{ij}\\). In this example, \\(\\sum_{ij} \\widehat{\\lambda}_{ij} = \\sum_{ij} y_{ij} = 400\\), so the probability of a tumour being in category \\((i,j)\\) is \\(y_{ij}/400\\) — just the observed proportions.\nNote that in Table 6.2 we have a total of \\(y_{++} = 400\\) observations. If the data were truly Poisson, this would be a suspiciously round number. In reality, this total was fixed by design, so we should take into account the fact that \\(y_{++} = 400\\) and fit a more suitable model, such as the multinomial model which we will meet in the next chapter."
  },
  {
    "objectID": "6_loglinearmodel.html#exercises",
    "href": "6_loglinearmodel.html#exercises",
    "title": "6  Loglinear models",
    "section": "6.5 Exercises",
    "text": "6.5 Exercises\nExercises to be added later."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH3823 Generalized Linear Models",
    "section": "",
    "text": "\\[\n\\def\\b#1{\\mathbf{#1}}\n\\]\n\n\nWeekly schedule\n\n\n\n\n\n\nCoursework Practical Sessions (20 - 24 March)\n\n\n\n\nCoursework for this module involves a single written report worth 20% of the module grade. This will mainly involve investigating different models using R and interpreting the results. Tasks are expected to be handed out on 14 March with hand-in deadline expect to be 31 March. See Learning Resources / Practical Assessment for details of the tasks and for submission links.\n\n\n\n\n\n\n\n\n\nWeek 8 (20 - 24 March)\n\n\n\n\nBefore next Lecture: Re-read Chapter 5: Sections 5.1-5.3.\nLecture on Tuesday: Cancelled due to UCU strike.\nComputer Practical on Tuesday: Cancelled due to UCU strike.\nComputer Practical on Wednesday: Cancelled due to UCU strike.\nLecture on Thursday: Continue Chapter 6: Loglinear Modelling.\nWeekly feedback: Relevant exercise questions from last year (solutions online).\n\n\n\n\n\n\n\n\n\nWeek 7 (13 - 17 March)\n\n\n\n\nBefore next Lecture: Re-read Chapter 5: Sections 5.1-5.3.\nLecture on Tuesday: Complete Chapter 5: by looking at Section 5.4 - Application to dose-response experiments.\nLecture on Thursday: Cancelled due to UCU strike. Please self-study Chapter 6: Loglinear Modelling with Section 6.1 - Overview and Section 6.2 - Motivating Examples.\nWeekly feedback: Relevant exercise questions from last year (solutions online).\n\n\n\n\n\n\n\n\n\nWeek 6 (6 - 10 March)\n\n\n\n\nBefore next Lecture: Re-read Chapter 4: Sections 4.1-4.4.\nLecture on Tuesday: Complete Chapter 4: with Section 4.5 Fitting GLMs in R\nLecture on Thursday: Start Chapter 5: Logistic Regression with Sections 5.1 & 5.2.\nWeekly feedback: Relevant exercise questions from last year (solutions online).\n\n\n\n\n\n\n\n\n\nWeek 5 (27 February - 3 March)\n\n\n\n\nBefore next Lecture: Re-read Chapter 4: Section 4.1.\nLecture on Tuesday: Cancelled due to illness. Please read Chapter 4: Section 4.2.\nLecture on Thursday: Chapter 4: Sections 4.3, 4.4 & 4.5.\nWeekly feedback: Start Exercises in Chapter 4.\n\n\n\n\n\n\n\n\n\nWeek 4 (20 - 24 February)\n\n\n\n\nBefore next Lecture: Be confident with all material up to, and including, Section 3.4 Moments of exponential-family distributions.\nLecture on Tuesday: Chapter 3: Sections 3.5 & 3.6\nLecture on Thursday: Start Chapter 4 by covering Section 4.1.\nWeekly feedback: Complete Exercises in Chapter 3.\n\n\n\n\n\n\n\n\n\nWeek 3 (13 - 17 February)\n\n\n\n\nBefore next Lecture: Be confident with material in Chapter 2: Essentials of Normal Linear Models.\nLecture on Tuesday: Cancelled due to UCU strike. Instead, self-study Chapter 3: Sections 3.1 & 3.2.\nLecture on Thursday: Cancelled due to UCU strike. Instead, self-study Chapter 3: Sections 3.3 & 3.4.\nBefore next Lecture: Complete questions and check solutions, including video(s), for all Exercises in Chapters 1 and 2. Start Exercises in Chapter 3.\n\n\n\n\n\n\n\n\n\nWeek 2 (6 - 10 February)\n\n\n\n\nBefore next Lecture: Please re-read Section 2.1: Overview and read Section 2.2: Types of normal linear model.\nLecture on Tuesday: We will briefly cover all remaining material in Chapter 2: Essentials of Normal Linear Models.\nBefore next Lecture: Please re-read Chapter 2 carefully.\nLecture on Thursday: Cancelled due to UCU strike.\nWeekly feedback: Self-study the Exercises in Section 2.6 – solutions to be posted during Week 3.\n\n\n\n\n\n\n\n\n\nWeek 1 (30 January - 3 February)\n\n\n\n\nBefore next Lecture: Please read the Overview.\nLecture on Tuesday: We will briefly cover all material in Chapter 1: Introduction.\nBefore next Lecture: Please re-read Chapter 1 carefully.\nLecture on Thursday: Start Chapter 2: Essentials of Normal Linear Models with Section 2.1: Overview.\nWeekly feedback: Self-study the Exercises in Section 1.5 – solutions to be posted during Week 1."
  }
]