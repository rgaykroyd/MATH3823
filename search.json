[
  {
    "objectID": "1_intro.html#overview",
    "href": "1_intro.html#overview",
    "title": "1  Introduction",
    "section": "1.1 Overview",
    "text": "1.1 Overview\nIn previous modules you have studied linear models with a normally distributed error term, such as simple linear regression, multiple linear regression and ANOVA for normally distributed observations. In this module we will study generalized linear models.\nOutline of the module:\n\nRevision of linear models with normal errors.\nIntroduction to generalized linear models, GLMs.\nLogistic regression models.\nLoglinear models, including contingency tables.\n\n\n\n\n\n\n\nImportant\n\n\n\nThis module will make extensive use of \\(\\mathbf{R}\\) and hence it is very important that you are comfortable with its use. If you need some revision, then material is available on Minerva under RStudio Support.\n\n\nThe purpose of a generalized linear model is to describe the dependence of a response variable \\(y\\) on a set of \\(p\\) explanatory variables \\(\\b{x}=(x_1, x_2, \\ldots, x_p)\\) where, conditionally on \\(\\b{x}\\), observation \\(y\\) has a distribution which is not necessarily normal. Note that the normal distribution situation is a special case of the general framework and we will study that in the next Chapter.\nPlease be aware that in this learning material we may use lowercase letters, for example \\(y\\) or \\(y_i,\\) to denote both observed values or random variables, which is being considered should be clear from the context.\n\n\n\n\n\n\nImportant\n\n\n\nThis module will make extensive use of many basic ideas from statistics. If you need some revision, then see Appendix A: Basic material on Minerva under Basic Pre-requisite Material."
  },
  {
    "objectID": "1_intro.html#motivating-example",
    "href": "1_intro.html#motivating-example",
    "title": "1  Introduction",
    "section": "1.2 Motivating example",
    "text": "1.2 Motivating example\nTable 1.1 shows data1 on the number of beetles killed by five hours of exposure to 8 different concentrations of gaseous carbon disulphide.\n\n\nTable 1.1: Numbers of beetles killed by five hours of exposure to 8 different concentrations of gaseous carbon disulphide\n\n\n\n\n\n\n\nDose\n\\(x_i\\)\nNo. of beetle\n\\(m_i\\)\nNo. killed\n\\(y_i\\)\n\n\n\n\n1.6907\n59\n6\n\n\n1.7242\n60\n13\n\n\n1.7552\n62\n18\n\n\n1.7842\n56\n28\n\n\n1.8113\n63\n52\n\n\n1.8369\n59\n53\n\n\n1.8610\n62\n61\n\n\n1.8839\n60\n60\n\n\n\n\nFigure 1.1 (a) shows the same data with a linear regression line superimposed. Although this line goes close to the plotted points, we can see some fluctuations around it. More seriously, this is a stupid model: it would predict a mortality rate of greater than 100% at a dose of 1.9 units, and a negative mortality rate at 1.65 units!\n\n\nCode\npar(mar=c(4,4,0,1))\n\nbeetle = read.table(\"https://rgaykroyd.github.io/MATH3823/Datasets/beetle.txt\", header=T)\n\ndose = beetle$dose\nmortality = beetle$died/beetle$total\n\nplot(dose, mortality, pch=16, col=\"blue\",\n     xlim=c(1.65, 1.90), xlab =\"Dose\",\n     ylim=c(-0.1, 1.1),  ylab=\"Mortality\")\nabline(h=c(0,1), lty=2)\n\n# Fit a linear model\nlm.fit = lm(mortality ~ dose)\nabline(lm.fit, col=\"red\")\n\n# Fit a logisitc model\nplot(dose, mortality, pch=16, col=\"blue\",\n     xlim=c(1.65, 1.90), xlab =\"Dose\",\n     ylim=c(-0.1, 1.1),  ylab=\"Mortality\")\nabline(h=c(0,1), lty=2)\n\ny = cbind(beetle$died, beetle$total-beetle$died)\nglm.fit = glm(y ~ dose, family=binomial(link='logit'))\n\noutput.dose = seq(1.6,1.95,0.001)\nfitted = predict(glm.fit, data.frame(dose=output.dose), type=\"response\")\nlines(output.dose, fitted, col=\"red\")\n\n\n\n\n\n\n\n\n(a) Linear model\n\n\n\n\n\n\n\n(b) Logistic model\n\n\n\n\nFigure 1.1: Beetle mortality rates with fitted dose- response curves.\n\n\n\nA more sensible dose–response relationship for the beetle mortality data might be based on the logistic function (to be defined later), as plotted in Figure 1.1 (b). The resulting curve is a closer, more-sensible, fit. Later in this module we will see how this curve was fitted using maximum likelihood estimation for an appropriate generalized linear model.\nThis is an example of a dose-response experiment which are widely used in medical and pharmaceutical situations.\n\n\n\n\n\n\nWarning\n\n\n\nWarning of potentially sensitive material. For further information on dose-response experiments see, for example, www.britannica.com/science/dose-response-relationship."
  },
  {
    "objectID": "1_intro.html#spot-the-correlation-quiz",
    "href": "1_intro.html#spot-the-correlation-quiz",
    "title": "1  Introduction",
    "section": "1.3 Spot the correlation quiz",
    "text": "1.3 Spot the correlation quiz\nTest your knowledge recall and comprehension to reinforce idea of linear relationships and correlation.\n\n\nFor each situation, choose one of the following statements which you think is most likely to apply.\n\nThe diastolic blood pressure and the weight of patients attending a heart health clinic at the Leeds General Infirmary. positive correlationuncorrlatednegative correlationother\nThe daily stock market closing prices of British Telecom and Virgin Media shares on the London Stock Exchange. positive correlationuncorrlatednegative correlationother\nThe daily rainfall and hours of sunshine collected at a weather monitoring station in the Pennines of Yorkshire. positive correlationuncorrlatednegative correlationother\nThe number of road accidents occurring at a busy roundabout and the UK Retail Prices Index. positive correlationuncorrlatednegative correlationother\n\n\n\n\nClick here to see explanations\n\n\nIt is well documented that people who are over-weight (or at least high BMI) are more likely to have high blood pressure. This is true for systolic and diastolic blood pressure as well as for men and women. It is not certain if the relationship will be linear, but it will lead to a strong positive correlation value. This is likely to be a causal relationship, excess weight causes high blood pressure.\nAlthough these two companies are in the same business sectors, technology and entertainment, it is unlikely that direct competition will be the main factor in the relative behaviour – if it were, then we might expect a negative correlation, that is one does well if the other does badly. Instead, they are both likely to be driven by the same economic and social trends. This is not a causal relationship but both are being driven by an (unseen) third variable. It does, however, lead to a correlation.\nAlthough both weather features, rainfall and sun, can happen at the same time – perhaps leading to a rainbow – and there are very many cases when neither occurs – for example a cloudy but dry day – there is a general pattern that it will not rain when it is sunny and it will not be sunny when it rains. This leads to a negative correlation and a moderate value is likely even if the relationship is not linear. Again, this is not a causal relationship but is being driven, perhaps, by the presence of clouds.\nIt is hard to imagine that there will be a relationship between the number of road accidents and an inflation measure, hence a value of correlation close to zero – though do not expect to ever get a value of exactly zero. It is not completely, inconceivable that the number of road accidents will be higher when the economy is active, but this is unlikely to lead to a substantial correlation value – if you know otherwise, let me know!\n\n\n\n\n\nFrom each of the following scatter plots, choose from the drop down list which correlation value is most likely.\n\nWhat is the most likely correlation for the data below? -1.0-0.7-0.30.0+0.3+0.7+1.06. What is the most likely correlation for the data below? -1.0-0.7-0.30.0+0.3+0.7+1.07. What is the most likely correlation for the data below? -1.0-0.7-0.30.0+0.3+0.7+1.08. What is the most likely correlation for the data below? -1.0-0.7-0.30.0+0.3+0.7+1.09. What is the most likely correlation for the data below? -1.0-0.7-0.30.0+0.3+0.7+1.010. What is the most likely correlation for the data below? -1.0-0.7-0.30.0+0.3+0.7+1.0\n\n\n\n\nClick here to see explanations\n\n\nImagine dividing the plot by horizontal and vertical lines at the respective mean values. There would be a majority of points in the top-right and bottom-left indicating a positive correlation, but there are still some on the other quadrants. Hence, +1 is too high and 0.3 is too low, but would be suitable if the points were closer to a line or more dispersed, respectively. For information, the exact value is 0.70.\nAgain, imagine dividing the plot by horizontal and vertical lines at the respective mean values. This time the majority of points would be in the top-left and bottom-right quadrants, and there is moderate spread. A value -1 is too extreme and -0.3 is too close to zero, but would be suitable if the points were closer to a line or more dispersed, respectively. For information, the exact value is -0.60.\nA similar situation to Question 6, but there is noticeably more spread. Although the majority of points are in the top-left and bottom-right quadrants there are substantial numbers in the other quadrants. It would be inaccurate to say that this shows uncorrelated variables as there is a definite negative slope to the pattern. For information, the exact value is -0.30.\nThis is a difficult one as there is a clear relationship, but it is quadratic rather than linear. For information, the exact value is 0.30. Perhaps without the accompanying graph, this correlation value would be misleading.\nDividing the plot by horizontal and vertical lines at the respective mean values leaves very similar numbers of points in al four quadrants. This indicates that the correlation will be close to zero – here is no relationship. For information, the exact value is 0.01.\nAlmost all of the points are in the top-left and bottom-right quadrants indicating a negative correlation. The points are very close to the linear and hence a value close to -1 is likely – such extreme cases are rare. For information, the exact value is -0.995."
  },
  {
    "objectID": "1_intro.html#revision-of-least-squares-estimation",
    "href": "1_intro.html#revision-of-least-squares-estimation",
    "title": "1  Introduction",
    "section": "1.4 Revision of least-squares estimation",
    "text": "1.4 Revision of least-squares estimation\nSuppose that we have \\(n\\) paired data values \\((x_1, y_1),\\dots, (x_n, y_n)\\) and that we believe these are related by a linear model\n\\[\ny_i = \\alpha+\\beta x_i +\\epsilon_i\n\\]\nfor all \\(i\\in \\{1, 2,\\dots,n\\}\\), where \\(\\epsilon_1,\\dots,\\epsilon_n\\) are independent and identically distributed (iid) with \\(\\mbox{E}(\\epsilon_i)=0\\) and \\(\\mbox{Var}(\\epsilon_i)=\\sigma^2\\). The aim will be to find values of the model parameters, \\(\\alpha, \\beta \\text{ and } \\sigma^2\\) using the data. Specifically, we will estimate \\(\\alpha\\) and \\(\\beta\\) using the values which minimize the residual sum of squares (RSS)\n\\[\nRSS(\\alpha, \\beta) = \\sum_{i=1}^n \\left(y_i-(\\alpha+\\beta x_i)\\right)^2.\n\\tag{1.1}\\]\nThis measures how close the data points are around the regression line and hence the resulting estimates, \\(\\hat\\alpha\\) and \\(\\hat\\beta\\), will give us a fitted regression line which is closest to the data.\nFigure 1.2 illustrates this process. The data points are fixed but we are free to choose values for \\(\\alpha\\) and \\(\\beta\\), that is to move the line up and down and to rotate it as needed. In general, the data points, however, do not sit exactly on any line. For any values of \\(\\alpha\\) and \\(\\beta\\) the value on the line \\(y=\\alpha+\\beta x\\) can be calculated and the discrepancy, as measured in the vertical direction, is defined as the residual, \\(r_i=y_i-\\left(\\alpha+\\beta x_i\\right)\\). Then, the residual sum of squares is formed as the sum of the squares of these individual residuals.\n\n\n\n\n\nFigure 1.2: Diagram showing linear regression method\n\n\n\n\nIt can be shown that Equation 1.1 takes its minimum when the parameters are given by\n\\[\n\\hat\\alpha = \\bar y -\\hat\\beta\\bar x, \\quad \\mbox{and} \\quad\n\\hat\\beta = \\frac{s_{xy}}{s^2_x}\n\\tag{1.2}\\]\nwhere \\(\\bar x\\) and \\(\\bar y\\) are the sample means,\n\\[\ns_{xy}=\\frac{1}{n-1}\\sum_{i=1}^n (x_i-\\bar x)(y_i-\\bar y)\n\\]\nis the sample covariance and\n\\[\ns^2_x = \\frac{1}{n-1} \\sum_{i=1}^n (x_i-\\bar x)^2\n\\]\nis the sample variance of the \\(x\\) values. It can be shown that these estimators are unbiased, that is \\(\\mbox{E}[\\hat\\alpha]=\\alpha\\) and \\(\\mbox{E}[\\hat\\beta]=\\beta\\) – see Section 1.7.\nThe fitted regression lines is then given by \\(\\hat y = \\hat \\alpha +\\hat \\beta x\\), the fitted values by \\(\\hat y_i = \\hat \\alpha +\\hat \\beta x_i\\), and the model residuals by \\(r_i= \\hat \\epsilon_i= y_i-\\hat y_i\\) for all \\(i\\in \\{1,\\dots,n\\}.\\)\nTo complete the model fitting, we also estimate the error variance, \\(\\sigma^2\\), using \\[\n\\hat \\sigma^2 = \\frac{1}{n-2} \\sum _{i=1}^n r_i^2.\n\\tag{1.3}\\]\nNote that, by construction, \\(\\bar r=0\\) and, further, it can be shown that \\(\\hat \\sigma^2\\) is an unbiased estimator of \\(\\sigma^2\\), that is \\(\\mbox{E}[\\hat\\sigma^2]=\\sigma^2\\).\n\n\nCode\nxbar = mean(dose)\nybar = mean(mortality)\ns2x = var(dose)\nsxy = cov(dose, mortality)\n\nbetahat = sxy/s2x\nalphahat = ybar-betahat*xbar\n\ns2hat = sum((mortality-alphahat-betahat*dose)^2)/(length(dose)-2)\n\n\nReturning to the above beetle data example, we have \\(\\hat\\alpha=\\)-8.947843, \\(\\hat\\beta=\\) 5.324937, and \\(\\hat \\sigma^2 =\\) 0.0075151.\nWe will interpret the output later, but in \\(\\b{R}\\), the fitting can be done with a single command with corresponding fitting output from a second command:\n\n\nCode\nlm.fit = lm(mortality ~ dose)\n\nsummary(lm.fit)\n\n\n\nCall:\nlm(formula = mortality ~ dose)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.10816 -0.06063  0.00263  0.05119  0.12818 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -8.9478     0.8717  -10.27 4.99e-05 ***\ndose          5.3249     0.4857   10.96 3.42e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.08669 on 6 degrees of freedom\nMultiple R-squared:  0.9524,    Adjusted R-squared:  0.9445 \nF-statistic: 120.2 on 1 and 6 DF,  p-value: 3.422e-05\n\n\n\n\n\n\n\n\nImportant\n\n\n\nYou should have met \\(\\b{R}\\) output like this in previous statistics modules, but if you need some revision then see Appendix-C: Background to Analysis of Variance on Minerva under Basic Pre-requisite Material."
  },
  {
    "objectID": "1_intro.html#sec-typevariables",
    "href": "1_intro.html#sec-typevariables",
    "title": "1  Introduction",
    "section": "1.5 Types of variables",
    "text": "1.5 Types of variables\nThe way a variable enters a model will depends on its type. The most common five types of variable are:\n\nQuantitative\n\nContinuous: for example, height; weight; duration. Real valued. Note that although recorded data is rounded it is still usually best regarded as continuous.\nCount (discrete): for example, number of children in a family; accidents at a road junction; number of items sold. Non-negative and integer-valued.\n\nQualitative\n\nOrdered categorical (ordinal): for example, severity of illness (Mild/ Moderate/Severe); degree classification (first/ upper-second/ lower-second/ third).\nUnordered categorical (nominal):\n\nDichotomous (binary): two categories: for example sex (M/ F); agreement (Yes/ No); coin toss (Head/ Tail).\nPolytomous (also known as polychotomous): more than two categories: for example blood group (A/ B/ O); eye colour (Brown/ Blue/ Green).\n\n\n\nNote that although dichotomous is clearly a special case of polytomous, making the distinction is usually worthwhile as it often leads to a simplified modelling and testing approach."
  },
  {
    "objectID": "1_intro.html#spot-the-data-type-quiz",
    "href": "1_intro.html#spot-the-data-type-quiz",
    "title": "1  Introduction",
    "section": "1.6 Spot the data type quiz",
    "text": "1.6 Spot the data type quiz\n\n\nTest your knowledge recall and comprehension to reinforce ideas ready for later in the module\nFor each of the following situations what is the most appropriate data type: nominal, ordinal, discrete, or continuous?\n\nThe eye colour of 100 patients visiting the Yorkshire Cancer Research Centre, for example, grey, green, brown, blue…. nominalordinaldiscretecontinuous\nThe nationality of students at the University of Leeds, for example, British, Chinese, Greek, Indian…. nominalordinaldiscretecontinuous\nThe five-star ratings submitted by 50 customers on TripAdvisor for the Leeds Queens Hotel, for example 1 star, 2 star,… 5 star. nominalordinaldiscretecontinuous\nThe diastolic blood pressure of 20 male and 20 female patients attending a heart health clinic at the Leeds General Infirmary in a study to investigate differences between men and women, for example, 80 mm Hg, 130 mm Hg,… nominalordinaldiscretecontinuous\nThe daily stock market closing price of British Telecom shares on the London Stock Exchange over a year to study the change over time, for example, 114.75p, 115.10p,… nominalordinaldiscretecontinuous\nThe January monthly rainfall collected since 1961 at a weather monitoring station in the Pennines of Yorkshire, for example, 8 mm , 12 mm,… nominalordinaldiscretecontinuous\nThe level of satisfaction of 100 randomly chosen voters with the policies of a political party, for example, agree, fully agree, neither agree nor disagree, disagree, and fully disagree. nominalordinaldiscretecontinuous\nThe number of new people following the TheRoyalFamily twitter page per day over a year, for example 459, 700,… to study the change due to a royal wedding. nominalordinaldiscretecontinuous\nThe number of road accidents occurring per month, at a busy roundabout over a 10-year period to study the change over time, for example, 0, 1, 2,… nominalordinaldiscretecontinuous\nThe number of Scottish strawberries in 50 randomly selected boxes bought from ASDA supermarket, for example, 50, 58, 68,… nominalordinaldiscretecontinuous\n\n\n\n\nClick here to see explanations\n\n\nEye colour is qualitative and can take any one of an unordered set of categories. Although the eye colours are categories, there is no clear ordering to the colours.\nNationality is qualitative and can take any one of an unordered set of categories. Although the nationality are categories, there is no clear ordering to the countries.\nThe rating is qualitative and can take any one of set of categories but the categories are clearly ordered, 5 star is better than 4 start etc. Although the ratings are represented by integers, there is no reason why the difference between 1 and 2 stars has the same interpretation as between 4 and 5 stars and hence it cannot be discrete.\nAlthough the recorded values might take only integer values, blood pressure is a measurement and could take be any real number.\nShare price is a measurement and could be any real number, even though in practice it will be rounded.\nRainfall is a measurement and although the recorded values might take only integer values, rainfall could be any real number.\nSatisfaction score is qualitative and can take any one of set of categories but the categories are clearly ordered, fully agree is better than agree etc. Although the scores could be represented by numerical values, e.g. 1,2,3,4,5, there is no reason why the difference between 1 and 2 has the same interpretation as between 4 and 5 and hence it is not discrete.\nThe number of people is a quantitative count which is limited to the non-negative integers. The variable is discrete.\nThe number of accidents is a quantitative count, being limited to the non-negative integers and hence is discrete.\nThe number of strawberries is a quantitative a count which is limited to the non-negative integers. The variable is discrete."
  },
  {
    "objectID": "1_intro.html#sec-exercises1",
    "href": "1_intro.html#sec-exercises1",
    "title": "1  Introduction",
    "section": "1.7 Exercises",
    "text": "1.7 Exercises\n\n\n\n\n\n\nImportant\n\n\n\nUnless otherwise stated, data files will be available online at: rgaykroyd.github.io/MATH3823/Datasets/filename.ext, where filename.ext is the stated filename with extension.\n\n\n\n1.1 Consider again the beetle data in Table 1.1. Perform the calculations by hand and then check the answers using \\(\\b{R}\\) – a copy of the data is available in the file beetle.txt. Finally plot the fitted regression line on a scatter plot of the data. [Hint: See the code chunk used to produce Figure 1.1.]\n1.2 Consider the following synthetic data:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(i=1\\)\n\\(i=2\\)\n\\(i=3\\)\n\\(i=4\\)\n\\(i=5\\)\n\\(i=6\\)\n\\(i=7\\)\n\\(i=8\\)\n\n\n\n\n\\(x_i\\)\n-1\n0\n1\n2\n2.5\n3\n4\n6\n\n\n\\(y_i\\)\n-2.8\n-1.1\n7.2\n8.0\n8.9\n9.2\n14.8\n24.7\n\n\n\nPlot the data to check that a linear model is suitable and then fit a linear regression model. Do you think that the fitted model can be reliably used to predict the values of \\(y\\) when \\(x=5\\) and \\(x=10\\)? Justify your answers.\n1.3 Starting from Equation 1.1, derive the estimation equations given in Equation 1.2. Further, show that \\(\\hat\\alpha\\) and \\(\\hat\\beta\\) are unbiased estimators of \\(\\alpha\\) and \\(\\beta\\). [Hint: Check your MATH1712 lecture notes.]\nWhat can be said about \\(\\hat\\sigma^2\\) as an estimator of \\(\\sigma^2\\)? [Hint: There is a careful theoretical proof, but here only an intuitive explanation is expected.]\n1.4 The Brownlee’s Stack Loss Plant Data2 is already available in \\(\\mathbf{R}\\), with background details on the help page, \\(\\texttt{?stackloss}\\). [Hint: You already met this example in MATH1712.]\nAfter plotting all pairs of variables, which of \\(\\texttt{Air.Flow}\\), \\(\\texttt{Water.Temp}\\) and \\(\\texttt{Acid.Conc}\\) do you think could be used to model \\(\\texttt{stack.loss}\\) using a linear regression? Justify your answer.\nPerform a simple linear regression with using \\(\\texttt{stack.loss}\\) as the response variable and your chosen variable as the explanatory variable. Add the fitted regression line to a scatter plot of the data and comment.\n1.5 In an experiment conducted by de Silva et al. in 20203 data was obtained to investigate falling objects and gravity, as first consider by Galileo and Newton. A copy of the data is available in the file physics_from_data.csv.\nRead the data file into \\(\\b{R}\\) and perform a simple linear regression of the maximum Reynolds number as the response variable and, in turn, each of the other variables as the explanatory variable.\nPlot the data and add the corresponding fitted linear models. Which variable do you think helps explain Reynolds number the best? Why do you think this?"
  },
  {
    "objectID": "1_intro.html#footnotes",
    "href": "1_intro.html#footnotes",
    "title": "1  Introduction",
    "section": "",
    "text": "Dobson and Barnett, 3rd edn, p.127↩︎\nBrownlee, K. A. (1960, 2nd ed. 1965) Statistical Theory and Methodology in Science and Engineering. New York: Wiley. pp. 491–500.↩︎\nde Silva BM, Higdon DM, Brunton SL, Kutz JN. Discovery of Physics From Data: Universal Laws and Discrepancies. Front Artif Intell. 2020 Apr 28;3:25. doi: 10.3389/frai.2020.00025. PMID: 33733144; PMCID: PMC7861345.↩︎"
  },
  {
    "objectID": "0_preface.html",
    "href": "0_preface.html",
    "title": "Overview",
    "section": "",
    "text": "Official Module Description"
  },
  {
    "objectID": "0_preface.html#welcome",
    "href": "0_preface.html#welcome",
    "title": "Overview",
    "section": "Welcome!",
    "text": "Welcome!\nHere is a short video [4 mins] to introduce the module."
  },
  {
    "objectID": "0_preface.html#preface",
    "href": "0_preface.html#preface",
    "title": "Overview",
    "section": "Preface",
    "text": "Preface\nThese lecture notes are produced for the University of Leeds module MATH3823 - Generalized Linear Models for the academic year 2023-24. Please note that this material also forms part of the module MATH5824 - Generalized Linear and Additive Models. They are based on the lecture notes used previously for this module and I am grateful to previous module lecturers for their considerable effort: Lanpeng Ji, Amanda Minter, John Kent, Wally Gilks, and Stuart Barber. This year, again, I am using Quarto (a successor to RMarkdown) from RStudio to produce both the html and PDF, and then GitHub to create the website which can be accessed at rgaykroyd.github.io/MATH3823/. Please note that the PDF versions will only be made available on the University of Leeds Minerva system. Although I am a long-term user of RStudio, I am a novice at Quarto/RMarkdown and a complete beginner using Github and hence please be patient if there are hitches along the way.\nRG Aykroyd, Leeds, January 3, 2024"
  },
  {
    "objectID": "0_preface.html#changes-since-last-year",
    "href": "0_preface.html#changes-since-last-year",
    "title": "Overview",
    "section": "Changes since last year",
    "text": "Changes since last year\nFeedback from the students last year was very positive, but there were consistent comments regarding two issues: (1) a shortage of practice exercises and the opportunity to discuss these in class, and (2) limited RStudio support in preparation for the assessment. For the first of these, additional exercises have been prepared and are included in the learning material. Also, I am trying some short quizzes so that you can check your basic knowledge. Further, I intend to set-aside some lecture time for us to discuss selected exercises. For the second, an additional computer session has been added, in Week 5 (26 February - 1 March), this is 3 weeks before the assessed practice in Week 8 (18 - 22 March). Further, a few new instructional videos will be available addressing some RStudio topics. Together, these represents a considerable about of extra work for me, but I hope that they are helpful and so please give your feedback whenever there is an opportunity.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nStatistical ethics and sensitive data\nPlease note that from time to time we will be using data sets from situations which some might perceive as sensitive. All such data sets will, however, be derived from real-world studies which appear in textbooks or in scientific journals. The daily work of many statisticians involves applying their professional skills in a wide variety of situations and as such it is important to include a range of commonly encountered examples in this module. Whenever possible, sensitive topics will be signposted in advance. If you feel that any examples may be personally upsetting then, if possible, please contact the module lecturer in advance. If you are significantly effected by any of these situations, then you can seek support from the Student Counselling and Wellbeing service."
  },
  {
    "objectID": "0_preface.html#module-summary",
    "href": "0_preface.html#module-summary",
    "title": "Overview",
    "section": "Module summary",
    "text": "Module summary\nLinear regression is a tremendously useful statistical technique but is very limited. Generalised linear models extend linear regression in many ways - allowing us to analyse more complex data sets. In this module we will see how to combine continuous and categorical predictors, analyse binomial response data and model count data."
  },
  {
    "objectID": "0_preface.html#objectives",
    "href": "0_preface.html#objectives",
    "title": "Overview",
    "section": "Objectives",
    "text": "Objectives\nOn completion of this module, students should be able to:\n\ncarry out regression analysis with generalised linear models including the use of link functions;\nunderstand the use of deviance in model selection;\nappreciate the problems caused by overdispersion;\nfit and interpret the special cases of log linear models and logistic regression;\nuse a statistical package with real data to fit these models to data and to write a report giving and interpreting the results."
  },
  {
    "objectID": "0_preface.html#syllabus",
    "href": "0_preface.html#syllabus",
    "title": "Overview",
    "section": "Syllabus",
    "text": "Syllabus\nGeneralised linear model; probit model; logistic regression; log linear models."
  },
  {
    "objectID": "0_preface.html#university-module-catalogue",
    "href": "0_preface.html#university-module-catalogue",
    "title": "Overview",
    "section": "University Module Catalogue",
    "text": "University Module Catalogue\nFor any further details, please see MATH3823 Module Catalogue page"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH3823 Generalized Linear Models",
    "section": "",
    "text": "\\[\n\\def\\b#1{\\mathbf{#1}}\n\\]\n\n\nWeekly schedule\nItems will be added here week-by-week and so keep checking when you need up-to-date information on what you should be doing.\n\n\n\n\n\n\nWeek 1 (29 January - 2 February)\n\n\n\n\nBefore first Lecture: Please read the Overview.\nLecture on Tuesday: We will briefly cover all material in Chapter 1: Introduction.\nBefore next Lecture: Please re-read Chapter 1 carefully, especially any sections not covered in Lecture.\nLecture on Thursday: Start Chapter 2: Essentials of Normal Linear Models with Section 2.1: Overview.\nWeekly feedback: Complete the Quizzes and self-study the Exercises in Section 1.7 – solutions to be posted during Week 1.\n\n\n\n\n\n\n\n\n\nAdvanced notice\n\n\n\n\nModule Assessment: Set on 14 March with submission deadline 23 April (that is after the break). You will be expected to write a short report based on an RStudio practical.\nComputer classes: 27/28 February for Practice and 19/20 March for Assessment – check your timetable."
  }
]