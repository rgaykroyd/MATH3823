[
  {
    "objectID": "1_intro.html",
    "href": "1_intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "In previous modules you have studied linear models with a normally distributed error term, such as simple linear regression, multiple linear regression and ANOVA for normally distributed observations. In this module we will study generalized linear models.\nOutline of the module:\n\nRevision of Gaussian linear models.\nIntroduction to generalized linear models, GLMs.\nLogistic regression models.\nLoglinear models, including contingency tables.\n\nThe purpose of a generalized linear model is to model the dependence of a dependent variable \\(y\\) on a set of \\(p\\) explanatory variables \\(x=(x_1, x_2, \\ldots, x_p),\\) where conditionally on \\(x\\), observation \\(y\\) has a distribution which is not necessarily normal.\nNote that in these notes we use lowercase \\(y\\) or \\(y_i\\) to denote both observed values or random variables, which should be clear from the context.\n\n\n\n\n\n\nWarning\n\n\n\nStatistical ethics and sensitive data\nPlease note that from time to time we will be using data sets from situations which might be perceived as sensitive. All such data sets will, however, be derived from real-world studies which appear in textbooks or in scientific journals. The daily work of many statisticians involves applying their professional skills in a wide variety of situations and as such it is important to include a range of common examples in this module. Whenever possible, sensitive topics will be signposted in advance. If you feel that any examples may be personally upsetting then, if possible, please contact the module lecturer in advance. If necessary, then please consider talking with the Student Counselling and Wellbeing service."
  },
  {
    "objectID": "1_intro.html#motivating-example",
    "href": "1_intro.html#motivating-example",
    "title": "1  Introduction",
    "section": "1.2 Motivating example",
    "text": "1.2 Motivating example\nTable 1.1 shows data1 on the number of beetles killed by five hours of exposure to 8 different concentrations of gaseous carbon disulphide.\n\n\nTable 1.1: Numbers of beetles killed by five hours of exposure to 8 different concentrations of gaseous carbon disulphide\n\n\n\n\n\n\n\nDose\n\\(x_i\\)\nNo. of beetle\n\\(m_i\\)\nNo. killed\n\\(y_i\\)\n\n\n\n\n1.6907\n59\n6\n\n\n1.7242\n60\n13\n\n\n1.7552\n62\n18\n\n\n1.7842\n56\n28\n\n\n1.8113\n63\n52\n\n\n1.8369\n59\n53\n\n\n1.8610\n62\n61\n\n\n1.8839\n60\n60\n\n\n\n\nFigure 1.1 (a) shows the same data with a linear regression line superimposed. Although this line goes close to the plotted points, we can see some fluctuations around it. More seriously, this is a stupid model: it would predict a mortality rate >100% at a dose of 1.9units, and a negative mortality rate at 1.65units!\n\n\nCode\nbeetle = read.table(\"data/beetle.txt\", header=T)\n\ndose = beetle$dose\nmortality = beetle$died/beetle$total\n\nplot(dose, mortality, pch=16,\n     xlim=c(1.65, 1.90), xlab =\"Dose\",\n     ylim=c(-0.1,1.1), ylab=\"Mortality\")\nabline(h=c(0,1), lty=2)\n\nlm.fit = lm(mortality ~ dose)\nabline(lm.fit)\n\nplot(dose, mortality, pch=16,\n     xlim=c(1.65, 1.90), xlab =\"Dose\",\n     ylim=c(-0.1,1.1), ylab=\"Mortality\")\nabline(h=c(0,1), lty=2)\n\nym = cbind(beetle$died, beetle$total-beetle$died)\nglm.fit = glm(ym~dose, family=binomial(link='logit'))\n\ntmp = seq(1.6,1.95,0.001)\nfitted = predict(glm.fit, data.frame(dose=tmp), type=\"response\")\nlines(tmp, fitted)\n\n\n\n\n\n\n\n\n(a) Linear model\n\n\n\n\n\n\n\n(b) Logistic model\n\n\n\n\nFigure 1.1: Beetle mortality rates with fitted dose- response curves.\n\n\n\nA more sensible dose–response relationship for the beetle mortality data might be based on the logistic function (to be defined later), as plotted in Figure 1.1 (b). The resulting curve is a closer, more-sensible, fit. Later in this module we will see how this curve was fitted using maximum likelihood estimation for an appropriate generalized linear model."
  },
  {
    "objectID": "1_intro.html#revision-of-least-squares-estimation",
    "href": "1_intro.html#revision-of-least-squares-estimation",
    "title": "1  Introduction",
    "section": "1.3 Revision of least-squares estimation",
    "text": "1.3 Revision of least-squares estimation\nSuppose that we have \\(n\\) paired data values \\((x_1, y_1),\\dots, (x_n, y_n)\\) and that we believe these are related by a linear model\n\\[\nY_i = \\alpha+\\beta x_i +\\epsilon_i\n\\]\nfor all \\(i\\in \\{1, 2,\\dots,n\\}\\), where \\(\\epsilon_1,\\dots,\\epsilon_n\\) are independent and identically distributed (iid) with \\(\\mbox{E}(\\epsilon_i)=0\\) and \\(\\mbox{Var}(\\epsilon_i)=\\sigma^2\\). The aim will be to find values of the model parameters, \\(\\alpha, \\beta \\text{ and } \\sigma^2\\) using the data. Specifically, we will estimate \\(\\alpha\\) and \\(\\beta\\) using the values which minimize the residual sum of squares (RSS)\n\\[\nRSS(\\alpha, \\beta) = \\sum_{i=1}^n \\left(y_i-(\\alpha+\\beta x_i)\\right)^2.\n\\tag{1.1}\\]\nThis measures how close the data points are around the regression line and hence the resulting estimates, \\(\\hat\\alpha\\) and \\(\\hat\\beta\\), will give us a fitted regression line which is ‘’closest’’ to the data.\nIt can be shown that Equation 2.1 takes its minimum when the parameters are given by\n\\[\n\\hat\\alpha = \\bar y -\\hat\\beta\\bar x, \\quad \\mbox{and} \\quad\n\\hat\\beta = \\frac{s_{xy}}{s^2_x}\n\\tag{1.2}\\]\nwhere \\(\\bar x\\) and \\(\\bar y\\) are the sample means,\n\\[\ns_{xy}=\\frac{1}{n-1}\\sum_{i=1}^n (x_i-\\bar x)(y_i-\\bar y)\n\\]\nis the sample covariance and\n\\[\ns^2_x = \\frac{1}{n-1} \\sum_{i=1}^n (x_i-\\bar x)^2\n\\]\nis the sample variance of the \\(x\\) values. It can be shown that these estimators are unbiased, that is \\(\\mbox{E}[\\hat\\alpha]=\\alpha\\) and \\(\\mbox{E}[\\hat\\beta]=\\beta\\).\nThe fitted regression lines is then given by \\(\\hat y = \\hat \\alpha +\\hat \\beta x\\), the fitted values by \\(\\hat y_i = \\hat \\alpha +\\hat \\beta x_i\\), and the model residuals by \\(r_i= \\hat \\epsilon_i= y_i-\\hat y_i\\) for all \\(i\\in \\{1,\\dots,n\\}\\).\nTo complete the model fitting, we must also estimate the residual variance, \\(\\sigma^2\\) which is given by\n\\[\n\\hat \\sigma^2 = \\frac{1}{n-2} \\sum _{i=1}^n r_i^2.\n\\tag{1.3}\\]\nNote that, by construction, \\(\\bar r=0\\). Further, it can be shown that this is an unbiased estimator, that is \\(\\mbox{E}[\\hat\\sigma^2]=\\sigma^2\\) .\n\n\nCode\nxbar = mean(dose)\nybar = mean(mortality)\ns2x = var(dose)\nsxy = cov(dose, mortality)\n\nbetahat = sxy/s2x\nalphahat = ybar-betahat*xbar\n\ns2hat = sum((mortality-alphahat-betahat*dose)^2)/(length(dose)-2)\n\n\nReturning to the above beetle data example, we have \\(\\hat\\alpha=\\)-8.947843, \\(\\hat\\beta=\\) 5.324937, and \\(\\hat \\sigma^2 =\\) 0.0075151.\nIn R, the fitting can be done with a single command with\n\nlm.fit = lm(mortality ~ dose)\n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = mortality ~ dose)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.10816 -0.06063  0.00263  0.05119  0.12818 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -8.9478     0.8717  -10.27 4.99e-05 ***\ndose          5.3249     0.4857   10.96 3.42e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.08669 on 6 degrees of freedom\nMultiple R-squared:  0.9524,    Adjusted R-squared:  0.9445 \nF-statistic: 120.2 on 1 and 6 DF,  p-value: 3.422e-05"
  },
  {
    "objectID": "1_intro.html#types-of-variables",
    "href": "1_intro.html#types-of-variables",
    "title": "1  Introduction",
    "section": "1.4 Types of variables",
    "text": "1.4 Types of variables\nThe way a variable enters a model will depends on its type. The most common five types of variable are:\n\nQuantitative\n\nContinuous: for example, height; weight; duration. Real valued – data is rounded but still usually best regarded as continuous.\nCount (discrete): for example, number of children in a family; accidents at a road junction; number of items sold. Non-negative and integer-valued.\n\nQualitative\n\nOrdered categorical (ordinal): for example, severity of illness (Mild/ Moderate/Severe); degree classification (first/ upper-second/ lower-second/ third).\nUnordered categorical (nominal):\n\nDichotomous (binary): two categories: for example sex (M/ F); agreement (Yes/ No); coin toss (Head/ Tail).\nPolytomous2: more than two categories: for example blood group (A/ B/ O); eye colour (Brown/ Blue/ Green)."
  },
  {
    "objectID": "1_intro.html#exercises",
    "href": "1_intro.html#exercises",
    "title": "1  Introduction",
    "section": "1.5 Exercises",
    "text": "1.5 Exercises\n\n1.1 Consider the following synthetic data:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(i=1\\)\n\\(i=2\\)\n\\(i=3\\)\n\\(i=4\\)\n\\(i=5\\)\n\\(i=6\\)\n\\(i=7\\)\n\\(i=8\\)\n\n\n\n\n\\(x_i\\)\n-1\n0\n1\n2\n2.5\n3\n4\n6\n\n\n\\(y_i\\)\n-2.8\n-1.1\n7.2\n8.0\n8.9\n9.2\n14.8\n24.7\n\n\n\nPlot the data to check that a linear model is suitable and then fit a linear regression model. Do you think that the fitted model can be reliably used to predict the value of \\(y\\) when \\(x=10\\)? Justify your answer.\n1.2 Starting from Equation 1.1, derive the estimation equations given in Equation 1.2. Further, show that \\(\\hat\\alpha\\), \\(\\hat\\beta\\) and \\(\\hat\\sigma^2\\) are unbiased estimators.\nHint: Check your MATH1712 lecture notes.\n1.3 In an experiment conducted by de Silva et al. in 20203 data was obtained to investigate falling objects and gravity, as first consider by Galileo and Newton.\nA copy of the data is available on Minerva in the file: physics_from_data.csv\nRead the data file into RStudio and perform a simple linear regression of the maximum Reynolds number as the response variables and, in turn, each of the other variables as the explanatory variable.\nPlot the data and add the corresponding fitted linear models. Which variable do you think helps explain Reynolds number the best? Why do you think this?\n\n\nHere are an infinite number of further numerical examples from maths e.g. (thanks to https://www.mathcentre.ac.uk/):\nFinding the intersercept\nFinding the slope - Part 1\nFinding the slope - Part 2"
  },
  {
    "objectID": "2_linearmodels.html",
    "href": "2_linearmodels.html",
    "title": "2  Essentials of Gaussian Linear Models",
    "section": "",
    "text": "In many fields of application, we might assume the dependent variable is Gaussian, that is normally distributed; for example: heights, weights, log prices.\nThe data1 in Table 2.1 record the birth weights of 12 girls and 12 boys and their gestational ages (time from conception to birth).\n\n\nTable 2.1: Gestational ages (in weeks) and birth weights (in grams) for 24 babies (12 girls and 12 boys).\n\n\n\n\n(a) Girls\n\n\nGestational Age\nBirth weight\n\n\n\n\n40\n3317\n\n\n36\n2729\n\n\n40\n2935\n\n\n37\n2754\n\n\n42\n3210\n\n\n39\n2817\n\n\n40\n3126\n\n\n37\n2539\n\n\n36\n2412\n\n\n38\n2991\n\n\n39\n2875\n\n\n40\n3231\n\n\n\n\n\n\n(b) Boys\n\n\nGestational Age\nBirth weight\n\n\n\n\n40\n2968\n\n\n38\n2795\n\n\n40\n3163\n\n\n35\n2925\n\n\n36\n2625\n\n\n37\n2847\n\n\n41\n3292\n\n\n40\n3473\n\n\n37\n2628\n\n\n38\n3176\n\n\n40\n3421\n\n\n38\n3975\n\n\n\n\n\n\nA key question is can we predict the birth weight of a baby born at a given gestational age using these data. For this we will need to make assumptions about the relationship between birth weight and gestational age, and the corresponding natural variation – that is we require a model.\nFirst we should explore the data. Figure 2.1 (a) shows a histogram of the birth weights indicating a spread with modal group 2800-300grams; Figure 2.1 (b) indicates slightly higher birth weights for the boys than the girls; and Figure 2.1 (c) shows an increasing relationship between weight and age. Together, these suggest that gestational age and sex are likely to be important for predicting weight.\n\n\nCode\nbirthweight = read.table(\"data/birthwt-numeric.txt\", header=T)\n\nweight = birthweight$weight\nage = birthweight$age\nsex = birthweight$sex\n\nhist(weight, breaks=6, probability = T, main = \"\", \n     xlab = \"Birth weight (grams)\")\nboxplot(weight~sex)\n\nplot(age, weight, pch=16, \n     xlab = \"Gestational age (weeks)\",\n     ylab = \"Birth weight (grams)\")\n\n\n\n\n\n\n\n\n(a) Weight distribution\n\n\n\n\n\n\n\n(b) Weight divided by Sex\n\n\n\n\n\n\n\n\n\n(c) Relationship beween variables\n\n\n\n\nFigure 2.1: Birthweight and gestational age for 24 babies.\n\n\n\nBefore considering possible models, Figure 2.2 again shows the relationship between weight and age but with the points coloured according to the baby’s sex. This, perhaps, shows the boys to have generally higher weights across the age range than girls.\n\n\nCode\nbirthweight = read.table(\"data/birthwt-numeric.txt\", header=T)\n\nweight = birthweight$weight\nage = birthweight$age\nsex = birthweight$sex\n\nplot(age, weight, col=sex+1, pch=16)\n\n\n\n\n\nFigure 2.2: Birthweight and gestational age for 12 girls (red dots) and 12 boys (black dots).\n\n\n\n\nOf course, there are very many possible models, but here we will consider the following:\n\n\n\n\n\n\n\n\\(\\texttt{Model 0}:\\)\n\\(\\texttt{Weight}=\\alpha\\)\n\n\n\\(\\texttt{Model 1}:\\)\n\\(\\texttt{Weight}=\\alpha + \\beta.\\texttt{Age}\\)\n\n\n\\(\\texttt{Model 2}:\\)\n\\(\\texttt{Weight}=\\alpha + \\beta.\\texttt{Age}+\\gamma.\\texttt{Sex}\\)\n\n\n\\(\\texttt{Model 3}:\\)\n\\(\\texttt{Weight}=\\alpha + \\beta.\\texttt{Age}+\\gamma.\\texttt{Sex} + \\delta.\\texttt{Age}.\\texttt{Sex}\\)\n\n\n\nIn these models, \\(\\texttt{Weight}\\) is called the response variable (sometimes called the dependent variable) and \\(\\texttt{Age}\\) and \\(\\texttt{Sex}\\) are called the covariates or explanatory variables (sometimes called the predictor or independent variables). Here, \\(\\texttt{Age}\\) is a continuous variable whereas \\(\\texttt{Sex}\\) is coded as a dummy variable taking the value 0 for girls and 1 for boys; it is an example of a factor, in this case with just two levels: Boy and Girl.\nIn these models, \\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\) and \\(\\delta\\) are model parameters. Parameter \\(\\alpha\\) is called the intercept term; \\(\\beta\\) is called the main effect of \\(\\texttt{Age}\\); and is interpreted as the effect on birth weight per week of gestational age. Similarly, \\(\\gamma\\) is the main effect of \\(\\texttt{Sex}\\), interpreted as the effect on birth weight of being a boy (because girl is the baseline category).\nParameter \\(\\delta\\) is called the interaction effect between \\(\\texttt{Age}\\) and \\(\\texttt{Sex}\\). Take care when interpreting an interaction effect. Here, it does not mean that age somehow affects sex, or vice-versa. It means that the effect of gestational age on birth weight depends on whether the baby is a boy or a girl.\nThese models can be fitted to the data using (Ordinary) Least Squares to produce the results presented in Figure 2.3.\nWhich model should we use?\n\n\nCode\nbirthweight = read.table(\"data/birthwt-numeric.txt\", header=T)\n\nweight = birthweight$weight\nage = birthweight$age\nsex = birthweight$sex\n\nplot(age, weight, pch=16, \n     xlab = \"Gestational age (weeks)\",\n     ylab = \"Birth weight (grams)\")\nabline(h=mean(weight))\n\n\nplot(age, weight, pch=16,  \n     xlab = \"Gestational age (weeks)\",\n     ylab = \"Birth weight (grams)\")\nM1.fit = lm(weight~age)\nabline(M1.fit$coefficients[1],M1.fit$coefficients[2])\n\n\nplot(age, weight, pch=16, col=sex+1, \n     xlab = \"Gestational age (weeks)\",\n     ylab = \"Birth weight (grams)\")\nM1.fit = lm(weight~age+sex)\n\nabline(M1.fit$coefficients[1],M1.fit$coefficients[2], col=1)\nabline(M1.fit$coefficients[1]+M1.fit$coefficients[3],M1.fit$coefficients[2], col=2)\n\n\nplot(age, weight, pch=16, col=sex+1, \n     xlab = \"Gestational age (weeks)\",\n     ylab = \"Birth weight (grams)\")\nM1.fit = lm(weight~age+sex+age*sex)\n\nabline(M1.fit$coefficients[1],M1.fit$coefficients[2], col=1)\nabline(M1.fit$coefficients[1]+M1.fit$coefficients[3],M1.fit$coefficients[2]+M1.fit$coefficients[4], col=2)\n\n\n\n\n\n\n\n\n(a) Model 0\n\n\n\n\n\n\n\n(b) Model 1\n\n\n\n\n\n\n\n\n\n(c) Model 2\n\n\n\n\n\n\n\n(d) Model 3\n\n\n\n\nFigure 2.3: Birthweight and gestational age data with superimposed fitted regression lines from various competing models.\n\n\n\nWe know from previous modules that statistical tests can be used to check the importance of regression coefficients and model parameters, but it is also important to use the graphical results, as in Figure 2.3, to guide us.\n\\(\\texttt{Model 0}\\) says that there is no change in birth weight with gestational age which means that we would use the average birth weight as the prediction whatever the gestational age – this makes no sense. As we can easily see from the scatter plot of the data, the fitted line in this case is clearly inappropriate.\n\\(\\texttt{Model 1}\\) does not take into account whether the baby is a girl or a boy, but does model the relationship between birth weight and gestational age. This does seem to provide a good fit and might be adequate for many purposes. Recall from Figure 2.1 (b) and Figure 2.2, however, that for a given gestational age the boys seem to have a higher birth weight than the girls.\n\\(\\texttt{Model 2}\\) does take the sex of the baby into account by allowing separate intersepts in the fitted lines – this means that the lines are parallel. By eye, there is a clear difference between these two lines but it might not be important.\n\\(\\texttt{Model 3}\\) allows for separate slopes as well as intercepts. There is a slight difference in the slopes, with the birth weight of the girls gradually catching-up as the gestational age increases. It is difficult to see, however, if this will be a general pattern or if it is only true for this data set – especially given the relatively small sample size.\nHere, it is not clear by eye which of the fitted models will be the best and hence we can use a statistical test to help. In particular, we can choose between the models using F-tests.\nLet the four models be indexed by \\(k=0,1,2,3\\). Let \\(y_i\\) denote the value of the dependent variable \\(\\texttt{Weight}\\) for individual \\(i=1,\\dots,n\\).\nLet \\(R_k\\) denote the residual sum of squares (RSS) for Model \\(k:\\)\n\\[\nR_k = \\sum_{i=1}^n (y_i - \\hat{\\mu}_{ki})^2,\n\\tag{2.1}\\]\nwhere \\(\\hat{\\mu}_{ki}\\) is the fitted value for individual \\(i\\) under \\(\\texttt{Model}\\) \\(k\\). Let \\(r_k\\) denote the corresponding residual degrees of freedom for \\(\\texttt{Model}\\) \\(k\\) (the number of observations minus the number of model parameters).\nConsider the following hypotheses:\n\\[\nH_0: \\texttt{Model } 0 \\text{ is true}; \\hspace{5mm} H_1: \\texttt{Model } 1 \\text{ is true}.\n\\]\nUnder the null hypothesis \\(H_0\\), the difference between \\(R_0\\) and \\(R_1\\) will be purely random, so the between-model mean-square \\((R_0 - R_1)/(r_0 - r_1)\\) should be comparable to the residual mean-square \\(R_1/r_1\\). Thus our test statistic for comparing Model 1 to the simpler Model 0 is:\n\\[\nF_{01} = \\frac{(R_0 - R_1)/(r_0 - r_1)}{R_1/r_1}.\n\\tag{2.2}\\]\nIt can be shown that, under the null hypothesis \\(H_0\\), the statistic \\(F_{01}\\) will have an \\(F\\)-distribution on \\(r_0 - r_1\\) and \\(r_1\\) degrees of freedom, which we write: \\(F_{r_0-r_1, r_1}\\). Under the alternative hypothesis \\(H_1\\), the difference \\(R_0-R_1\\) will tend to be larger than expected under \\(H_0\\), and so the observed value \\(F_{01}\\) will probably lie in the upper tail of the \\(F_{r_0-r_1; r_1}\\) distribution.\nReturning to the birth weight data, we obtain the following output from R:\n\n\nCode\n# read the data from file into a dataframe called ’birthweight’\nbirthweight = read.table(\"data/birthwt.txt\", header=T)\n\n# fit Model 1\nfit1 = lm(weight ~ age, data=birthweight)\n\n# print the parameter estimates from Model 1\ncoefficients(fit1)\n\n\n(Intercept)         age \n -1484.9846    115.5283 \n\n\nCode\n# perform an analysis of variance of Model 1\nanova(fit1)\n\n\nAnalysis of Variance Table\n\nResponse: weight\n          Df  Sum Sq Mean Sq F value   Pr(>F)    \nage        1 1013799 1013799   27.33 3.04e-05 ***\nResiduals 22  816074   37094                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThus we have parameter estimates: \\(\\hat\\alpha = -1484.98\\) and \\(\\hat\\beta = 115.5\\). The Analysis of Variance (ANOVA) gives: \\(R_0-R_1 = 1013799\\) with \\(r_1-r_1 = 1\\) and \\(R_1 = 816074\\) with \\(r_1 = 22\\).\nIf we wanted \\(R_0\\) and \\(r_0\\) then we can either fit \\(\\texttt{Model 0}\\) or get them by subtraction.\nThe \\(F_{01}\\) statistic, Equation 2.2, is then\n\\[\nF_{01} = \\frac{113799/1}{816074/22} = 27.33,\n\\]\nwhich can read directly from the ANOVA table in the column headed ‘F value’. Is \\(F_{01} = 27.33\\) in the upper tail of the \\(F_{1,22}\\) distribution? (See Figure 2.4 and note that 27.33 is very far to the right.) The final column of the ANOVA table tells us that the probability of observing \\(F_{01} > 27.33\\) is only \\(3.04\\times10^5\\) – this is called a p-value. The *** beside this p-value highlights that its value lies between 0 and 0.001. This indicates that we should reject \\(H_0\\) in favour of \\(H_1\\). Thus we would conclude that the effect of gestational age is statistically significant in these data.\n\n\nCode\ncurve(df(x,1,22), 0,30, \n      ylab=expression(\"PDF of \"*\"F\"[0][1]))\n\n\n\n\n\nFigure 2.4: Probability density function of \\(F_{01}\\) distribution.\n\n\n\n\nNext, consider the following hypotheses:\n\\[\nH_0: \\texttt{Model } 1 \\text{ is true}; \\hspace{5mm} H_1: \\texttt{Model } 2 \\text{ is true}.\n\\]\nUnder the null hypothesis \\(H_0\\), the difference between \\(R_1\\) and \\(R_2\\) will be purely random, so the between-model mean-square \\((R_1 - R_2)/(r_1 - r_2)\\) should be comparable to the residual mean-square \\(R_2/r_2\\). Thus our test statistic for comparing Model 2 to the simpler Model 1 is:\n\\[\nF_{12} = \\frac{(R_1 - R_2)/(r_1 - r_2)}{R_2/r_2}.\n\\tag{2.3}\\]\nUnder the null hypothesis \\(H_0\\), the statistic \\(F_{12}\\) will have an \\(F\\)-distribution on \\(r_1 - r_2\\) and \\(r_2\\) degrees of freedom, which we write: \\(F_{r_1-r_2, r_2}\\). Under the alternative hypothesis \\(H_1\\), the difference \\(R_1-R_2\\) will tend to be larger than expected under \\(H_0\\), and so the observed value \\(F_{12}\\) will probably lie in the upper tail of the \\(F_{r_1-r_2; r_2}\\) distribution.\nReturning to the birth weight data, we obtain the following output from R (where \\(\\texttt{sexM}\\) denotes Boy):\n\n\nCode\n# fit Model 2\nfit2 = lm(weight ~ age + sex, data=birthweight)\n# print the parameter estimates from Model 2\ncoefficients(fit2)\n\n\n(Intercept)         age        sexM \n -1773.3218    120.8943    163.0393 \n\n\nCode\n# perform an analysis of variance on the fitted model\nanova(fit2)\n\n\nAnalysis of Variance Table\n\nResponse: weight\n          Df  Sum Sq Mean Sq F value    Pr(>F)    \nage        1 1013799 1013799 32.3174 1.213e-05 ***\nsex        1  157304  157304  5.0145   0.03609 *  \nResiduals 21  658771   31370                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThus we have parameter estimates: \\(\\hat \\alpha = -1773.3\\), \\(\\hat\\beta = 120.9\\)\nand \\(\\hat\\gamma = 163.0\\), the latter being the effect of being a boy compared to the baseline category of being a girl.\nThe Analysis of Variance (ANOVA) gives: \\(R_1-R_2 = 157304\\) with \\(r_1-r_2=1\\), and \\(R_2=658771\\) with \\(r_2=21\\). The \\(F_{12}\\) statistic, Equation 2.3, is then\n\\[\nF_{12} = \\frac{157304/1}{658771/21} = 5.0145,\n\\]\nwhich can read directly from the ANOVA table in the column headed ‘F value’. Is \\(F_{12} = 5.01\\) in the upper tail of the \\(F_{1,21}\\) distribution? The final column of the ANOVA table tells us that the probability of observing \\(F_{12} > 5.01\\) is only \\(0.03609\\) – this is called a p-value. The * beside this p-value highlights that its value lies between 0.01 and 0.05. This indicates that we should reject \\(H_0\\) in favour of \\(H_1\\). Thus we would conclude that the effect of the sex of the baby, after controlling for gestational age, is statistically significant in these data."
  },
  {
    "objectID": "2_linearmodels.html#types-of-gaussian-linear-model",
    "href": "2_linearmodels.html#types-of-gaussian-linear-model",
    "title": "2  Essentials of Gaussian Linear Models",
    "section": "2.2 Types of Gaussian linear model",
    "text": "2.2 Types of Gaussian linear model\nWe consider how Gaussian linear models can be set up for different types of explanatory variable. The dependent variable \\(y\\) is modelled as a linear combination of \\(p\\) explanatory variables \\(x = (x_j)=(x_1, x_2,\\ldots, x_p)\\) plus a random error \\(\\epsilon \\sim N(0, \\sigma^2)\\), where ‘~’ means ‘is distributed as’. Several models are of this kind, depending on the number and type of explanatory variables. Table 2.2 lists some types of Gaussian linear models with their explanatory variable types.\n\n\nTable 2.2: Types of normal linear model and their explanatory variable types where indicator function \\(I(x=j)=1\\) if \\(x=j\\) and \\(0\\) otherwise.\n\n\n\n\n\n\n\n\\(p\\)\nExplanatory variables\nModel\n\n\n1\nQuantitative\nSimple linear regression\\(y=\\alpha+\\beta x+\\epsilon\\)\n\n\n>1\nQuantitative\nMultiple linear regression\\(y=\\alpha+\\sum_{i=1}^p\\beta_i x_i+\\epsilon\\)\n\n\n1\nDichotomous (\\(x=1\\) or \\(2\\))\nTwo-sample t-test \\(y=\\alpha+\\delta ~ I(x=2)+\\epsilon\\)\n\n\n1\nPolytomous, \\(k\\) levels \\((x=1,\\ldots,k)\\)\nOne-way ANOVA\\(y=\\alpha+\\sum_{j=1}^k \\delta_j \\ I(x=j)+\\epsilon\\)\n\n\n>1\nQualitative\n\\(p\\)-way ANOVA\n\n\n\n\nFor the two-sample t-test model2, observations in the two groups have means \\(\\alpha+\\beta_1\\) and \\(\\alpha + \\beta_2\\) . Notice, however, that we have three parameters with only two group sample means and hence parameter estimation is not possible. To avoid this identification problem, we either impose a ‘corner’ constraint: \\(\\beta_1=0\\) and then \\(\\beta_2\\) represents the difference in the Group 2 mean relative to a baseline of Group \\(1\\). Alternatively, we may impose a ‘sum-to-zero’ constraint: \\(\\beta_1+ \\beta_2 =0\\), the values \\(\\beta_1=-\\beta_2\\) then give differences in the groups means relative to the overall mean. Table 2.3 shows the effect of the parameter constraint on the group means.\n\n\nTable 2.3: Parameters in the two-sample t-test model after imposing parameter constraint to avoid the identification problem.\n\n\nConstraint\nGroup 1 mean\nGroup 2 mean\n\n\n\n\n\\(\\beta_1=0\\)\n\\(\\alpha\\)\n\\(\\alpha+\\beta_2\\)\n\n\n\\(\\beta_1+\\beta_2=0\\)\n\\(\\alpha-\\beta_2\\)\n\\(\\alpha+\\beta_2\\)\n\n\n\n\nFor the general one-way ANOVA model with \\(k\\) groups, observations in Group \\(j\\) have mean \\(\\alpha + \\delta_j\\) , for \\(j =1, \\ldots, k\\) – that leads to \\(k + 1\\) parameters describing \\(k\\) group means. Again we can impose the ‘corner’ constraint: \\(\\delta_1 = 0\\) and then \\(\\delta_j\\) represents the difference in means between Group \\(j\\) and the baseline Group \\(1\\). Alternatively, we may impose a ‘sum-to-zero’ constraint:\\(\\sum_{j=1}^k \\delta_j =0\\) and again \\((\\delta_1, \\delta_2,\\dots,\\delta_k)\\) then represents an individual group effect relative to the overall data mean."
  },
  {
    "objectID": "2_linearmodels.html#matrix-representation-of-linear-models",
    "href": "2_linearmodels.html#matrix-representation-of-linear-models",
    "title": "2  Essentials of Gaussian Linear Models",
    "section": "2.3 Matrix representation of linear models",
    "text": "2.3 Matrix representation of linear models\nAll of the models in Table Table 2.2 can be fitted by least squares (OLS). To describe this, a matrix formulation will be most convenient:\n\\[\n\\mathbf{Y} = X\\boldsymbol{\\beta}+\\boldsymbol{\\epsilon}\n\\tag{2.4}\\]\nwhere\n\n\\(\\mathbf{Y}\\) is an \\(n\\times 1\\) vector of observed response values with \\(n\\) being the number of observations.\n\\(X\\) is an \\(n\\times p\\) design matrix, to be discussed below.\n\\(\\boldsymbol{\\beta}\\) is a \\(p\\times 1\\) vector of parameters or coefficients to be estimated.\n\\(\\boldsymbol{\\epsilon}\\) is an \\(n\\times 1\\) vector of independent and identically distributed (IID) random variables, which here \\(\\epsilon \\sim N(0, \\sigma^2)\\) and is called the “error” term."
  },
  {
    "objectID": "2_linearmodels.html#construction-of-the-design-matrix",
    "href": "2_linearmodels.html#construction-of-the-design-matrix",
    "title": "2  Essentials of Gaussian Linear Models",
    "section": "2.4 Construction of the design matrix",
    "text": "2.4 Construction of the design matrix\nCreating the design matrix is a key part of the modelling as it describes the important structure of investigation or experiment. The design matrix can be constructed by the following process.\n\nBegin with an \\(X\\) containing only one column: a vector of ones for the overall mean or intercept term (the \\(\\alpha\\) in Table 2.2).\nFor each explanatory variable \\(x_j\\), do the following:\n\nIf a variable \\(x_j\\) is quantitative, add a column to \\(X\\) containing the values of \\(x_j.\\)\nIf \\(x_j\\) is qualitative with \\(k\\) levels, add \\(k\\) “dummy” columns to \\(X\\), taking values 0 and 1, where a 1 in the \\(\\ell\\)th dummy column identifies that the corresponding observation is at level \\(\\ell\\) of factor \\(x_j\\) . For example, suppose we have a factor \\(\\mathbf{x}_j = (M, M, F, M, F)\\) representing the sex of \\(n = 5\\) individuals. This information can be coded into two dummy columns of \\(X\\):\n\n\\[\n\\begin{matrix}\n\\begin{matrix}\nF & M\n\\end{matrix}\\\\\n\\begin{bmatrix}\n0 & 1 \\\\\n0 & 1 \\\\\n1 & 0 \\\\\n0 & 1 \\\\\n1 & 0\n\\end{bmatrix}\n\\end{matrix}\n\\]\nWhen qualitative variables are present, \\(X\\) will be singular – that is, there will be linear dependencies between the columns of \\(X\\). For example, the sum of the two columns above is a vector of ones, the same as the intercept column. We resolve this identification problem by deleting some columns of \\(X\\). This is equivalent to applying the corner constraint \\(\\delta_1 = 0\\) in the one-way ANOVA.\nIn the above example, after removing a column, we get:\n\\[\n\\mathbf{X}=\\begin{bmatrix}\n1 &  1 \\\\\n1 &  1 \\\\\n1 &  0 \\\\\n1 &  1 \\\\\n1 &  0\n\\end{bmatrix}.\n\\]\nEach column of \\(X\\) represents either a quantitative variable, or a level of a qualitative variable. We will use \\(i = 1, \\ldots, n\\) to label the observations (rows of \\(X\\)) and \\(j = 1, \\ldots, p\\) to label the columns of \\(X\\).\n\n\n2.4.1 Example: Simple linear regression\nConsider the simple linear regression model \\(y=\\alpha+\\beta x+\\epsilon\\) with \\(\\epsilon \\sim N(0, \\sigma^2)\\). Given data on \\(n\\) pairs \\((x_i, y_i), i = 1, \\ldots, n\\), we write this as\n\\[\ny_i = \\alpha+\\beta x_i+\\epsilon_i, \\quad \\text{for } i=1,2,\\dots,n,\n\\tag{2.5}\\]\nwhere the \\(\\epsilon_i\\) are IID \\(N(0,\\sigma^2)\\). In matrix form, this becomes\n\\[\n\\mathbf{Y}=X\\boldsymbol{\\beta}+\\boldsymbol{\\epsilon}\n\\tag{2.6}\\] with \\[\n\\mathbf{Y}=\\begin{bmatrix}\ny_1\\\\\n\\vdots\\\\\ny_n\n\\end{bmatrix},\n%\n\\hspace{5mm}\n%\nX=\\begin{bmatrix}\n1 & x_1\\\\\n\\vdots & \\vdots\\\\\n1 & x_n\n\\end{bmatrix},\n%\n\\hspace{5mm}\n%\n\\boldsymbol{\\beta}=\n\\begin{bmatrix}\n\\beta_1\\\\\n\\beta_2\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\alpha\\\\\n\\beta\n\\end{bmatrix},\n%\n\\hspace{5mm}\n%\n\\boldsymbol{\\epsilon}=\n\\begin{bmatrix}\n\\epsilon_1\\\\\n\\vdots\\\\\n\\epsilon_n\n\\end{bmatrix}.\n\\] The \\(i\\)th row of Equation 2.6 has the same meaning as Equation 2.5: \\[\ny_i = 1\\times \\beta_1 + x_i\\times \\beta_2 +\\epsilon_i = \\alpha+\\beta x_i +\\epsilon_i,  \\hspace{2mm} \\text{for } i=1,2,\\dots,n.\n\\]\n\n\n2.4.2 Example: One-way ANOVA\nFor one-way ANOVA with \\(k\\) levels, the model is \\[\ny_i =\\alpha+\\sum_{j=1}^k \\delta_j \\ I(x_i=j)+\\epsilon_i, \\quad \\text{for } i=1, 2, \\dots,n,\n\\] where \\(x_i\\) denotes the group level of individual \\(i\\). So if \\(y_i\\) is from the \\(j\\)th group then \\(y_i \\sim N(\\alpha+\\delta_j, \\sigma^2)\\). Here \\(\\alpha\\) is the intercept and the \\((\\delta_1, \\delta_2, \\dots,\\delta_k)\\) represent the “main effects”.\nWe can store the information about the levels of \\(g\\) in a dummy matrix \\(X^* = (x^*_{ij})\\) where\n\\[\nx^*_{ij} = \\left\\{\n\\begin{array}{cl}\n1, & g_i=j,\\\\\n0, & \\text{otherwise.}\n\\end{array}\n\\right.\n\\]\nThen set \\(X = [1, X^*]\\), where \\(1\\) is an \\(n\\)-vector of \\(1\\)’s. For the male–female example at (1.12), we have \\(n = 5\\) and a sex factor:\n\\[\ng=\\begin{bmatrix}1\\\\ 1 \\\\2\\\\1\\\\2\\end{bmatrix},\n%\n\\hspace{5mm}\n%\nX=\\begin{bmatrix}\n1 & 1 & 0\\\\\n1&1& 0 \\\\ 1& 0 & 1\\\\ 1& 1 & 0 \\\\ 1& 0 & 1\n\\end{bmatrix},\n%\n\\hspace{5mm}\n%\n\\beta=\\begin{bmatrix}\\beta_1\\\\\\beta_2\\\\\\beta_3\\end{bmatrix}\n=\\begin{bmatrix}\\alpha\\\\\\delta_1 \\\\\\delta_2\\end{bmatrix}.\n\\]\nThen the \\(i\\)th row of \\(X\\) becomes \\(\\beta_1 + \\beta_2 = \\alpha + \\delta_1\\) if \\(g_i = 1\\) and \\(\\beta_1 + \\beta_3 = \\alpha + \\delta_2\\) if \\(g_i\\) = 2. That is, the \\(i\\)th row of \\(X\\) is\n\\[\n\\alpha+\\sum_{j=1}^2 \\delta_j I(g_i=j)\n\\] so this model can be written \\(Y=X\\beta+\\epsilon\\). Here, \\(X\\) is singular: its last two columns added together equal its first column. Statistically, the problem is that we are trying to estimate two means (the mean response for Boys and the mean response for girls) with three parameters (\\(\\alpha\\), \\(\\delta_2\\) and \\(\\delta_2\\)).\nIn practice, we often resolve this aliasing or identification problem by setting one of the parameters to be zero, that is \\(\\delta_1 = 0\\), which corresponds to deleting the second column of \\(X\\))."
  },
  {
    "objectID": "2_linearmodels.html#model-shorthand-notation",
    "href": "2_linearmodels.html#model-shorthand-notation",
    "title": "2  Essentials of Gaussian Linear Models",
    "section": "2.5 Model shorthand notation",
    "text": "2.5 Model shorthand notation\nIn R, a qualitative (categorical) variable is called a factor, and its categories are called levels. For example, variable \\(\\texttt{Sex}\\) in the birthweight data (above) has levels coded “M” for ‘Boy’ and “F” for ‘Girl’. It may not be obvious to R whether a variable is quantitative or qualitative. For example, a qualitative variable called \\(\\texttt{Grade}\\) might have categories 1, 2 and 3. If was included in a model, R would treat it as quantitative unless we declare it to be a factor, which we can do with the command:\n\\(\\texttt{grade = as.factor(grade)}\\)\nA convenient model-specification notation has been developed from which the design matrix \\(X\\) can be constructed. Below, \\(E, F, \\ldots\\) denote generic quantitative (continuous) or qualitative (categorical) variables. Terms in this notation may take the following forms:\n\n\\(1\\) : a column of 1’s to accommodate an intercept term (the \\(\\alpha\\)’s of Table 2.2 ). This is included in the model by default.\n\\(E\\) : variable \\(E\\) is included in the model. The design matrix includes \\(k_E\\) columns for \\(E\\). If \\(E\\) is quantitative,\\(k_E = 1\\). If E is qualitative, \\(k_E\\) is the number of levels of \\(E\\) minus 1.\n\\(E +F\\) : both \\(E\\) and \\(F\\) are included the model. The design matrix includes \\(k_E +k_F\\) columns accordingly.\n\\(E : F\\) (sometimes \\(E \\cdot F\\)) : the model includes an interaction between \\(E\\) and \\(F\\); each column that would be included for \\(E\\) is multiplied by each column for \\(F\\) in turn. The design matrix includes \\(k_E \\times k_F\\) columns accordingly.\n\\(E * F\\) : shorthand for \\(1 + E + F + E : F\\): useful for crossed models where \\(E\\) and \\(F\\) are different factors. For example, \\(E\\) labels age groups; \\(F\\) labels medical conditions.\n\\(E/F\\) : shorthand for \\(1 + E + E : F\\): useful for nested models where \\(F\\) is a factor whose levels have meaning only within levels of factor \\(E\\). For example, \\(E\\) labels different hospitals; \\(F\\) labels wards within hospitals.\n\\(\\text{poly}(E; \\ell)\\) : shorthand for an orthogonal polynomial, wherein \\(x\\) contains a set of mutually orthogonal columns containing polynomials in \\(E\\) of increasing order, from order \\(1\\) through order \\(\\ell\\).\n\\(-E\\) : shorthand for removing a term from the model; for example \\(E * F -E\\) is short for \\(1 + F + E : F\\).\n\\(I()\\) : shorthand for an arithmetical expression (not to be confused with the indicator function of equation (1.10)). For example, \\(I(E + F)\\) denotes a new quantitative variable constructed by adding together quantitative variables \\(E\\) and \\(F\\). This would cause an error if either \\(E\\) or \\(F\\) has been declared as a factor. What would happen in this example if we omitted the \\(I(\\dot)\\) notation?\n\nThe notation uses “~” as shorthand for “is modelled by” or “is regressed on”. For example,\n\nWeight is regressed on age-group and sex with no interaction between them: \\[\n\\texttt{Weight} \\sim \\texttt{Age} + \\texttt{Sex}\n\\] as for the birthweight data in Figure 1.2c.\nWell being is regressed on age-group and income-group, where income is thought to affect wellbeing differentially by age: \\[\n\\texttt{Wellbeing} \\sim \\texttt{Age} * \\texttt{Income}\n\\]\nClass of degree is regressed on school of the university and on degree subject within the school: \\[\n\\texttt{DegreeClass} \\sim \\texttt{School/Subject}\n\\]\nYield of wheat is regressed on seed-variety and annual rainfall: \\[\n\\texttt{Yield} \\sim \\texttt{Variety} + \\texttt{poly}(\\texttt{Rainfall}, 2)\n\\]\nProfit is regressed on amount invested: \\[\n\\texttt{Profit}\\sim \\texttt{Investment}- 1\n\\] (no intercept term, that is a regression through the origin).\n\nSee Handout 4 for material on intrinsic aliasing to deal with singularity problem."
  },
  {
    "objectID": "3_GLM-Theory.html#the-glm-structure",
    "href": "3_GLM-Theory.html#the-glm-structure",
    "title": "3  GLM Theory",
    "section": "3.2 The GLM structure",
    "text": "3.2 The GLM structure\nA generalized linear model relates a continuous or discrete response variable \\(y\\) to a set of explanatory variables \\(x=(x_1, \\ldots, x_p)\\). The model contains three parts:\nRandom part: The probability function of \\(y\\) is assumed to belong to the two-parameter exponential family of distributions with parameters \\(\\theta\\) and \\(\\phi\\):\n\\[\nf(y; \\theta, \\phi) = \\exp \\left\\{ \\frac{y \\theta - b(\\theta)}\n                  {\\phi} + c(y, \\phi) \\right\\},\n\\tag{3.1}\\] where \\(\\phi>0\\). Here, \\(\\theta\\) is called the canonical or natural parameter of the distribution and \\(\\phi\\) is called the scale parameter. We show below that the mean \\(\\mbox{E}[y]\\) depends only on \\(\\theta\\), and \\(\\mbox{Var}[y]\\) depends on \\(\\phi\\) and possibly also \\(\\theta\\). Various choices for functions \\(b(\\cdot)\\) and \\(c(\\cdot)\\) produce a wide variety of familiar distributions (see below). Sometimes we may set \\(\\phi=1\\); then Equation 3.1 is called the one-parameter exponential family.\nFurther, note that in some references to generalized linear models (such as Dobson and Barnett, 3rd edn.), \\(\\phi\\) does not appear at all in the exponential family formula Equation 3.1, instead it is absorbed into \\(\\theta\\) and \\(b(\\theta)\\).\nIn this module, we will generally assume that each observation \\(y_i\\), \\(i=1,\\dots,n\\), is independently drawn from an exponential family where \\(\\theta\\) depends on the covariates for each unit of observation \\(i\\). Thus we write \\[\nf(y_i; \\theta_i, \\phi) = \\exp \\left\\{ \\frac{y_i \\theta_i - b(\\theta_i)}   {\\phi} + c(y_i, \\phi) \\right\\}.\n\\] Note the subscripts on both \\(y\\) and \\(\\theta\\).\nSystematic part: This is a linear predictor: \\[\n\\eta = \\sum_{j=1}^p \\beta_j x_j.\n\\tag{3.2}\\]\nLink function: This is an isomorphic function providing the link between the linear predictor \\(\\eta\\) and the mean \\(\\mu = \\mbox{E}[y]\\):\n\\[\n\\eta = g(\\mu), \\quad \\mbox{and} \\quad \\mu  = g^{-1}(\\eta) = h(\\eta).\n\\tag{3.3}\\]\nHere, \\(g(\\mu)\\) is called the link function, and \\(h(\\eta)\\) is called the inverse link function.\nWe will now discuss each of these parts in more detail."
  },
  {
    "objectID": "3_GLM-Theory.html#the-random-part-of-a-glm",
    "href": "3_GLM-Theory.html#the-random-part-of-a-glm",
    "title": "3  GLM Theory",
    "section": "3.3 The random part of a GLM",
    "text": "3.3 The random part of a GLM\nWe begin with some examples of exponential family members.\n\n3.3.1 Example: Poisson distribution\nIf \\(y\\) has a Poisson distribution with parameter \\(\\lambda\\), \\(y \\sim \\text{Po}(\\lambda)\\), then \\(y\\) takes values in \\(\\{0,1,2,\\dots\\}\\) and has probability mass function: \\[\nf(y) = \\frac{e^{-\\lambda} \\lambda^y} {y!}  \n     = \\exp \\left\\{y \\log \\lambda - \\lambda - \\log y! \\right\\},\n\\tag{3.4}\\] which has the form of Equation 3.1 with\n\n\n\n\\(\\theta\\)\n\\(\\phi\\)\n\\(b(\\theta)\\)\n\\(c(y,\\phi)\\)\n\n\n\n\n\\(\\log\\lambda\\)\n\\(1\\)\n\\(\\lambda=e^\\theta\\)\n\\(-\\log y!\\)\n\n\n\nFor example, to model the cyclones data in Table 3.1, we might simply assume that the number of cyclones in each season has a Poisson distribution, assuming a constant rate \\(\\lambda\\) across all seasons \\(i\\). That is \\(y_i \\sim \\text{Po}(\\lambda).\\)\n\n\n3.3.2 Example: Binomial distribution\nLet \\(y\\) have a Binomial distribution, (write \\(y \\sim \\text{Bin}(m, p)\\) with \\(m\\) fixed. Then \\(y\\) is discrete, taking values in \\(\\{0,1,\\dots,m\\}\\), and has probability mass function: \\[\\begin{align}\nf(y) &= {m \\choose y} p^y (1 - p)^{m - y} = {m \\choose y} \\left(\\frac{p}{1-p} \\right)^y (1 - p)^m \\\\\n     &= \\exp \\left\\{ y\\ \\mbox{logit } p  + m \\log (1-p) + \\log {m \\choose y}\\right\\},\n\\end{align}\\] which has the form of with\n\n\n\n\\(\\theta\\)\n\\(\\phi\\)\n\\(b(\\theta)\\)\n\\(c(y,\\phi)\\)\n\n\n\n\n\\(\\mbox{logit }p\\)\n\\(1\\)\n\\(m\\log(1+e^\\theta)\\)\n\\(\\log{m\\choose y}\\)\n\n\n\nWhere it can be shown that \\(-m\\log(1-p)=m\\log(1+e^\\theta)\\) – see Exercises.\n\n\n3.3.3 Example: Normal distribution\nLet \\(y\\) have a Normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Then \\(y\\) takes values on the whole real line and has probability density function\n\\[\\begin{align*}\nf(y; \\mu, \\sigma^2) &= \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left\\{ \\frac{-1}{2\\sigma^2} (y - \\mu)^2 \\right\\}, \\notag \\\\\n                    &= \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left\\{-\\frac{y^2}{2 \\sigma^2} + \\frac{y\\mu}{\\sigma^2} - \\frac{\\mu^2}{2 \\sigma^2}\\right\\}\\\\\n                    &= \\exp \\left\\{ \\frac{y \\mu - \\mu^2/2}{\\sigma^2} + \\left[\\frac{-y^2}{2\\sigma^2} - \\frac{1}{2} \\log (2 \\pi \\sigma^2)\n                              \\right]\\right\\},\n\\end{align*}\\] which has the form of Equation 3.1 with\n\n\n\n\n\n\n\n\n\n\\(\\theta\\)\n\\(\\phi\\)\n\\(b(\\theta)\\)\n\\(c(y,\\phi)\\)\n\n\n\n\n\\(\\mu\\)\n\\(\\sigma^2\\)\n\\(\\theta^2/2\\)\n\\(-\\frac{y^2}{2\\phi} - \\frac{1}{2} \\log (2 \\pi \\phi)\\)\n\n\n\nWhere it can be shown that \\(\\frac{-y^2}{2\\sigma^2} - \\frac{1}{2} \\log (2 \\pi \\sigma^2)=-\\frac{y^2}{2\\phi} - \\frac{1}{2} \\log (2 \\pi \\phi)\\) – see Exercises.\nFrom the usual regression point of view, we write \\(y = \\alpha + \\beta x + \\epsilon\\), with \\(\\epsilon \\sim N(0, \\sigma^2)\\). From the point of view of a generalized linear model, we write \\(y \\sim N(\\mu, \\sigma^2)\\) where \\(\\mu(x) = \\alpha + \\beta x\\)."
  },
  {
    "objectID": "3_GLM-Theory.html#moments-of-exponential-family-distributions",
    "href": "3_GLM-Theory.html#moments-of-exponential-family-distributions",
    "title": "3  GLM Theory",
    "section": "3.4 Moments of exponential-family distributions",
    "text": "3.4 Moments of exponential-family distributions\nIt is straightforward to find the mean and variance of \\(y\\) in terms of \\(b(\\theta)\\) and \\(\\phi\\). Since we want to explore the dependence of \\(\\mbox{E}[y]\\) on explanatory variables, this property makes the exponential family very convenient.\nProposition For random ariables in the exponential family: \\[\n\\mbox{E}[y] = b'(\\theta), \\quad \\mbox{and } \\quad \\mbox{Var}[y] =  b''(\\theta)\\phi.\n\\tag{3.5}\\]\nProof We give the proof for continuous \\(y\\). For discrete \\(y\\), replace all integrals by sums. We have \\[\\begin{align}\n  1 &= \\int f(y; \\theta) dy.\n\\\\ \\mathclap{\\text{Differentiating both sides w.r.t.\\ $\\theta$ gives:}}  &  \\\\\n0 &= \\frac{d}{d \\theta} \\int f(y; \\theta)\\ dy \\nonumber \\\\\n  &= \\int \\frac{d}{d \\theta} \\exp \\left\\{ \\frac{y \\theta - b(\\theta)}{\\phi} + c(y,\\phi)\\right\\}\\ dy \\nonumber \\\\\n  &= \\int \\left[\\frac{ y - b'(\\theta)}{\\phi} \\right] f(y; \\theta)\\ dy\n\\\\ \\mathrlap{\\mbox{using  again,}} & \\\\\n  &= \\frac{1}{\\phi} \\left(\\int y f(y; \\theta) dy - b'(\\theta) \\int f(y;\\theta)\\ dy \\right)\\nonumber \\\\\n  &= \\frac{1}{\\phi} \\left(E(y) - b'(\\theta)\\right) \\nonumber \\\\\n\\mbox{E}[y] &= b'(\\theta),\\notag\n\\\\ \\rlap{\\mbox{which proves. Differentiating by parts yields:}}\\\\\n0 &= \\int \\left\\{ -\\frac{b''(\\theta)}{\\phi} + \\left[\\frac{ y - b'(\\theta)}{\\phi} \\right]^2 \\right\\} f(y; \\theta)\\ dy \\notag \\\\\n  &=  -\\frac{b''(\\theta)}{\\phi} +\\int \\left[\\frac{ y - \\mbox{E}[y]}{\\phi} \\right]^2  f(y; \\theta)\\ dy \\notag \\\\\n  &= -\\frac{b''(\\theta)}{\\phi} + \\frac{\\mbox{Var}[y]}{\\phi^2} \\notag \\\\\n\\mbox{Var}[y] &= \\phi b''(\\theta).\n\\end{align}\\]\n\n\nTable 3.2: Summary of moment calculations via exponential family peoperties\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\theta\\)\n\\(b(\\theta)\\)\n\\(\\phi\\)\n\\(\\mbox{E}[y]=b'(\\theta)\\)\n\\(b''(\\theta)\\)\n\\(\\mbox{Var}[y]=b''(\\theta)\\phi\\)\n\n\n\n\nPoisson, \\(Po(\\lambda)\\)\n\\(\\log \\lambda\\)\n\\(e^\\theta\\)\n\\(1\\)\n\\(e^\\theta=\\lambda\\)\n\\(e^\\theta\\)\n\\(e^\\theta\\times 1=\\lambda\\)\n\n\nNormal, \\(N(\\mu,\\sigma^2)\\)\n\\(\\mu\\)\n\\(\\theta^2/2\\)\n\\(\\sigma^2\\)\n\\(\\theta=\\mu\\)\n\\(1\\)\n\\(1\\times \\sigma^2=\\sigma^2\\)"
  },
  {
    "objectID": "3_GLM-Theory.html#the-systematic-part-of-the-model",
    "href": "3_GLM-Theory.html#the-systematic-part-of-the-model",
    "title": "3  GLM Theory",
    "section": "3.5 The systematic part of the model",
    "text": "3.5 The systematic part of the model\nThe second part of the generalized linear model, the linear predictor, is given in as \\(\\eta = \\sum_{j=1}^p \\beta_j x_j\\), where \\(x_j\\) is the \\(j\\)th explanatory variable (with \\(x_1=1\\) for the intercept). Now, for each observation \\(y_i,\\ i=1,\\dots,n\\), the explanatory variables may differ. To make explicit this dependence on \\(i\\), we write: \\[\n\\eta_i = \\sum_{j=1}^p \\beta_j x_{ij},\n\\] where \\(x_{ij}\\) is the value of the \\(j\\)th explanatory variable on individual \\(i\\) (with \\(x_{i1}=1\\)). Rewriting this in matrix notation: \\[\n\\eta = X \\beta,\n\\] where now \\(\\boldsymbol{\\eta} = (\\eta_1,\\dots,\\eta_n)\\) is a vector of linear predictor variables, \\(\\boldsymbol{\\beta} = (\\beta_1,\\dots,\\beta_p)\\) is a vector of regression parameters, and \\(X\\) is an \\(n\\times p\\) design matrix.\nRecall from that we are concerned with two kinds of explanatory variable:\nQuantitative — e.g. \\(x_j \\in (-\\infty, \\infty)\\) etc.\nQualitative — e.g. \\(x_j \\in \\{A, B, C\\}\\) etc.\nAs discussed in , each quantitative variable is represented in \\(X\\) by an \\(n \\times 1\\) column vector. Each qualitative variable, with \\(k+1\\) levels, say, is represented by a dummy \\(n \\times k\\) matrix of 0’s and 1’s (one column, usually the first, being dropped to avoid identification problems)."
  },
  {
    "objectID": "3_GLM-Theory.html#the-link-function",
    "href": "3_GLM-Theory.html#the-link-function",
    "title": "3  GLM Theory",
    "section": "3.6 The link function",
    "text": "3.6 The link function\nOn we saw that the contribution of randomness to an observation \\(y\\) might be described with a member of the exponential family. We also saw that the systematic part of \\(y\\) might be described using a linear predictor \\(\\eta\\) of the explanatory variables. In we introduced the notion of a link function \\(\\eta = g(\\mu)\\) to link these two parts together, where \\(\\mu\\) is the mean of \\(y\\).\nRarely, the choice of link function \\(g(\\mu)\\) is motivated by theory underlying the data at hand. For example, in a dose–response setting, the appropriate model could possibly be motivated by the solution to a set of partial differential equations describing the flow through the body of a dose of a drug.\nWhen there is no compelling underlying substantive theory, we typically choose a link function that will transform a restricted range of the dependent variable onto the whole real line. For example, when observations are measurements they are typically positive, so we have \\(\\mu>0\\) and might choose the logarithmic link: \\[\ng(\\mu) = \\log(\\mu).\n\\]When observations are binomial counts from \\(B(m,p), \\ 0 < p < 1\\), with mean \\(\\mu = mp\\), we might choose the logit link from\n\\[\\begin{align}\n\\eta &= g(\\mu) = \\text{logit}(\\mu/m)= \\text{logit}(p) = \\log\\{p/(1-p)\\}\n\\intertext{ or the {\\it probit} link which is the inverse of the cdf of the N(0,1)~distribution:}\n\\eta &= g(\\mu) = \\Phi^{-1}(\\mu/m) = \\Phi^{-1}(p), \\label{eq:probit.link}\n\\intertext{or the {\\it complementary log-log (cloglog)} link:}\n\\eta &= g(\\mu) = \\log(-\\log(1-\\mu/m))= \\log(-\\log(1-p)), \\label{eq:cloglog.link}\n\\intertext{or the {\\it cauchit} link which is the inverse of the cdf of the Cauchy ($t_1$) distribution:}\n\\eta &= g(\\mu) = \\tan(\\pi(\\mu/m-\\tfrac{1}{2}))\n               = \\tan(\\pi(p-\\tfrac{1}{2})). \\label{eq:cauchit.link}\n\\end{align}\\] shows these link functions for proportions fitted to the beetle mortality data. This demonstrates that the logit and probit links are very similar, that the complementary log-log link fits these data slightly better in the extremes, but that the cauchit link fits these data quite poorly in the extremes."
  },
  {
    "objectID": "3_GLM-Theory.html#the-canonical-link",
    "href": "3_GLM-Theory.html#the-canonical-link",
    "title": "3  GLM Theory",
    "section": "3.7 The canonical link",
    "text": "3.7 The canonical link"
  },
  {
    "objectID": "4_GLM-Fitting.html#deviance",
    "href": "4_GLM-Fitting.html#deviance",
    "title": "4  GLM fitting",
    "section": "4.2 Deviance",
    "text": "4.2 Deviance"
  },
  {
    "objectID": "4_GLM-Fitting.html#residuals",
    "href": "4_GLM-Fitting.html#residuals",
    "title": "4  GLM fitting",
    "section": "4.3 Residuals",
    "text": "4.3 Residuals"
  },
  {
    "objectID": "4_GLM-Fitting.html#fitting-generalized-linear-models-in-r",
    "href": "4_GLM-Fitting.html#fitting-generalized-linear-models-in-r",
    "title": "4  GLM fitting",
    "section": "4.4 Fitting generalized linear models in R",
    "text": "4.4 Fitting generalized linear models in R"
  },
  {
    "objectID": "5_logisticmodel.html",
    "href": "5_logisticmodel.html",
    "title": "5  Logistic regression model",
    "section": "",
    "text": "temporary"
  },
  {
    "objectID": "5_logisticmodel.html#application-doseresponse-experiments",
    "href": "5_logisticmodel.html#application-doseresponse-experiments",
    "title": "5  Logistic regression model",
    "section": "5.2 Application: dose–response experiments",
    "text": "5.2 Application: dose–response experiments"
  },
  {
    "objectID": "5_logisticmodel.html#residuals-and-deviance",
    "href": "5_logisticmodel.html#residuals-and-deviance",
    "title": "5  Logistic regression model",
    "section": "5.3 Residuals and deviance",
    "text": "5.3 Residuals and deviance"
  },
  {
    "objectID": "6_loglinearmodel.html#general-log-linear-model",
    "href": "6_loglinearmodel.html#general-log-linear-model",
    "title": "6  Loglinear models",
    "section": "6.2 General log-linear model",
    "text": "6.2 General log-linear model"
  },
  {
    "objectID": "6_loglinearmodel.html#margins-of-a-contingency-table",
    "href": "6_loglinearmodel.html#margins-of-a-contingency-table",
    "title": "6  Loglinear models",
    "section": "6.3 Margins of a contingency table",
    "text": "6.3 Margins of a contingency table"
  },
  {
    "objectID": "6_loglinearmodel.html#conditioning-on-marginal-totals",
    "href": "6_loglinearmodel.html#conditioning-on-marginal-totals",
    "title": "6  Loglinear models",
    "section": "6.4 Conditioning on marginal totals",
    "text": "6.4 Conditioning on marginal totals"
  },
  {
    "objectID": "6_loglinearmodel.html#maximum-likelihood-estimates",
    "href": "6_loglinearmodel.html#maximum-likelihood-estimates",
    "title": "6  Loglinear models",
    "section": "6.5 Maximum likelihood estimates",
    "text": "6.5 Maximum likelihood estimates"
  },
  {
    "objectID": "6_loglinearmodel.html#analysis-of-melanoma-data",
    "href": "6_loglinearmodel.html#analysis-of-melanoma-data",
    "title": "6  Loglinear models",
    "section": "6.6 Analysis of Melanoma data",
    "text": "6.6 Analysis of Melanoma data"
  },
  {
    "objectID": "7_extendedloglinearmodel.html#multi-way-contingency-tables",
    "href": "7_extendedloglinearmodel.html#multi-way-contingency-tables",
    "title": "7  Extensions to Loglinear models",
    "section": "7.2 Multi-way contingency tables",
    "text": "7.2 Multi-way contingency tables"
  },
  {
    "objectID": "7_extendedloglinearmodel.html#product-multinomial-models",
    "href": "7_extendedloglinearmodel.html#product-multinomial-models",
    "title": "7  Extensions to Loglinear models",
    "section": "7.3 Product-multinomial models",
    "text": "7.3 Product-multinomial models"
  },
  {
    "objectID": "revision.html",
    "href": "revision.html",
    "title": "9  Appendix: Revision of vectors and matrices",
    "section": "",
    "text": "Note that in these notes we use lowercase \\(y\\) or \\(y_i\\) to denote both observed values or random variables, which should be clear from the context.\nSimilarly, although all vectors are column vectors, for ease of writing, we may write simply as a horizontal list. Again, the meaning will be clear from context.\nExamples of scalars: \\[\nx, y, \\alpha, \\beta, \\gamma, \\delta, \\epsilon\n\\hspace{5mm} \\text{or} \\hspace{5mm}\nx_i, y_i, \\alpha_i, \\beta_j, \\gamma_j, \\delta_j, \\epsilon_i\n\\] Examples of \\(n\\times 1\\) vectors: \\[\n\\mathbf{y}=(y_i)=(y_1, y_2, \\ldots, y_n),\n\\mathbf{Y}=(Y_i)=(Y_1, Y_2, \\ldots, Y_n),\n\\] \\[\n\\boldsymbol{\\epsilon} = (\\epsilon_i)=(\\epsilon_1, \\epsilon_2,\\ldots, \\epsilon_n)\n\\] or as: \\[\n\\mathbf{y}=(y_i)=\n\\begin{bmatrix} y_1\\\\ y_2\\\\ \\vdots \\\\ y_n \\end{bmatrix},\n\\mathbf{Y}=(Y_i)=\n\\begin{bmatrix} Y_1\\\\ Y_2\\\\ \\vdots \\\\ Y_n \\end{bmatrix},\n\\boldsymbol{\\epsilon} = (\\epsilon_i)=\n\\begin{bmatrix} \\epsilon_1\\\\ \\epsilon_2\\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}\n\\]\nExamples of \\(p\\times 1\\) vectors: \\[\n\\mathbf{x}=(x_j) = (x_1, x_2, \\ldots, x_p),\n\\mathbf{X}=(X_j) = (X_1, X_2, \\ldots, X_p),\n\\] \\[\n\\boldsymbol{\\beta}=(\\beta_j)=(\\beta_1, \\beta_2, \\ldots, \\beta_p)\n\\] or as \\[\n\\mathbf{x}=(x_j) =\n\\begin{bmatrix}x_1\\\\ x_2\\\\ \\vdots\\\\ x_p\\end{bmatrix},\n\\mathbf{X}=(X_j) = \\begin{bmatrix}X_1\\\\ X_2\\\\ \\vdots\\\\ X_p\\end{bmatrix},\n\\boldsymbol{\\beta}=\\begin{bmatrix}\\beta_1\\\\ \\beta_2\\\\ \\vdots\\\\ \\beta_p\\end{bmatrix},\n\\]\nExamples of \\(n\\times p\\) matrices:\n\\[\nX= (X_{ij})=\n\\begin{pmatrix}\nx_{11} & x_{12}& \\ \\ldots & x_{1p} \\\\\nx_{21} & x_{22}& \\ \\ldots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{n1} & x_{n2}& \\ \\ldots & x_{np} \\\\\n\\end{pmatrix}\n\\]"
  },
  {
    "objectID": "revision.html#operations",
    "href": "revision.html#operations",
    "title": "9  Appendix: Revision of vectors and matrices",
    "section": "9.2 Operations",
    "text": "9.2 Operations\nExamples of addition:\n\\[\n\\alpha + \\boldsymbol{\\beta} =\n\\begin{bmatrix}\n\\alpha +\\beta_1 \\\\\n\\alpha +\\beta_2 \\\\\n\\vdots \\\\\n\\alpha +\\beta_p \\\\\n\\end{bmatrix}\n\\hspace{5mm}\n\\mathbf{x} +\\boldsymbol{\\beta} =\n\\begin{bmatrix}\nx_1 +\\beta_1 \\\\\nx_2 +\\beta_2 \\\\\n\\vdots \\\\\nx_p +\\beta_p \\\\\n\\end{bmatrix}\n\\]\nExamples of multiplication:\n\\[\n\\alpha \\boldsymbol{\\beta} =\n\\alpha \\times \\boldsymbol{\\beta} =\n\\alpha \\cdot \\boldsymbol{\\beta} =\n\\begin{bmatrix}\n\\alpha \\beta_1 \\\\\n\\alpha \\beta_2 \\\\\n\\vdots \\\\\n\\alpha \\beta_p \\\\\n\\end{bmatrix}\n\\hspace{5mm}\n\\mathbf{x} \\cdot \\boldsymbol{\\beta} =\n\\mathbf{x}^T \\boldsymbol{\\beta} =\nx_1\\beta_1 +\nx_2\\beta_2 +\n\\dots +\nx_p\\beta_p\n\\] \\[\nX \\boldsymbol{\\beta} =\n\\begin{bmatrix}\nx_{11}\\beta_1 + x_{12}\\beta_2+ \\dots + x_{1p}\\beta_p \\\\\nx_{21}\\beta_1 + x_{22}\\beta_2+ \\dots + x_{2p}\\beta_p \\\\\n\\vdots \\\\\nx_{n1}\\beta_1 + x_{n2}\\beta_2+ \\dots + x_{np}\\beta_p \\\\\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "revision.html#special-vectors-and-matrices",
    "href": "revision.html#special-vectors-and-matrices",
    "title": "9  Appendix: Revision of vectors and matrices",
    "section": "9.3 Special vectors and matrices",
    "text": "9.3 Special vectors and matrices\nVector of zero’s or one’s: \\[\n\\mathbf{0} = (0,0,\\dots,0),\n\\hspace{5mm}\n\\mathbf{1} = (1,1,\\dots,1)\n\\]"
  },
  {
    "objectID": "formula-sheet.html#basic-rules-in-probability-statistics",
    "href": "formula-sheet.html#basic-rules-in-probability-statistics",
    "title": "10  Appendix: Standard distributions",
    "section": "10.2 Basic rules in probability & statistics",
    "text": "10.2 Basic rules in probability & statistics\n\nRules of expectation Expectation is linear so for random variables \\(X_1\\) and \\(X_2\\), and constants \\(a\\), \\(b\\) and \\(c\\), the \\[\n\\mbox{E}[aX_1+bX_2+c] = a\\mbox{E}[X_1]+b\\mbox{E}[X_2] +c.\n\\] There is no change to this rule when \\(X_1\\) and \\(X_2\\) are independent.\nRules of variance By definition, the \\(\\mbox{Var}[X]\\) is given by \\[\n\\mbox{Var}[X] = \\mbox{E}[(X-\\mbox{E}[X])^2] = \\mbox{E}[X]^2-\\mbox{E}[X^2]\n\\] with the variance of the sum of two random variables \\[\n\\mbox{Var}[X_1+X_2] = \\mbox{Var}[X_1]+\\mbox{Var}[X_2]-2\\mbox{Cov}[X_1,X_2]\n\\] where \\(\\mbox{Cov}[X_1,X_2]\\) is the covariance which is defined in the following section. If, however, \\(X_1\\) and \\(X_2\\) are independent, then the above reduces to \\[\n\\mbox{Var}[X_1+X_2] = \\mbox{Var}[X_1]+\\mbox{Var}[X_2].\n\\]\nDefinition of covariance and correlation By definition, the \\(\\mbox{Cov}[X_1,X_2]\\) is given by \\[\n\\mbox{Cov}[X_1,X_2] = \\mbox{E}[(X_1-\\mbox{E}[X_1])(X_1-\\mbox{E}[X_1])]\n= \\mbox{E}[X_1 X_2]-\\mbox{E}[X_1] \\mbox{E}[X_2]\n\\] and \\[\n\\mbox{Cov}[X_1,X_2] = \\frac{\\mbox{Cov}[X_1,X_2]}{\\sqrt{\\mbox{Var}[X_1]\\mbox{Var}[X_2]}}.\n\\] In the case when \\(X_1\\) and \\(X_2\\) are independent random variables then \\(\\mbox{E}[X_1 X_2]=\\mbox{E}[X_1] \\mbox{E}[X_2]\\) and so \\(\\mbox{Cov}[X_1,X_2]=0\\) and hence \\(\\mbox{Cov}[X_1,X_2]=0\\) also. This gives justification of the variance formula for the sum of independent random variables.\nJoint distribution for independent events. Suppose that \\(X_1\\) and \\(X_2\\) are a pair of independent on identically distributed random variables then their joint probability function is given by \\[\nf_{X_1,X_2}(x_1,x_2) = f_{X_1}(x_1)  f_{X_2}(x_2).\n\\]"
  },
  {
    "objectID": "formula-sheet.html#standard-distributions",
    "href": "formula-sheet.html#standard-distributions",
    "title": "10  Appendix: Standard distributions",
    "section": "10.3 Standard distributions",
    "text": "10.3 Standard distributions\nHere, \\(f(x;\\theta) \\equiv f(x| \\theta)\\) is written to denote a probability function if \\(X\\) is discrete, or a probability density function if \\(X\\) is continuous, where \\(\\theta\\) is the parameter of the distribution.\n\nBernoulli, with parameter \\(\\theta\\) (\\(0 < \\theta < 1\\)). A discrete random variable \\(X\\) with probability function \\[\nf(x;\\theta) ~~=~~ \\theta^x (1 - \\theta)^{1-x}  \\quad \\qquad x = 0, 1\n\\]\n\\(\\mbox{E}[X] ~=~ \\theta~\\) and \\(\\mbox{Var}[X] = \\theta (1 - \\theta)\\).\nGeometric, with parameter \\(\\theta\\) (\\(0 < \\theta < 1\\)). A discrete random variable $X$ with probability function \\[\nf(x;\\theta) = \\theta (1 - \\theta)^{x-1} \\quad \\qquad x = 1, 2, \\dots\n\\]\n\\(\\mbox{E}[X] = 1/\\theta\\) and \\(\\mbox{Var}[X] = (1 - \\theta)/\\theta^2\\).\nBinomial, with parameters \\(n\\) and \\(\\theta\\) (where \\(n\\) is a known positive integer and \\(0 < \\theta < 1\\)). A discrete random variable \\(X\\) with probability function \\[\nf(x;n,\\theta) = {n \\choose x} \\theta^x (1 - \\theta)^{n-x}\n\\quad \\qquad x = 0, 1, \\dots, n\n\\]\n\\(\\mbox{E}[X] = n \\theta\\) and \\(\\mbox{Var}[X] = n \\theta(1 - \\theta)\\).\nPoisson, with parameter \\(\\theta\\) (\\(\\theta >0\\)). A discrete random variable \\(X\\) with probability function \\[\nf(x;\\theta) ~~=~~ \\frac{\\theta^x e^{- \\theta}}{x!}  \\quad \\qquad x = 0, 1, \\dots\n\\]\n\\(\\mbox{E}[X] ~=~ \\theta~\\) and \\(~\\mbox{Var}[X] ~=~ \\theta\\).\nNegative Binomial, with parameters \\(r\\) and \\(\\theta\\) (\\(r > 0\\) and \\(0 < \\theta < 1\\)). A discrete random variable \\(X\\) with probability function \\[\nf(x;r,~\\theta) ~~=~~ {{x-1} \\choose {r-1}} \\theta^r (1 - \\theta)^{x-r}  \\quad \\qquad x = r, r+1, \\dots\n\\]\n\\(\\displaystyle \\mbox{E}[X] ~=~ \\frac{r}{\\theta}~\\) and \\(~\\displaystyle \\mbox{Var}[X] ~=~ \\frac{r(1 - \\theta)}{\\theta^2}\\).\nUniform, with parameter \\(\\theta\\) \\((\\theta > 0)\\). A continuous random variable \\(X\\), with probability density function \\[\nf(x;\\theta) ~~=~~ \\frac{1}{\\theta}  \\quad \\qquad 0 < x < \\theta\n\\]\n\\(\\displaystyle \\mbox{E}[X] ~=~ \\frac{\\theta}{2}~\\) and \\(~\\displaystyle \\mbox{Var}[X] ~=~ \\frac{\\theta^2}{12}\\).\nExponential, with parameter \\(\\lambda\\) \\((\\lambda > 0)\\).\nA continuous random variable \\(X\\), with probability density function \\[\nf(x;\\lambda) ~~=~~ \\lambda e^{- \\lambda x}  \\quad \\qquad x > 0\n\\]\n\\(\\displaystyle \\mbox{E}[X] ~=~ \\frac{1}{\\lambda}~\\) and \\(~\\displaystyle \\mbox{Var}[X] ~=~ \\frac{1}{\\lambda^2}\\).\nGamma with parameters \\(\\alpha\\) and \\(\\beta\\) \\((\\alpha, \\beta > 0)\\). A continuous random variable \\(X\\), with probability density function \\[\nf(x;\\alpha, \\beta) ~~=~~ \\frac{\\beta^\\alpha x^{\\alpha-1} e^{-\\beta x}}{\\Gamma(\\alpha)}\n\\quad \\qquad x > 0\n\\]\n\\(\\displaystyle \\mbox{E}[X] ~=~ \\frac{\\alpha}{\\beta}~\\) and \\(~\\displaystyle \\mbox{Var}[X] ~=~ \\frac{\\alpha}{\\beta^2}\\). \nNote: \\(\\Gamma(\\alpha+1) ~=~ \\alpha \\Gamma(\\alpha) ~~ \\forall \\alpha\\) and \\(\\Gamma(\\alpha+1) ~=~ \\alpha!\\) for integers \\(\\alpha>1\\).\nBEWARE some authors replace \\(\\beta\\) by \\(1/\\beta\\) but still calling it Gamma\\((\\alpha, \\beta)\\). You always need to be clear which parameterization is being used.\nBeta with parameters \\(\\alpha\\) and \\(\\beta\\) \\((\\alpha, \\beta > 0)\\). A continuous random variable \\(X\\), with probability density function \\[\nf(x;\\alpha, \\beta) ~~=~~ \\frac{x^{\\alpha-1} (1 - x)^{\\beta-1}}{B(\\alpha, \\beta)}\n\\quad \\qquad 0< x < 1\n\\] where \\[\nB(\\alpha, \\beta) ~~=~~ \\int_0^1 x^{\\alpha-1} (1-x)^{\\beta-1} dx\n          ~~=~~ \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}\n\\]\n\\(\\displaystyle \\mbox{E}[X] ~=~ \\frac{\\alpha}{\\alpha+\\beta}~\\) and \\(~\\displaystyle \\mbox{Var}[X] ~=~ \\frac{\\alpha \\beta}{(\\alpha+\\beta)^2 (\\alpha+\\beta+1)}\\).\nAlso referred to as the Beta\\((\\alpha, \\beta)\\) distribution.\nNormal with parameters \\(\\mu\\) and \\(\\sigma^2\\) \\((-\\infty < \\mu < \\infty\\) and \\(\\sigma^2 > 0)\\). A continuous random variable \\(X\\), with probability density function \\[\nf(x;\\mu, \\sigma^2) ~~=~~ \\frac{1}{\\sqrt{2\\pi}~\\sigma}~\\exp \\left\\{ -\\, \\frac{1}{2 \\sigma^2}\n(x - \\mu)^2 \\right\\}  \\quad \\qquad -\\infty < x < \\infty\n\\] \\(\\mbox{E}[X] ~=~ \\mu~\\) and \\(~\\mbox{Var}[X] ~=~ \\sigma^2\\).\nAlso referred to as the N\\((\\mu, \\sigma^2)\\) distribution.\nPareto with parameters \\(\\theta\\) and \\(\\alpha\\) \\((\\theta, \\alpha > 0)\\). A continuous random variable \\(X\\), with probability density function \\[\nf(x;\\theta, \\alpha) ~~=~~ \\frac{\\alpha \\theta^\\alpha}{x^{\\alpha+1}}\n\\quad \\qquad x > \\theta\n\\]\n\\(\\displaystyle \\mbox{E}[X] ~=~ \\frac{\\alpha \\theta}{(\\alpha-1)}~\\) and \\(~\\displaystyle \\mbox{Var}[X] ~=~ \\frac{\\alpha \\theta^2}{(\\alpha-1)^2 (\\alpha-2)} ~~~ (\\alpha>2)\\).\nChi-Square with parameter \\(n\\) (\\(n\\) is a positive integer). A continuous random variable \\(X\\), with probability density function \\[\nf(x;n) ~~=~~ \\left( \\frac{1}{2} \\right)^{\\frac{n}{2}} \\frac{x^{\\frac{n}{2}-1}\ne^{-\\frac{x}{2}}}{\\Gamma(\\frac{n}{2})} \\quad \\qquad x > 0\n\\]\n\\(\\mbox{E}[X] ~=~ n~\\) and \\(~\\mbox{Var}[X] ~=~ 2n\\).\nAlso referred to as the \\(\\chi^2_n\\) distribution, with \\(n\\) degrees of freedom.\nNote: A \\(\\chi^2_n\\) distribution is also a Gamma\\((\\frac{n}{2},\\frac{1}{2})\\) distribution.\nStudent’s t with parameter \\(n\\) (\\(n\\) is a positive integer). A continuous random variable \\(X\\), with probability density function \\[\nf(x;n) ~~=~~ \\frac{\\Gamma(\\frac{n+1}{2})}{\\sqrt{n \\pi} ~~ \\Gamma(\\frac{n}{2})\n\\left[1+\\frac{x^2}{n}\\right]^{\\frac{n+1}{2}}} \\quad \\qquad -\\infty < x < \\infty\n\\]\n\\(\\mbox{E}[X] ~=~ 0 ~~ (n>1)~\\) and \\(~\\displaystyle \\mbox{Var}[X] ~=~ \\frac{n}{n-2} ~~ (n>2)\\).\nAlso referred to as the \\(t\\) distribution, with \\(n\\) degrees of freedom.\nF with parameters \\(m\\) and \\(n\\) (\\(m,n\\) are positive integers). A continuous random variable \\(X\\), with probability density function \\[\nf(x;m,n) ~~=~~ \\left( \\frac{m}{n} \\right)^{\\frac{m}{2}} \\frac{\\Gamma(\\frac{m+n}{2})\n~ x^{\\frac{m}{2}-1}}{\\Gamma(\\frac{m}{2})\\Gamma(\\frac{n}{2})\n~ \\left[1+\\frac{mx}{n}\\right]^{\\frac{m+n}{2}}} \\quad \\qquad x > 0\n\\]\n\\(\\displaystyle \\mbox{E}[X] ~=~ \\frac{n}{n-2} ~~ (n>2)~\\) and \\(~\\displaystyle \\mbox{Var}[X] ~=~ \\frac{2n^2 (m+n-2)}{m(n-2)^2 (n-4)} ~~ (n>4).\\)\nAlso referred to as the \\(F\\) distribution, with \\(m\\) and \\(n\\) degrees of freedom.\nNote: The order of \\(m\\) and \\(n\\) is important."
  },
  {
    "objectID": "2_linearmodels.html#exercises",
    "href": "2_linearmodels.html#exercises",
    "title": "2  Essentials of Gaussian Linear Models",
    "section": "2.6 Exercises",
    "text": "2.6 Exercises\n\nAn exta model which could have been considered for the Birthweight data example would be one that say that Weight is different for girls and boys, but does not depend on gestational age.\nWrite down the equation corresponding to this model. Then, load the birthweight data into RStudio and fit the model. How are the fitted model parameters related to the overall birthweight mean and the mean birthweights of the girls and boys? Is this a good fit to the data? Is Sex statistically significant?\nConsider a new data set…"
  },
  {
    "objectID": "3_GLM-Theory.html",
    "href": "3_GLM-Theory.html",
    "title": "3  GLM Theory",
    "section": "",
    "text": "We cannot always assume that the dependent variable \\(y\\) is normally distributed. For example, for the beetle mortality data in Table 1.1, suppose each beetle subjected to a dose \\(x_i\\) has a probability \\(p_i\\) of being killed. Then the number of beetles killed \\(y_i\\) out of a total number \\(m_i\\) at dose-level \\(x_i\\) will have a \\(\\text{Bin}(m_i,p_i)\\) distribution:\n\\[\n\\text{Pr}(y_i ;~ p_i,m_i) = \\left(\\begin{array}{c} m_i\\\\ y_i \\end{array} \\right) p_i^{y_i} (1-p_i)^{m_i-y_i}\n\\] where \\(y_i\\) takes values in \\(\\{0,1,\\dots,m_i\\}\\).\nTable 3.1 contains seasonal data on tropical cyclones for 13 seasons. Suppose that, within season \\(i\\), there is a constant probability \\(\\lambda_i dt\\) of a cyclone occurring in any short time-interval \\(dt\\). Then the total number of cyclones \\(y_i\\) during season \\(i\\) will have a Poisson distribution with mean \\(\\lambda_i\\), that is \\(y_i\\sim \\text{Po}(\\lambda_i)\\):\n\\[\n\\text{Pr}(y_i ;~ \\lambda_i) = \\frac{\\lambda_i^{y_i} e^{-\\lambda_i} }{y_i!}\n\\] where \\(y_i\\) takes values in \\(\\{0,1,2,\\dots\\}\\).\n\n\nTable 3.1: Numbers of tropical cyclones in \\(n = 13\\) successive seasons1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeason\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\nNo of cyclones\n6\n5\n4\n6\n6\n3\n12\n7\n4\n2\n6\n7\n4\n\n\n\n\nIn these two examples, we have non-normal data and would like to know whether and how the dependent variable \\(y_i\\) depends on the covariate \\(x_i\\) or \\(i\\).\nGeneralized linear models provide a modelling framework for data analysis in the non-normal setting. We will revisit the beetle mortality and cyclone data sets after describing the structure of a generalized linear model."
  },
  {
    "objectID": "3_GLM-Theory.html#exercises",
    "href": "3_GLM-Theory.html#exercises",
    "title": "3  GLM Theory",
    "section": "3.8 Exercises",
    "text": "3.8 Exercises\n\nIn the binomial distribution, show that \\(-m\\log(1-p)=m\\log(1+e^\\theta)\\) where \\(\\theta=\\mbox{logit }p\\).\nUse the results in Section 3.4 and the exponential family description of the Binomial distribution in Section 3.3.2 to show that the mean and variance of a \\(\\text{Bin}(m,p)\\) are \\(mp\\) and \\(mp(1-p)\\).\nHint: \\(f'(g(x)) = f'(g(x)) g'(x)\\)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH3823 Generalized Linear Models",
    "section": "",
    "text": "These lecture notes are based on those used previously for this module and I am grateful to Lapeng Ji, Amanda Minter, John Kent, Wally Gilks, and Stuart Barber."
  },
  {
    "objectID": "3_GLM-Theory.html#sec-exponential-family",
    "href": "3_GLM-Theory.html#sec-exponential-family",
    "title": "3  GLM Theory",
    "section": "3.4 Moments of exponential-family distributions",
    "text": "3.4 Moments of exponential-family distributions\nIt is straightforward to find the mean and variance of \\(y\\) in terms of \\(b(\\theta)\\) and \\(\\phi\\). Since we want to explore the dependence of \\(\\mbox{E}[y]\\) on explanatory variables, this property makes the exponential family very convenient.\nProposition For random variables in the exponential family: \\[\n\\mbox{E}[y] = b'(\\theta), \\quad \\mbox{and } \\quad \\mbox{Var}[y] =  b''(\\theta)\\phi.\n\\tag{3.5}\\]\n\\text{Differentiating both sides w.r.t. \\(\\theta\\) gives\nProof We give the proof for continuous \\(y\\). For discrete \\(y\\), replace all integrals by sums. We have \\[\\begin{align}\n  1 &= \\int f(y; \\theta) dy  \\\\\n0 &= \\frac{d}{d \\theta} \\int f(y; \\theta)\\ dy \\nonumber \\\\\n  &= \\int \\frac{d}{d \\theta} \\exp \\left\\{ \\frac{y \\theta - b(\\theta)}{\\phi} + c(y,\\phi)\\right\\}\\ dy \\nonumber \\\\\n  &= \\int \\left[\\frac{ y - b'(\\theta)}{\\phi} \\right] f(y; \\theta)\\ dy\n\\\\ \\mathrlap{\\mbox{using  again,}} & \\\\\n  &= \\frac{1}{\\phi} \\left(\\int y f(y; \\theta) dy - b'(\\theta) \\int f(y;\\theta)\\ dy \\right)\\nonumber \\\\\n  &= \\frac{1}{\\phi} \\left(E(y) - b'(\\theta)\\right) \\nonumber \\\\\n\\mbox{E}[y] &= b'(\\theta),\\notag\n\\\\ \\rlap{\\mbox{which proves. Differentiating by parts yields:}}\\\\\n0 &= \\int \\left\\{ -\\frac{b''(\\theta)}{\\phi} + \\left[\\frac{ y - b'(\\theta)}{\\phi} \\right]^2 \\right\\} f(y; \\theta)\\ dy \\notag \\\\\n  &=  -\\frac{b''(\\theta)}{\\phi} +\\int \\left[\\frac{ y - \\mbox{E}[y]}{\\phi} \\right]^2  f(y; \\theta)\\ dy \\notag \\\\\n  &= -\\frac{b''(\\theta)}{\\phi} + \\frac{\\mbox{Var}[y]}{\\phi^2} \\notag \\\\\n\\mbox{Var}[y] &= \\phi b''(\\theta).\n\\end{align}\\]\n\n\nTable 3.2: Summary of moment calculations via exponential family peoperties\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\theta\\)\n\\(b(\\theta)\\)\n\\(\\phi\\)\n\\(\\mbox{E}[y]=b'(\\theta)\\)\n\\(b''(\\theta)\\)\n\\(\\mbox{Var}[y]=b''(\\theta)\\phi\\)\n\n\n\n\nPoisson, \\(Po(\\lambda)\\)\n\\(\\log \\lambda\\)\n\\(e^\\theta\\)\n\\(1\\)\n\\(e^\\theta=\\lambda\\)\n\\(e^\\theta\\)\n\\(e^\\theta\\times 1=\\lambda\\)\n\n\nNormal, \\(N(\\mu,\\sigma^2)\\)\n\\(\\mu\\)\n\\(\\theta^2/2\\)\n\\(\\sigma^2\\)\n\\(\\theta=\\mu\\)\n\\(1\\)\n\\(1\\times \\sigma^2=\\sigma^2\\)"
  },
  {
    "objectID": "3_GLM-Theory.html#the-canonical-links",
    "href": "3_GLM-Theory.html#the-canonical-links",
    "title": "3  GLM Theory",
    "section": "3.7 The canonical links",
    "text": "3.7 The canonical links\nA mathematically and computationally convenient choice of link function \\(g(\\mu)\\) can be constructed by setting: \\[\n\\theta =\\eta,\n\\] where \\(\\theta\\) is the canonical parameter of the exponential family~\\(\\eqref{eq:exponential.family}\\). Equation~\\(\\eqref{eq:exponential.family.E}\\) on page~ shows that the mean \\(\\mu\\) is a function of \\(\\theta\\). Therefore, \\(\\eqref{eq:canonical.linkfun.theta}\\) indirectly provides a link between \\(\\mu\\) and \\(\\eta\\). That is, \\(\\eqref{eq:canonical.linkfun.theta}\\) implicitly defines a link function \\(\\eta=g(\\mu)\\). What is the form of this \\(g(\\cdot)\\)?\nFrom~\\(\\eqref{eq:exponential.family.E}\\), \\[\\begin{align}\n    \\mu &= b'(\\theta). \\notag\n    \\shortintertext{So, provided function $b'(\\cdot)$ has an inverse $(b')^{-1}(\\cdot)$, we may write}\n    \\theta &= (b')^{-1}(\\mu). \\label{eq:dbinv}\n    \\intertext{Now, from~(\\ref{eq:linkfun}), $g(\\mu) = \\eta$, so using~\\eqref{eq:canonical.linkfun.theta}:}\n    g(\\mu) &= \\theta \\label{eq:canonical.linkfun.theta.2} \\\\\n           &= (b')^{-1}(\\mu), \\label{eq:canonical.linkfun}\n\\end{align}\\] % from \\(\\eqref{eq:dbinv}\\). This makes explicit the \\(g(\\mu)\\) that is implicitly asserted by \\(\\eqref{eq:canonical.linkfun.theta}\\). Equation~\\(\\eqref{eq:canonical.linkfun}\\) is called the {} link function.\n% {} From~(\\(\\ref{eq:exponential.family.E}\\)), \\(\\mu = b'(\\theta)\\), so \\[\\begin{align*}\n\\frac{\\text{d} \\mu }{\\text{d} \\theta} &= b''(\\theta).\n\\intertext{From~(\\ref{eq:canonical.linkfun.theta.2}), for the canonical link function, we have $\\theta = g(\\mu)$, so}\n\\frac{\\text{d} \\theta }{ \\text{d} \\mu} &= g'(\\mu).\n\\intertext{Now $\\tfrac{\\text{d} \\theta }{ \\text{d} \\mu}  = \\left(\\tfrac{\\text{d} \\mu }{\\text{d} \\theta}\\right)^{-1}$. Hence }\ng'(\\mu) &= 1/b''(\\theta).\n\\end{align*}\\] \nFor the Poisson distribution \\(\\text{Po}(\\lambda)\\), we have from~\\(\\eqref{eq:poisson-theta}\\) that \\(b(\\theta) = e^\\theta\\). Therefore, % \\[\\begin{align}\nb'(\\theta) &= e^\\theta, \\notag\n\\intertext{so the inverse of function $b'(\\cdot)$ exists and is the inverse of the exponential function, which is the logarithmic function. Then, applying~\\eqref{eq:canonical.linkfun},}\ng(\\mu) &= \\log(\\mu) .\\label{eq:canonical.linkfun.poisson}\n\\end{align}\\] % Thus the canonical link for the Poisson distribution is \\(\\log\\). %\nFor the Normal distribution \\(N(\\mu, \\sigma^2)\\), we have from~\\(\\eqref{eq:normal-theta}\\) that \\(b(\\theta) = \\theta^2/2\\). Therefore % \\[\\begin{align}\nb'(\\theta) &= \\theta, \\notag\n\\intertext{so the inverse of function $b'(\\cdot)$ exists and is the inverse of the identity function, which is the identity function. (The identity function is that which maps a value onto itself.) Then, applying~\\eqref{eq:canonical.linkfun},\n}\ng(\\mu) &= \\mu. \\label{eq:canonical.linkfun.normal}\n\\end{align}\\] % Thus the canonical link for the Normal distribution is the identity function. %\nFor many models, \\(\\mu\\) has a restricted range, but we would like \\(\\eta\\) to have unlimited range. It turns out, for several members of the exponential family, that the canonical link function provides \\(\\eta\\) with unlimited range. However, Table~\\(\\ref{tab:canonical.range}\\) shows that this is not always so.\nWhy is the canonical link function~\\(\\eqref{eq:canonical.linkfun}\\) convenient? The assertion~\\(\\eqref{eq:canonical.linkfun.theta}\\) means that, in the exponential-family formula~\\(\\eqref{eq:exponential.family}\\), we can simply substitute the linear predictor \\(\\eta=\\sum_j \\beta_j x_j\\) from (\\(\\ref{eq:linpred}\\)) in place of \\(\\theta\\), to give: % \\[\\begin{equation} \\label{eq:canonical.linkfun.f}\n    f(y; \\{x_{j}\\}, \\{\\beta_j\\},\\phi) = \\exp \\left\\{ \\frac{y \\left[\\sum_j \\beta_j x_j\\right] - b\\left(\\left[\\sum_j \\beta_j x_j\\right]\\right)}\n                                            {\\phi} + c(y, \\phi) \\right\\},\n\\end{equation}\\] % where \\(\\{x_{j}\\}=\\{x_{j}, j=1,\\dots,p\\}\\) and \\(\\{\\beta_j\\}=\\{\\beta_j, j=1,\\dots,p\\}\\). Suppose we have \\(n\\) independent observations, \\(\\{y_i,\\ i=1,\\ldots,n\\}\\). As discussed in Section~\\(\\ref{sect:systematic}\\), the explanatory variables \\((x_{1},\\ldots,x_{p})\\) will depend on~\\(i\\), and so \\(\\eta\\) will also depend on~\\(i\\). Therefore, we attach subscript \\(i\\) to \\(y\\) and to each \\(x_j\\), giving: % \\[\\begin{align}\n    f(y_i; \\{x_{ij}\\}, \\{\\beta_j\\}, \\phi) &= \\exp \\left\\{ \\frac{y_i \\left[\\sum_j \\beta_j x_{ij}\\right] - b\\left(\\left[\\sum_j \\beta_j x_{ij}\\right]\\right)}\n                                            {\\phi} + c(y_i, \\phi) \\right\\}. \\label{eq:canonical.linkfun.f2}\n%\n\\intertext{By independence, the joint distribution of all observations $\\{y_i\\} = \\{y_i,\\ i=1,\\dots,n\\}$ is:}\n%\nf(\\{y_i\\}; \\{x_{ij}\\}, \\{\\beta_j\\},\\phi) &= \\prod_{i=1}^n f(y_i; \\theta_i, \\phi), \\notag\n%\n\\intertext{so\\vspace{-3mm}}\n%\n\\log f(\\{y_i\\}; \\{x_{ij}\\}, \\{\\beta_j\\},\\phi)\n                                   &= \\sum_{i=1}^n \\log f(y_i; \\theta_i, \\phi) \\notag \\\\\n                                   &= \\sum_{i=1}^n   \\frac{y_i \\left[\\sum_j \\beta_j x_{ij}\\right] - b\\left(\\left[\\sum_j \\beta_j x_{ij}\\right]\\right)}\n                                            {\\phi} + c(y_i, \\phi)  \\notag \\\\\n                                   &= \\frac{\\sum_j \\beta_j S_j - \\sum_i b\\left(\\left[\\sum_j \\beta_j x_{ij}\\right]\\right)}\n                                            {\\phi} + \\sum_i c(y_i, \\phi) , \\label{eq:canonical.linkfun.loglik}\n\\intertext{where\\vspace{-3mm}}\n%\nS_j &= \\sum_{i=1}^n  y_i  x_{ij}.\n\\end{align}\\] % Thus, in the log-likelihood~\\(\\eqref{eq:canonical.linkfun.loglik}\\), it is only the first term that involves both the observations \\(\\{y_i,\\ i=1,\\dots,n\\}\\) and the parameters \\(\\{\\beta_j\\}\\), and this term depends on the observations only through the statistics \\(\\{S_j\\}\\). These are called {}, and their appearance in~\\(\\eqref{eq:canonical.linkfun.loglik}\\) confers both theoretical and practical advantages.\n%% %\\begin{align} %_i &= b’(_i), \\[0mm] %_i &= g(_i) \\[-6mm] % %x_i /&= _i.\\[-6mm] % %x_i /&= g(_i) = g(b’(i))* .\\[-5mm] % %x_i /&= i.* %\\end{align} %% %Thus, from~(\\(\\ref{eq:ltheta}\\)), we have the log-likelihood of the \\(n\\) observations:%% %\\begin{align} %l(/; /y) %& = %^n / a_i() + \\ %& = %^n / a_i() + , %\\end{align} %% %where \\(x_i\\) denotes the vector \\((x_{i1},\\ldots,x_{ir})\\), \\(\\beta\\) denotes a column vector of regression parameters of length \\(r\\) and ``\\(\\mbox{const}\\)” denotes a function which does not depend on \\(\\beta\\). %%This is the log-likelihood of the %% %\n% %The canonical link function is such that \\(\\theta=g(\\mu)\\) in equation~\\(\\eqref{eq:exponential.family}\\). Now, we have seen in \\(\\eqref{eq:exponential.family.E}\\) that \\(\\mu = b'(\\theta)\\). Therefore, for the canonical link, %% %\\begin{align} %&= g() \\ % &= g(b’()) % %h() &= b’() % %&= (b’)^{-1}h(). %\\end{align} % % %has the property that its inverse is defined as the function that is the first derivative w.r.t.~\\(\\theta\\) of \\(b(\\theta)\\) in the definition of the exponential family~\\(\\eqref{eq:exponential.family}\\): %% %\\begin{align} %h() &= (b’)(). % %g() &= (b’)^{-1}() . % %&= g() \\ % &= (b’)^{-1}() \\ % &= (b’)^{-1}(b’()) \\[-5mm] % %&= g() = . %\\end{align} %\n%\\begin{figure}[htb] % % the following figures were generated in R from directory C:/Users/wally/Documents/Leeds_University/MATH3823-5824/WallyGilksFiles/MATH3823/Rcode: % % % % % %"
  }
]