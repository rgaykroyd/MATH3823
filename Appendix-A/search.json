[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH3823 Generalized Linear Models",
    "section": "",
    "text": "Appendix A: Basic material\nThis Appendix contains a brief summary of ideas which are considered pre-requisite. Although you should be familiar with the ideas and definitions, the notation and layout may be slightly different to that you are used to. If this is the case then please take a little time to get accustomed to the style below, as it will be used through this module. At the University of Leeds, most ideas are covered by the pair of first-year modules, MATH1710 and MATH1712, with a few ideas from MATH2715 in second year. If, however, you have not seen this material previously then please consult a standard introductory text, such as\nPlease note that this is the first time these notes have been used. If you see errors or you think of some important points which are not covered, then please let me know."
  },
  {
    "objectID": "index.html#a.1-notation",
    "href": "index.html#a.1-notation",
    "title": "MATH3823 Generalized Linear Models",
    "section": "A.1 Notation",
    "text": "A.1 Notation\nWhere ever possible, upper case letters from near the end of the alphabet will be used for random variables, for example \\(X\\) and \\(Y\\), or \\(X_1,\\dots, X_n\\) etc and corresponding lower case letters for particular values of a random variable, for example, \\(x\\) and \\(y\\), or \\(x_1,\\dots, x_n\\). Constants will usually be denoted using lower case letters from the start of the alphabet, for example, \\(a\\), \\(b\\), \\(c\\) etc. Further, lower case letters such as \\(i\\), \\(j\\), \\(n\\), \\(m\\) etc are often reserved for integer constants.\nIn general, parameters of distributions, or of models, will be denoted using lower case Greek letters, such as \\(\\alpha\\), \\(\\beta\\), with a generic parameter often being denote \\(\\theta\\) (as below). A common exception to this convention is the probability in the binomial, geometric etc, where the letter \\(p\\) is often used.\nThe probability mass function of a discrete random variable will be denoted \\(p(\\cdot)\\), for example \\(p(x)\\), or \\(p_X(x)\\) when there are multiple distributions being considered at the same time. In these cases, the probability of the event occurring is being given, for example \\(p_X(x) = Pr(X=x)\\). For a continuous random variable, the probability density function will be denoted \\(f(\\cdot)\\), for example \\(f(x)\\) or \\(f_X(x)\\). For all types of distribution, the cumulative distribution function will be written \\(F(\\cdot)\\), for example \\(F(x)\\) or \\(F_X(x)\\), referring to the probability that the random variable is less than or equal to the specified value, for example \\(F(x)=Pr(X\\le x)\\).\nFurther, where useful, parameters of distributions may be added to this notation, for example as \\(f(x;\\theta)\\). Note that here a semi-colon is used to separate the variable from the parameter. In some areas of statistics, in particular Bayesian methods, similar notation will be used for conditional distributions, such as \\(f(x|\\theta)\\), but in that context \\(\\theta\\) is also being considered a random variable and hence it is a true conditional distribution.\nNote that \\(p\\) is one of the most overused symbols as it might represent the number of variables or parameters in a model, \\(p\\in (1,2,\\dots)\\), the probability in, say, a geometric distribution, \\(\\mbox{Geom}(p)\\) with \\(p\\in (0,1)\\), a discrete distribution, \\(p(x)\\), or a simple event probability, \\(p(A)\\). Which of these is being considered should be clear by context, but it is important not to get confused."
  },
  {
    "objectID": "index.html#a.2-rules-of-probability-statistics",
    "href": "index.html#a.2-rules-of-probability-statistics",
    "title": "MATH3823 Generalized Linear Models",
    "section": "A.2 Rules of probability & statistics",
    "text": "A.2 Rules of probability & statistics\n\nRules of expectation. Expectation is linear so for random variables \\(X_1\\) and \\(X_2\\), and constants \\(a\\), \\(b\\) and \\(c\\), then \\[\n\\mbox{E}[aX_1+bX_2+c] = a\\mbox{E}[X_1]+b\\mbox{E}[X_2] +c.\n\\] There is no change to this rule when \\(X_1\\) and \\(X_2\\) are independent.\nRules of variance. By definition, the \\(\\mbox{Var}[X]\\) is given by \\[\n\\mbox{Var}[X] = \\mbox{E}[(X-\\mbox{E}[X])^2] = \\mbox{E}[X]^2-\\mbox{E}[X^2]\n\\] with the variance of the sum of two random variables \\[\n\\mbox{Var}[X_1+X_2] = \\mbox{Var}[X_1]+\\mbox{Var}[X_2]-2\\mbox{Cov}[X_1,X_2]\n\\] where \\(\\mbox{Cov}[X_1,X_2]\\) is the covariance which is defined in the following section. Further, \\[\n\\mbox{Var}[aX_1+bX_2+c] = a^2\\mbox{Var}[X_1]+b^2\\mbox{Var}[X_2]-2ab\\mbox{Cov}[X_1,X_2].\n\\] If, however, \\(X_1\\) and \\(X_2\\) are independent, then the above reduces to \\[\n\\mbox{Var}[aX_1+bX_2] = a^2\\mbox{Var}[X_1]+b^2\\mbox{Var}[X_2].\n\\]\nDefinition of covariance and correlation. By definition, the covariance, \\(\\mbox{Cov}[X_1,X_2]\\), is given by \\[\n\\mbox{Cov}[X_1,X_2] = \\mbox{E}[(X_1-\\mbox{E}[X_1])(X_1-\\mbox{E}[X_1])]\n\\]\nwhich has the alternate form \\[\n\\mbox{Cov}[X_1,X_2] = \\mbox{E}[X_1 X_2]-\\mbox{E}[X_1] \\mbox{E}[X_2]\n\\] and also the correlation \\[\n\\mbox{Cor}[X_1,X_2] = \\frac{\\mbox{Cov}[X_1,X_2]}{\\sqrt{\\mbox{Var}[X_1]\\mbox{Var}[X_2]}}.\n\\] In the case when \\(X_1\\) and \\(X_2\\) are independent random variables then \\(\\mbox{E}[X_1 X_2]=\\mbox{E}[X_1] \\mbox{E}[X_2]\\) hence \\(\\mbox{Cov}[X_1,X_2]=0\\) and so \\(\\mbox{Cor}[X_1,X_2]=0\\) also. This gives justification for the variance formula for the sum of independent random variables. Note that \\[\n\\mbox{Cov}[aX_1+c,bX_2+d] = ab\\mbox{Cov}[X_1,X_2]\n\\] but that \\[\n\\mbox{Cor}[aX_1+c,bX_2+d] = \\mbox{Cor}[X_1,X_2].\n\\] That is correlation is unaffected by linear trasnformations of the variables.\nJoint distribution for independent events. Suppose that \\(X_1\\) and \\(X_2\\) are a pair of independent continuous random variables then their joint probability density function is given by \\[\nf_{X_1,X_2}(x_1,x_2) = f_{X_1}(x_1)  f_{X_2}(x_2).\n\\] Further, if \\(\\mathbf{X}=\\{X_1,\\dots,X_n\\}\\) are a collection of independent continuous random variables then their joint probability function is given by \\[\nf_{\\mathbf{X}}(\\mathbf{x}) = \\prod_{i=1}^n f_{X_i}(x_i).\n\\] When dealing with discrete random variables, the density functions are simply replaced by the appropriate probability mass functions."
  },
  {
    "objectID": "index.html#a.3-standard-distributions",
    "href": "index.html#a.3-standard-distributions",
    "title": "MATH3823 Generalized Linear Models",
    "section": "A.3 Standard distributions",
    "text": "A.3 Standard distributions\n\nBernoulli, with parameter \\(\\theta\\) (\\(0 &lt; \\theta &lt; 1\\)). A discrete random variable \\(X\\) with probability mass function \\[\np(x;\\theta) ~~=~~ \\theta^x (1 - \\theta)^{1-x}  \\quad \\qquad x = 0, 1\n\\]\n\\(\\mbox{E}[X] ~=~ \\theta~\\) and \\(\\mbox{Var}[X] = \\theta (1 - \\theta)\\).\nGeometric, with parameter \\(\\theta\\) (\\(0 &lt; \\theta &lt; 1\\)). A discrete random variable \\(X\\) with probability mass function \\[\np(x;\\theta) = \\theta (1 - \\theta)^{x-1} \\quad \\qquad x = 1, 2, \\dots\n\\]\n\\(\\mbox{E}[X] = 1/\\theta\\) and \\(\\mbox{Var}[X] = (1 - \\theta)/\\theta^2\\).\nBinomial, with parameters \\(n\\) and \\(\\theta\\) (where \\(n\\) is a known positive integer and \\(0 &lt; \\theta &lt; 1\\)). A discrete random variable \\(X\\) with probability mass function \\[\np(x;n,\\theta) = {n \\choose x} \\theta^x (1 - \\theta)^{n-x}\n\\quad \\qquad x = 0, 1, \\dots, n\n\\]\n\\(\\mbox{E}[X] = n \\theta\\) and \\(\\mbox{Var}[X] = n \\theta(1 - \\theta)\\).\nPoisson, with parameter \\(\\lambda\\) (\\(\\lambda &gt;0\\)). A discrete random variable \\(X\\) with probability mass function \\[\np(x;\\theta) ~~=~~ \\frac{\\lambda^x e^{- \\lambda}}{x!}  \\quad \\qquad x = 0, 1, \\dots\n\\]\n\\(\\mbox{E}[X] ~=~ \\lambda~\\) and \\(~\\mbox{Var}[X] ~=~ \\lambda\\).\nNegative Binomial, with parameters \\(r\\) and \\(\\theta\\) \\((r &gt; 0\\) and \\(0 &lt; \\theta &lt; 1).\\) A discrete random variable \\(X\\) with probability mass function \\[\np(x;r,~\\theta) ~~=~~ {{x-1} \\choose {r-1}} \\theta^r (1 - \\theta)^{x-r}  \\quad \\qquad x = r, r+1, \\dots\n\\]\n\\(\\displaystyle \\mbox{E}[X] ~=~ \\frac{r}{\\theta}~\\) and \\(~\\displaystyle \\mbox{Var}[X] ~=~ \\frac{r(1 - \\theta)}{\\theta^2}\\).\nUniform, with parameter \\(\\theta\\) \\((\\theta &gt; 0)\\). A continuous random variable \\(X\\), with probability density function \\[\nf(x;\\theta) ~~=~~ \\frac{1}{\\theta}  \\quad \\qquad 0 &lt; x &lt; \\theta\n\\]\n\\(\\displaystyle \\mbox{E}[X] ~=~ \\frac{\\theta}{2}~\\) and \\(~\\displaystyle \\mbox{Var}[X] ~=~ \\frac{\\theta^2}{12}\\).\nExponential, with parameter \\(\\lambda\\) \\((\\lambda &gt; 0)\\). A continuous random variable \\(X\\), with probability density function \\[\nf(x;\\lambda) ~~=~~ \\lambda e^{- \\lambda x}  \\quad \\qquad x &gt; 0\n\\]\n\\(\\displaystyle \\mbox{E}[X] ~=~ \\frac{1}{\\lambda}~\\) and \\(~\\displaystyle \\mbox{Var}[X] ~=~ \\frac{1}{\\lambda^2}\\).\nGamma with parameters \\(\\alpha\\) and \\(\\beta\\) \\((\\alpha, \\beta &gt; 0)\\). A continuous random variable \\(X\\), with probability density function \\[\nf(x;\\alpha, \\beta) ~~=~~ \\frac{\\beta^\\alpha x^{\\alpha-1} e^{-\\beta x}}{\\Gamma(\\alpha)}\n\\quad \\qquad x &gt; 0\n\\] \\(\\displaystyle \\mbox{E}[X] = \\alpha/\\beta\\) and \\(~\\displaystyle \\mbox{Var}[X] ~=~ \\alpha/\\beta^2\\). Note: \\(\\Gamma(\\alpha+1) ~=~ \\alpha \\Gamma(\\alpha)\\) for all \\(\\alpha\\) and \\(\\Gamma(\\alpha+1) ~=~ \\alpha!\\) for integers \\(\\alpha&gt;1.\\) BEWARE some authors replace \\(\\beta\\) by \\(1/\\beta\\) but still calling it Gamma\\((\\alpha, \\beta)\\). You always need to be clear which parameterization is being used.\nBeta with parameters \\(\\alpha\\) and \\(\\beta\\) \\((\\alpha, \\beta &gt; 0)\\). A continuous random variable \\(X\\), with probability density function \\[\nf(x;\\alpha, \\beta) ~~=~~ \\frac{x^{\\alpha-1} (1 - x)^{\\beta-1}}{B(\\alpha, \\beta)}\n\\quad \\qquad 0&lt; x &lt; 1\n\\] where \\[\nB(\\alpha, \\beta) ~~=~~ \\int_0^1 x^{\\alpha-1} (1-x)^{\\beta-1} dx\n          ~~=~~ \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}\n\\]\n\\(\\displaystyle \\mbox{E}[X] ~=~ \\frac{\\alpha}{\\alpha+\\beta}~\\) and \\(~\\displaystyle \\mbox{Var}[X] ~=~ \\frac{\\alpha \\beta}{(\\alpha+\\beta)^2 (\\alpha+\\beta+1)}\\).\nAlso referred to as the Beta\\((\\alpha, \\beta)\\) distribution.\nNormal with parameters \\(\\mu\\) and \\(\\sigma^2\\) \\((-\\infty &lt; \\mu &lt; \\infty\\) and \\(\\sigma^2 &gt; 0)\\). A continuous random variable \\(X\\), with probability density function \\[\nf(x;\\mu, \\sigma^2) ~~=~~ \\frac{1}{\\sqrt{2\\pi}~\\sigma}~\\exp \\left\\{ -\\, \\frac{1}{2 \\sigma^2}\n(x - \\mu)^2 \\right\\}  \\quad  -\\infty &lt; x &lt; \\infty\n\\] \\(\\mbox{E}[X] ~=~ \\mu~\\) and \\(~\\mbox{Var}[X] ~=~ \\sigma^2\\).\nAlso referred to as the N\\((\\mu, \\sigma^2)\\) distribution.\nPareto with parameters \\(\\theta\\) and \\(\\alpha\\) \\((\\theta, \\alpha &gt; 0)\\). A continuous random variable \\(X\\), with probability density function \\[\nf(x;\\theta, \\alpha) ~~=~~ \\frac{\\alpha \\theta^\\alpha}{x^{\\alpha+1}}\n\\quad \\qquad x &gt; \\theta\n\\] \\(\\displaystyle \\mbox{E}[X] ~=~ \\frac{\\alpha \\theta}{(\\alpha-1)}~\\) and \\(~\\displaystyle \\mbox{Var}[X] ~=~ \\frac{\\alpha \\theta^2}{(\\alpha-1)^2 (\\alpha-2)} ~~~ (\\alpha&gt;2)\\).\nChi-Square with parameter \\(n\\) (\\(n\\) is a positive integer). A continuous random variable \\(X\\), with probability density function \\[\nf(x;n) ~~=~~ \\left( \\frac{1}{2} \\right)^{\\frac{n}{2}} \\frac{x^{\\frac{n}{2}-1}\ne^{-\\frac{x}{2}}}{\\Gamma(\\frac{n}{2})} \\quad \\qquad x &gt; 0\n\\]\n\\(\\mbox{E}[X] ~=~ n~\\) and \\(~\\mbox{Var}[X] ~=~ 2n\\).\nAlso referred to as the \\(\\chi^2_n\\) distribution, with \\(n\\) degrees of freedom.\nNote: A \\(\\chi^2_n\\) distribution is also a Gamma\\((\\frac{n}{2},\\frac{1}{2})\\) distribution.\nStudentâ€™s t with parameter \\(n\\) (\\(n\\) is a positive integer). A continuous random variable \\(X\\), with probability density function \\[\nf(x;n) ~~=~~ \\frac{\\Gamma(\\frac{n+1}{2})}{\\sqrt{n \\pi} ~~ \\Gamma(\\frac{n}{2})\n\\left[1+\\frac{x^2}{n}\\right]^{\\frac{n+1}{2}}} \\quad \\qquad -\\infty &lt; x &lt; \\infty\n\\]\n\\(\\mbox{E}[X] ~=~ 0 ~~ (n&gt;1)~\\) and \\(~\\displaystyle \\mbox{Var}[X] ~=~ \\frac{n}{n-2} ~~ (n&gt;2)\\).\nAlso referred to as the \\(t\\) distribution, with \\(n\\) degrees of freedom.\nF with parameters \\(m\\) and \\(n\\) (\\(m,n\\) are positive integers). A continuous random variable \\(X\\), with probability density function \\[\nf(x;m,n) ~~=~~ \\left( \\frac{m}{n} \\right)^{\\frac{m}{2}} \\frac{\\Gamma(\\frac{m+n}{2})\n~ x^{\\frac{m}{2}-1}}{\\Gamma(\\frac{m}{2})\\Gamma(\\frac{n}{2})\n~ \\left[1+\\frac{mx}{n}\\right]^{\\frac{m+n}{2}}} \\quad \\qquad x &gt; 0\n\\] \\(\\displaystyle \\mbox{E}[X] ~=~ \\frac{n}{n-2} ~~ (n&gt;2)~\\) and \\(~\\displaystyle \\mbox{Var}[X] ~=~ \\frac{2n^2 (m+n-2)}{m(n-2)^2 (n-4)} ~~ (n&gt;4).\\)\nAlso referred to as the \\(F\\) distribution, with \\(m\\) and \\(n\\) degrees of freedom. Note: The order of \\(m\\) and \\(n\\) is important."
  }
]