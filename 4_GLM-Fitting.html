<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>MATH3823 Generalized Linear Models - 4&nbsp; GLM Estimation</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./3_GLM-Theory.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">GLM Estimation</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">MATH3823 Generalized Linear Models</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Weekly schedule</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./0_preface.html" class="sidebar-item-text sidebar-link">Overview</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./1_intro.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2_linearmodels.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Essentials of Normal Linear Models</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3_GLM-Theory.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">GLM Theory</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./4_GLM-Fitting.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">GLM Estimation</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-mleiid" id="toc-sec-mleiid" class="nav-link active" data-scroll-target="#sec-mleiid"><span class="toc-section-number">4.1</span>  The identically distributed case</a>
  <ul class="collapse">
  <li><a href="#maximum-likelihood-estimation" id="toc-maximum-likelihood-estimation" class="nav-link" data-scroll-target="#maximum-likelihood-estimation"><span class="toc-section-number">4.1.1</span>  Maximum likelihood estimation</a></li>
  <li><a href="#estimation-accuracy" id="toc-estimation-accuracy" class="nav-link" data-scroll-target="#estimation-accuracy"><span class="toc-section-number">4.1.2</span>  Estimation accuracy</a></li>
  </ul></li>
  <li><a href="#sec-mlegeneral" id="toc-sec-mlegeneral" class="nav-link" data-scroll-target="#sec-mlegeneral"><span class="toc-section-number">4.2</span>  The general case</a>
  <ul class="collapse">
  <li><a href="#mle-estimation" id="toc-mle-estimation" class="nav-link" data-scroll-target="#mle-estimation"><span class="toc-section-number">4.2.1</span>  MLE Estimation</a></li>
  <li><a href="#the-score-function-and-fisher-information" id="toc-the-score-function-and-fisher-information" class="nav-link" data-scroll-target="#the-score-function-and-fisher-information"><span class="toc-section-number">4.2.2</span>  The score function and Fisher information</a></li>
  <li><a href="#sec-mlesaturated" id="toc-sec-mlesaturated" class="nav-link" data-scroll-target="#sec-mlesaturated"><span class="toc-section-number">4.2.3</span>  The saturated case</a></li>
  </ul></li>
  <li><a href="#model-deviance" id="toc-model-deviance" class="nav-link" data-scroll-target="#model-deviance"><span class="toc-section-number">4.3</span>  Model deviance</a></li>
  <li><a href="#model-residuals" id="toc-model-residuals" class="nav-link" data-scroll-target="#model-residuals"><span class="toc-section-number">4.4</span>  Model residuals</a></li>
  <li><a href="#fitting-generalized-linear-models-in-r" id="toc-fitting-generalized-linear-models-in-r" class="nav-link" data-scroll-target="#fitting-generalized-linear-models-in-r"><span class="toc-section-number">4.5</span>  Fitting generalized linear models in R</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="toc-section-number">4.6</span>  Exercises</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">GLM Estimation</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<div class="callout-warning callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>Please note that to make these lecture notes available early, there has not been sufficient time to check the following sections. This, in particular, may mean that some of the cross-referencing is not accurate, as well as small typos. Further, the final few sections have not yet available.</p>
</div>
</div>
<p>Throughout this module we use the principle of maximum likelihood estimation (MLE) to estimate model parameters and will consider two cases.</p>
<section id="sec-mleiid" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="sec-mleiid"><span class="header-section-number">4.1</span> The identically distributed case</h2>
<section id="maximum-likelihood-estimation" class="level3" data-number="4.1.1">
<h3 data-number="4.1.1" class="anchored" data-anchor-id="maximum-likelihood-estimation"><span class="header-section-number">4.1.1</span> Maximum likelihood estimation</h3>
<p>Suppose we have <span class="math inline">\(n\)</span> independent and identically distributed (i.i.d.) observations <span class="math inline">\(y_i,\ i=1,\ldots,n\)</span>, where each <span class="math inline">\(y_i\)</span> is sampled from the same exponential family density <span id="eq-iidobs"><span class="math display">\[
f(y_i; \theta,\phi) = \exp \left\{ \frac{y_i\theta  - b(\theta)}{\phi} + c(y_i, \phi) \right\},
\tag{4.1}\]</span></span> for <span class="math inline">\(i=1,\dots,n.\)</span> In this case, the canonical parameter <span class="math inline">\(\theta\)</span> does not depend on <span class="math inline">\(i\)</span>.</p>
<p>By independence, the joint distribution of all the observations <span class="math inline">\(\mathbf{y} = \{y_i,\ i=1,\dots,n\}\)</span> is: <span class="math display">\[
f(\mathbf{y}; \theta,\phi) = \prod_{i=1}^n f(y_i; \theta, \phi).
\]</span> So, taking logs and then substituting for the probability function using the exponential family form, <a href="3_GLM-Theory.html#eq-exponential-family">Equation&nbsp;<span>3.3</span></a>, gives <span class="math display">\[
\log f(\mathbf{y}; \theta,\phi) = \sum_{i=1}^n \log f(y_i; \theta, \phi)
= \sum_{i=1}^n \left[\frac{y_i\theta  - b(\theta)}{\phi} + c(y_i, \phi)\right].
\]</span> Regarding the observations <span class="math inline">\(\mathbf{y}\)</span> as constants (which they are, once we have them) and the scale parameter <span class="math inline">\(\phi\)</span> as a fixed <em>nuisance</em> parameter (whose value we may not know), the log-likelihood as a function of the parameter <span class="math inline">\(\theta\)</span> of interest is: <span id="eq-ltheta"><span class="math display">\[
l(\theta; \mathbf{y},\phi)  = n\left( \frac{\bar{y}\, \theta  -  b(\theta)}{\phi}\right) + \mbox{constant},
\tag{4.2}\]</span></span> where <span class="math inline">\(\bar{y}= \sum y_i/n.\)</span></p>
<p>We estimate <span class="math inline">\(\theta\)</span> by maximizing the log likelihood – i.e.&nbsp;given the data <span class="math inline">\(\mathbf{y}\)</span>, we estimate the value of <span class="math inline">\(\theta\)</span> to be that value for which the likelihood, and hence the log-likelihood, is greatest.</p>
<p>We maximize the log-likelihood by differentiating it and setting it to zero: <span class="math display">\[
\frac{d l(\theta; \mathbf{y},\phi)}{d \theta}  
= n \left(\frac{\bar{y} -  b'(\theta)}{\phi}\right)
\]</span> and hence the MLE for <span class="math inline">\(\theta\)</span>, which we denote <span class="math inline">\(\hat\theta\)</span>, satisfies <span id="eq-exponentialfamilybary"><span class="math display">\[
b'(\hat\theta) = \bar{y}
\tag{4.3}\]</span></span> and hence <span id="eq-exponentialfamilybary2"><span class="math display">\[
\hat\theta =  (b')^{-1}(\bar{y}).
\tag{4.4}\]</span></span></p>
<p>Further, we showed in <a href="3_GLM-Theory.html#prp-moments">Proposition&nbsp;<span>3.1</span></a> that <span class="math inline">\(\mbox{E}[Y] = \mu =b'(\theta)\)</span> and if we let <span class="math inline">\(\hat\mu\)</span> denote the MLE of <span class="math inline">\(\mu\)</span>, then <span class="math inline">\(\hat\mu = b'(\hat{\theta})\)</span><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, hence we have <span class="math inline">\(\hat\mu = \bar{y}\)</span>. So we find that <span class="math inline">\(\hat\theta\)</span> is the value of <span class="math inline">\(\theta\)</span> for which the theoretical mean <span class="math inline">\(\hat\mu = b'(\hat\theta)\)</span> matches the sample mean <span class="math inline">\(\bar{y}\)</span>.</p>
<blockquote class="blockquote">
<p><strong>Example: MLE of the Poisson distribution</strong></p>
<p>For the Poisson distribution, <span class="math inline">\(\text{Po}(\lambda)\)</span>, we have found that <span class="math inline">\(b(\theta) = e^\theta\)</span> and therefore <span class="math inline">\(b'(\theta) = e^\theta\)</span>. Hence, the MLE of natural parameter <span class="math inline">\(\theta\)</span> is found as the solution of <span class="math inline">\(b'(\hat\theta) = e^{\hat\theta} = \bar{y}\)</span>, that is <span class="math inline">\(\hat\theta = \log(\bar{y})\)</span>.</p>
</blockquote>
</section>
<section id="estimation-accuracy" class="level3" data-number="4.1.2">
<h3 data-number="4.1.2" class="anchored" data-anchor-id="estimation-accuracy"><span class="header-section-number">4.1.2</span> Estimation accuracy</h3>
<p>For our i.i.d. sample <span class="math inline">\(y_i,\ i=1,\ldots,n\)</span>, we have <span class="math inline">\(b'(\hat\theta) = \hat\mu = \bar y\)</span>. Let <span class="math inline">\(\theta_0\)</span> be the true value of <span class="math inline">\(\theta\)</span> with corresponding mean <span class="math inline">\(\mu_0\)</span>, i.e.&nbsp;<span id="eq-mu0"><span class="math display">\[
b'(\theta_0) = \mu_0.  
\tag{4.5}\]</span></span> How accurate is <span class="math inline">\(\hat\theta\)</span>? We know that <span id="eq-Ebary"><span class="math display">\[
\mbox{E}[\bar Y] = \mbox{E}\left[\frac{1}{n}\sum_{i=1}^n Y_i\right]
= \frac{1}{n} \sum_{i=1}^n \mbox{E}[Y_i]
= \mu_0  
= b'(\theta_0),
\tag{4.6}\]</span></span> using <a href="#eq-mu0">Equation&nbsp;<span>4.5</span></a>, and <span class="math display">\[
\mbox{Var}[\bar Y] = \mbox{Var}\left[\frac{1}{n} \sum_{i=1}^n Y_i\right]
= \frac{1}{n^2}  \sum_{i=1}^n \mbox{Var}[Y_i]
\]</span> because the observations are independent. Then, using the result <a href="3_GLM-Theory.html#eq-exponential-moments">Equation&nbsp;<span>3.8</span></a> <span id="eq-varbary"><span class="math display">\[
\mbox{Var}[\bar Y]= \frac{1}{n} \ b''(\theta_0) \phi.
\tag{4.7}\]</span></span></p>
<p>We can use Taylor’s theorem to expand <span class="math inline">\(b'(\hat\theta)\)</span> about <span class="math inline">\(\theta_0\)</span>: <span class="math display">\[
\bar y = b'(\hat\theta) \approx  b'(\theta_0) + (\hat\theta - \theta_0) b''(\theta_0),
\]</span> which implies that <span id="eq-hatthetatheta0"><span class="math display">\[
(\hat\theta - \theta_0) \approx  b''(\theta_0)^{-1}\{b'(\hat\theta) - b'(\theta_0)\}  
= b''(\theta_0)^{-1}(\bar Y - \mu_0),
\tag{4.8}\]</span></span> using <a href="#eq-exponentialfamilybary">Equation&nbsp;<span>4.3</span></a> and <a href="#eq-mu0">Equation&nbsp;<span>4.5</span></a>. We can use <a href="#eq-hatthetatheta0">Equation&nbsp;<span>4.8</span></a> to get approximations to the mean and variance of <span class="math inline">\(\hat\theta\)</span>: <span class="math display">\[
\mbox{E}[\hat\theta - \theta_0]  \approx   b''(\theta_0)^{-1} \mbox{E}[\bar Y - \mu_0]
= 0,
\]</span> using <a href="#eq-Ebary">Equation&nbsp;<span>4.6</span></a>, so <span id="eq-Ehattheta"><span class="math display">\[
\mbox{E}[\hat\theta] \approx \theta_0,
\tag{4.9}\]</span></span> and <span class="math display">\[
\mbox{Var}(\hat\theta) \approx \mbox{E}\left[(\hat\theta - \theta_0)^2\right]
\]</span> using <a href="#eq-Ehattheta">Equation&nbsp;<span>4.9</span></a>, <span class="math display">\[
\mbox{Var}(\hat\theta) \approx \mbox{E}\left[\left(b''(\theta_0)^{-1}(\bar Y - \mu_0)\right)^2\right]
\]</span> using <a href="#eq-hatthetatheta0">Equation&nbsp;<span>4.8</span></a>, <span class="math display">\[
\mbox{Var}(\hat\theta)\approx  \left(b''(\theta_0)\right)^{-2} \mbox{Var}[\bar Y]  
\]</span> using <a href="#eq-Ebary">Equation&nbsp;<span>4.6</span></a>, <span id="eq-Varhattheta"><span class="math display">\[
\mbox{Var}(\hat\theta) = \frac{\phi}{n \ b''(\theta_0)}
\tag{4.10}\]</span></span> using <a href="#eq-varbary">Equation&nbsp;<span>4.7</span></a>.</p>
<p>Thus we see that the first two derivatives of <span class="math inline">\(b(\theta)\)</span> play a key role in inference.</p>
<blockquote class="blockquote">
<p><strong>Example: Accuracy for the Poisson distribution</strong></p>
<p>For the Poisson distribution, <span class="math inline">\(\text{Po}(\lambda)\)</span>, we have found that <span class="math inline">\(\hat\theta = \log(\bar{Y})\)</span>. Using <a href="#eq-Ehattheta">Equation&nbsp;<span>4.9</span></a> we know that <span class="math inline">\(\hat\theta\)</span> is, at least, approximately unbiased. Then, using that <span class="math inline">\(\phi=1\)</span> (<a href="3_GLM-Theory.html#tbl-GLM-poisson">Table&nbsp;<span>3.2</span></a>), <span class="math inline">\(b'(\theta)=e^{\theta}\)</span> (<a href="3_GLM-Theory.html#tbl-canonicalrange">Table&nbsp;<span>3.6</span></a>) hence <span class="math inline">\(b''(\theta)=e^{\theta}\)</span>, and using <a href="#eq-Varhattheta">Equation&nbsp;<span>4.10</span></a>, leads to the result <span class="math display">\[
\mbox{Var}(\hat\theta) = \frac{\phi}{n \ b''(\theta_0)}
=
\frac{1}{n e^{\theta_0}}.
\]</span></p>
</blockquote>
</section>
</section>
<section id="sec-mlegeneral" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="sec-mlegeneral"><span class="header-section-number">4.2</span> The general case</h2>
<section id="mle-estimation" class="level3" data-number="4.2.1">
<h3 data-number="4.2.1" class="anchored" data-anchor-id="mle-estimation"><span class="header-section-number">4.2.1</span> MLE Estimation</h3>
<p>Suppose that now the <span class="math inline">\(n\)</span> independent observations <span class="math inline">\(\{y_i,\ i=1,\dots,n\}\)</span> are not identically distributed. They are, however, sampled from the same exponential family density but with differing parameters, that is <span id="eq-iidobs"><span class="math display">\[
f(y_i; \theta_i,\phi) = \exp \left\{ \frac{y_i\theta_i  - b(\theta_i)}{\phi} + c(y_i, \phi) \right\},
\tag{4.11}\]</span></span> for <span class="math inline">\(i=1,\dots,n.\)</span> In this case, the canonical parameter does depend on <span class="math inline">\(i\)</span> – but we assume that the scale parameter <span class="math inline">\(\phi\)</span> does not – and let <span class="math inline">\(\boldsymbol{\theta} = \{\theta_i,\ i=1,\dots,n\}\)</span>.</p>
<p>In most examples, we are not interested in estimation of <span class="math inline">\(\boldsymbol{\theta}\)</span> but instead we are interested in the linear predictor parameters <span class="math inline">\(\boldsymbol{\beta}=\{\beta_1,\dots,\beta_p\}\)</span> but note that, in general, each <span class="math inline">\(\theta_i\)</span> will depend on all <span class="math inline">\(\boldsymbol{\beta}\)</span>. This is most obvious for the canonical parameter case where a convenient choice of link function is obtained using <span class="math inline">\(\boldsymbol{\theta} = \eta = X\boldsymbol{\beta}\)</span>, hence <span class="math inline">\(\theta_i= \sum_{j=1}^p \beta_{j} x_{ij}\)</span> and each <span class="math inline">\(\theta_i\)</span> clearly depends on all <span class="math inline">\(\boldsymbol{\beta}\)</span>.</p>
<p>The principle of maximum likelihood will be used to estimate the model parameters <span class="math inline">\(\boldsymbol{\beta}\)</span>. Using the independence of <span class="math inline">\(\mathbf{y}\)</span>, given all the parameters <span class="math inline">\(\boldsymbol{\beta}\)</span>, the likelihood function is: <span class="math display">\[
L(\boldsymbol{\beta}; \mathbf{y}, \phi)
= f(\mathbf{y}; X, \boldsymbol{\beta},\phi)
= \prod_{i=1}^n f(y_i; \boldsymbol{\beta}, \phi)
\]</span> and the log-likelihood by <span class="math display">\[
l (\boldsymbol{\beta}; \mathbf{y}, \phi)
= \log L(\boldsymbol{\beta}; \mathbf{y}, \phi)
= \sum_{i=1}^n \log f(y_i; \boldsymbol{\beta}, \phi).
\]</span> Hence we wish to find the value of <span class="math inline">\(\boldsymbol{\beta}\)</span> which maximizes this function <span class="math display">\[
\hat{\boldsymbol{\beta}} =
\max_{\boldsymbol{\beta}}
l (\boldsymbol{\beta}; \mathbf{y}, \phi).
\]</span> For generalized linear models there is usually no closed-form expression for the MLE <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>. Instead, an iterative approach based on the Newton–Raphson algorithm is usually adopted.</p>
<p>For the normal linear regression model, however, that is where the exponential family is Gaussian and the link function <span class="math inline">\(g(\mu)\)</span> is the identity function, we have the familiar closed-form expression <span id="eq-linearregressionMLE"><span class="math display">\[
\hat{\boldsymbol{\beta}} = \left(X^T X\right)^{-1} X^T \mathbf{y}.
\tag{4.12}\]</span></span></p>
</section>
<section id="the-score-function-and-fisher-information" class="level3" data-number="4.2.2">
<h3 data-number="4.2.2" class="anchored" data-anchor-id="the-score-function-and-fisher-information"><span class="header-section-number">4.2.2</span> The score function and Fisher information</h3>
<p>We define the <em>score function</em>: <span id="eq-U"><span class="math display">\[
U(\boldsymbol{\beta}) = \frac{\partial l(\boldsymbol{\beta}; \mathbf{y},\phi)} {\partial \boldsymbol{\beta}},
\tag{4.13}\]</span></span> which is a <span class="math inline">\(p \times 1\)</span> vector. We define the <em>observed Fisher information</em>: <span id="eq-I"><span class="math display">\[
{\cal I}(\boldsymbol{\beta}) = - \frac{\partial U(\boldsymbol{\beta})} {\partial \boldsymbol{\beta}^T}
          = -\frac{\partial^2 l(\boldsymbol{\beta}; \mathbf{y},\phi)} {\partial \boldsymbol{\beta} \, \partial \boldsymbol{\beta}^T},
\tag{4.14}\]</span></span> which is a <span class="math inline">\(p \times p\)</span> matrix whose <span class="math inline">\((j,k)\)</span>th element is: <span class="math inline">\(-\frac{\partial^2 l(\boldsymbol{\beta}; \mathbf{y},\phi)} {\partial \beta_j\partial \beta_k}\)</span>. We also define the <em>expected Fisher information</em>: <span id="eq-J"><span class="math display">\[
{\cal J}(\boldsymbol{\beta}) = \mbox{E}\left[ -\frac{\partial^2 l(\boldsymbol{\beta}; \mathbf{y},\phi)} {\partial \boldsymbol{\beta} \, \partial \boldsymbol{\beta}^T} \right],
\tag{4.15}\]</span></span> which is also a <span class="math inline">\(p \times p\)</span> matrix.</p>
<div id="prp-UJ" class="proposition theorem proposition">
<p><span class="theorem-title"><strong>Proposition 4.1 </strong></span>With definitions <a href="#eq-U">Equation&nbsp;<span>4.13</span></a>, <a href="#eq-I">Equation&nbsp;<span>4.14</span></a>, <a href="#eq-J">Equation&nbsp;<span>4.15</span></a> above,</p>
<ul>
<li><span class="math inline">\(\mbox{E}\left[U(\boldsymbol{\beta})\right] = 0\)</span>,</li>
<li><span class="math inline">\({\cal J}(\boldsymbol{\beta}) = \mbox{E}\left[U(\boldsymbol{\beta}) U^T(\boldsymbol{\beta})\right]\)</span>.</li>
</ul>
</div>
<p><strong>Proof</strong> We give the proof for continuous random variables. For the discrete case, replace integration by sums – see Exercises.</p>
<p>To start the proof, notice that we can re-write the joint density of the data given the parameters as <span class="math display">\[
f(\mathbf{y}; \boldsymbol{\beta},\phi) = L(\boldsymbol{\beta}; \mathbf{y}, \phi) =\exp\left\{ l(\boldsymbol{\beta}; \mathbf{y},\phi)\right\}
\]</span> and then <span class="math display">\[
1 = \int f(\mathbf{y}; \boldsymbol{\beta},\phi) d\mathbf{y}
  = \int \exp\{ l(\boldsymbol{\beta}; \mathbf{y},\phi)\} d\mathbf{y},
\]</span> where <span class="math inline">\(d\mathbf{y}=dy_1\cdots dy_n\)</span>. Differentiating this with respect to <span class="math inline">\(\boldsymbol{\beta}=(\beta_1,\dots,\beta_p)^T\)</span> gives: <span class="math display">\[\begin{align*}
0 &amp; = \int \frac{\partial l(\boldsymbol{\beta}; y,\phi)} {\partial \boldsymbol{\beta}}
      \exp\{ l(\boldsymbol{\beta}; y,\phi)\} dy  \\
&amp; = \int U(\boldsymbol{\beta}) f(y; \boldsymbol{\beta},\phi) dy  \tag{$\star$} \\
&amp;= \mbox{E}\left[U(\boldsymbol{\beta})\right],
\end{align*}\]</span> proving the first part.</p>
<p>Differentiating <span class="math inline">\((\star)\)</span> by parts w.r.t.&nbsp;<span class="math inline">\(\boldsymbol{\beta}^T\)</span>, <span class="math display">\[\begin{align*}
0 &amp; = \int \frac{\partial U(\boldsymbol{\beta})} {\partial \boldsymbol{\beta}^T}
      f(y; \boldsymbol{\beta},\phi)  + \frac{\partial l(\boldsymbol{\beta}; y,\phi)}
      {\partial \boldsymbol{\beta}} \frac{\partial l(\boldsymbol{\beta}; y,\phi)}
      {\partial \boldsymbol{\beta}^T} f(y; \boldsymbol{\beta},\phi) dy \\[2mm]
&amp; = \int - {\cal I}(\boldsymbol{\beta}) f(y; \boldsymbol{\beta},\phi) dy
    + \int U(\boldsymbol{\beta}) U^T(\boldsymbol{\beta}) f(y; \boldsymbol{\beta},\phi) dy \\[2mm]
&amp; = - {\cal J}(\boldsymbol{\beta}) + \mbox{E}\left[U(\boldsymbol{\beta}) U^T(\beta)\right],
\end{align*}\]</span> proving the second part.</p>
<div id="prp-asymptotic" class="proposition theorem proposition">
<p><span class="theorem-title"><strong>Proposition 4.2 </strong></span>Under some regularity conditions, the MLE <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> of <span class="math inline">\(\boldsymbol{\beta}\)</span> has the following asymptotic properties:</p>
<ul>
<li><span class="math inline">\(\mbox{E}(\hat{\boldsymbol{\beta}}) = \boldsymbol{\beta}\)</span>; i.e.&nbsp;<span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> is unbiased for <span class="math inline">\(\boldsymbol{\beta}\)</span>.</li>
<li><span class="math inline">\(\mbox{Var}(\hat{\boldsymbol{\beta}}) = {\cal J}^{-1}(\boldsymbol{\beta})\)</span>.</li>
<li><span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> follows a <span class="math inline">\(p\)</span>-dimensional Normal distribution.</li>
</ul>
<p>Combining these three statements we have that asymptotically <span id="eq-betahatdistribution"><span class="math display">\[
\hat{\boldsymbol{\beta}} \sim N_p (\boldsymbol{\beta}, {\cal J}^{-1}(\boldsymbol{\beta})).
\tag{4.16}\]</span></span></p>
</div>
<p><strong>Proof</strong></p>
<p>Omitted. Similar to the proof by Taylor’s theorem in <a href="#sec-mleiid"><span>Section&nbsp;4.1</span></a>.</p>
<p>From <a href="#eq-betahatdistribution">Equation&nbsp;<span>4.16</span></a>, standard errors <span class="math inline">\(\text{se}(\hat{\beta}_{k})\)</span> and correlations between parameter estimates <span class="math inline">\(\text{corr}(\hat{\beta}_{k},\hat{\beta}_{h})\)</span> can be estimated.</p>
</section>
<section id="sec-mlesaturated" class="level3" data-number="4.2.3">
<h3 data-number="4.2.3" class="anchored" data-anchor-id="sec-mlesaturated"><span class="header-section-number">4.2.3</span> The saturated case</h3>
<p><strong>Notes to be added later</strong></p>
</section>
</section>
<section id="model-deviance" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="model-deviance"><span class="header-section-number">4.3</span> Model deviance</h2>
<p><strong>Notes to be added later</strong></p>
</section>
<section id="model-residuals" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="model-residuals"><span class="header-section-number">4.4</span> Model residuals</h2>
<p><strong>Notes to be added later</strong></p>
</section>
<section id="fitting-generalized-linear-models-in-r" class="level2" data-number="4.5">
<h2 data-number="4.5" class="anchored" data-anchor-id="fitting-generalized-linear-models-in-r"><span class="header-section-number">4.5</span> Fitting generalized linear models in R</h2>
<p><strong>Notes to be added later</strong></p>
</section>
<section id="exercises" class="level2" data-number="4.6">
<h2 data-number="4.6" class="anchored" data-anchor-id="exercises"><span class="header-section-number">4.6</span> Exercises</h2>
<p>4.1 Use <a href="#eq-exponentialfamilybary2">Equation&nbsp;<span>4.4</span></a> to obtain estimation equations for the natural parameter <span class="math inline">\(\theta\)</span>, based on a sample <span class="math inline">\(\mathbf{y}=\{y_1,\dots, y_n\}\)</span>, for each of the following situations:</p>
<ol type="a">
<li>the binomial, <span class="math inline">\(Y\sim\mbox{Bin}(n,p)\)</span>,</li>
<li>the geometric, <span class="math inline">\(Y\sim\mbox{Ge}(p)\)</span>,</li>
<li>the exponential, <span class="math inline">\(Y\sim\mbox{Exp}(\lambda)\)</span>.</li>
</ol>
<p>Are these estimators unbiased for <span class="math inline">\(\theta\)</span>? What is the variance of the estimator in each case?</p>
<p><strong>Further questions to be added later</strong></p>


</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Using the result that the MLE of any function of a parameter is given by the same function applied to the MLE of the parameter.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./3_GLM-Theory.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">GLM Theory</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->



</body></html>