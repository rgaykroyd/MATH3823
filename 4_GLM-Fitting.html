<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>MATH3823 Generalized Linear Models - 4&nbsp; GLM Estimation</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./5_logisticmodel.html" rel="next">
<link href="./3_GLM-Theory.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">GLM Estimation</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">MATH3823 Generalized Linear Models</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Weekly schedule</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./0_preface.html" class="sidebar-item-text sidebar-link">Overview</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./1_intro.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2_linearmodels.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Essentials of Normal Linear Models</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3_GLM-Theory.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">GLM Theory</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./4_GLM-Fitting.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">GLM Estimation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./5_logisticmodel.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Modelling Proportions</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./6_loglinearmodel.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Loglinear Models</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./7_extendedloglinearmodel.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Extensions to Loglinear models</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-mleiid" id="toc-sec-mleiid" class="nav-link active" data-scroll-target="#sec-mleiid"><span class="toc-section-number">4.1</span>  The identically distributed case</a>
  <ul class="collapse">
  <li><a href="#maximum-likelihood-estimation" id="toc-maximum-likelihood-estimation" class="nav-link" data-scroll-target="#maximum-likelihood-estimation"><span class="toc-section-number">4.1.1</span>  Maximum likelihood estimation</a></li>
  <li><a href="#estimation-accuracy" id="toc-estimation-accuracy" class="nav-link" data-scroll-target="#estimation-accuracy"><span class="toc-section-number">4.1.2</span>  Estimation accuracy</a></li>
  </ul></li>
  <li><a href="#sec-mlegeneral" id="toc-sec-mlegeneral" class="nav-link" data-scroll-target="#sec-mlegeneral"><span class="toc-section-number">4.2</span>  The general case</a>
  <ul class="collapse">
  <li><a href="#mle-estimation" id="toc-mle-estimation" class="nav-link" data-scroll-target="#mle-estimation"><span class="toc-section-number">4.2.1</span>  MLE Estimation</a></li>
  <li><a href="#sec-score" id="toc-sec-score" class="nav-link" data-scroll-target="#sec-score"><span class="toc-section-number">4.2.2</span>  The score function and Fisher information</a></li>
  <li><a href="#sec-mlesaturated" id="toc-sec-mlesaturated" class="nav-link" data-scroll-target="#sec-mlesaturated"><span class="toc-section-number">4.2.3</span>  The saturated case</a></li>
  </ul></li>
  <li><a href="#sec-deviance" id="toc-sec-deviance" class="nav-link" data-scroll-target="#sec-deviance"><span class="toc-section-number">4.3</span>  Model deviance</a></li>
  <li><a href="#model-residuals" id="toc-model-residuals" class="nav-link" data-scroll-target="#model-residuals"><span class="toc-section-number">4.4</span>  Model residuals</a></li>
  <li><a href="#fitting-generalized-linear-models-in-r" id="toc-fitting-generalized-linear-models-in-r" class="nav-link" data-scroll-target="#fitting-generalized-linear-models-in-r"><span class="toc-section-number">4.5</span>  Fitting generalized linear models in <strong>R</strong></a>
  <ul class="collapse">
  <li><a href="#glm-related-r-commands" id="toc-glm-related-r-commands" class="nav-link" data-scroll-target="#glm-related-r-commands"><span class="toc-section-number">4.5.1</span>  GLM-related <strong>R</strong> commands</a></li>
  <li><a href="#example-of-fitting-poisson-glm-in-r" id="toc-example-of-fitting-poisson-glm-in-r" class="nav-link" data-scroll-target="#example-of-fitting-poisson-glm-in-r"><span class="toc-section-number">4.5.2</span>  Example of fitting Poisson GLM in <strong>R</strong></a></li>
  </ul></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="toc-section-number">4.6</span>  Exercises</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">GLM Estimation</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<div class="callout-warning callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>Please note that to make these lecture notes available on time, there has not been sufficient time to check the following sections. This, in particular, may mean that some of the cross-referencing is not accurate, as well as small typos. Further, the final few sections are not yet complete.</p>
</div>
</div>
<p>Throughout this module we use the principle of maximum likelihood estimation (MLE) to estimate model parameters and will consider two cases.</p>
<section id="sec-mleiid" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="sec-mleiid"><span class="header-section-number">4.1</span> The identically distributed case</h2>
<section id="maximum-likelihood-estimation" class="level3" data-number="4.1.1">
<h3 data-number="4.1.1" class="anchored" data-anchor-id="maximum-likelihood-estimation"><span class="header-section-number">4.1.1</span> Maximum likelihood estimation</h3>
<p>Suppose we have <span class="math inline">\(n\)</span> independent and identically distributed (i.i.d.) observations <span class="math inline">\(y_i,\ i=1,\ldots,n\)</span>, where each <span class="math inline">\(y_i\)</span> is sampled from the same exponential family density <span id="eq-iidobs"><span class="math display">\[
f(y_i; \theta,\phi) = \exp \left\{ \frac{y_i\theta  - b(\theta)}{\phi} + c(y_i, \phi) \right\},
\tag{4.1}\]</span></span> for <span class="math inline">\(i=1,\dots,n.\)</span> In this case, the canonical parameter <span class="math inline">\(\theta\)</span> does not depend on <span class="math inline">\(i\)</span>.</p>
<p>By independence, the joint distribution of all the observations <span class="math inline">\(\mathbf{y} = \{y_i,\ i=1,\dots,n\}\)</span> is: <span class="math display">\[
f(\mathbf{y}; \theta,\phi) = \prod_{i=1}^n f(y_i; \theta, \phi).
\]</span> So, taking logs and then substituting for the probability function using the exponential family form, <a href="#eq-exponential-family">Equation&nbsp;<span class="quarto-unresolved-ref">eq-exponential-family</span></a>, gives <span class="math display">\[
\log f(\mathbf{y}; \theta,\phi) = \sum_{i=1}^n \log f(y_i; \theta, \phi)
= \sum_{i=1}^n \left[\frac{y_i\theta  - b(\theta)}{\phi} + c(y_i, \phi)\right].
\]</span> Regarding the observations <span class="math inline">\(\mathbf{y}\)</span> as constants (which they are, once we have them) and the scale parameter <span class="math inline">\(\phi\)</span> as a fixed <em>nuisance</em> parameter (whose value we may not know), the log-likelihood as a function of the parameter <span class="math inline">\(\theta\)</span> of interest is: <span id="eq-ltheta"><span class="math display">\[
l(\theta; \mathbf{y},\phi)  = n\left( \frac{\bar{y}\, \theta  -  b(\theta)}{\phi}\right) + \mbox{constant},
\tag{4.2}\]</span></span> where <span class="math inline">\(\bar{y}= \sum y_i/n.\)</span></p>
<p>We estimate <span class="math inline">\(\theta\)</span> by maximizing the log likelihood – i.e.&nbsp;given the data <span class="math inline">\(\mathbf{y}\)</span>, we estimate the value of <span class="math inline">\(\theta\)</span> to be that value for which the likelihood, and hence the log-likelihood, is greatest.</p>
<p>We maximize the log-likelihood by differentiating it and setting it to zero: <span class="math display">\[
\frac{d l(\theta; \mathbf{y},\phi)}{d \theta}  
= n \left(\frac{\bar{y} -  b'(\theta)}{\phi}\right)
\]</span> and hence the MLE for <span class="math inline">\(\theta\)</span>, which we denote <span class="math inline">\(\hat\theta\)</span>, satisfies <span id="eq-exponentialfamilybary"><span class="math display">\[
b'(\hat\theta) = \bar{y}
\tag{4.3}\]</span></span> and hence <span id="eq-exponentialfamilybary2"><span class="math display">\[
\hat\theta =  (b')^{-1}(\bar{y}).
\tag{4.4}\]</span></span></p>
<p>Further, we showed in <a href="#prp-moments">Proposition&nbsp;<span class="quarto-unresolved-ref">prp-moments</span></a> that <span class="math inline">\(\mbox{E}[Y] = \mu =b'(\theta)\)</span> and if we let <span class="math inline">\(\hat\mu\)</span> denote the MLE of <span class="math inline">\(\mu\)</span>, then <span class="math inline">\(\hat\mu = b'(\hat{\theta})\)</span><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, hence we have <span class="math inline">\(\hat\mu = \bar{y}\)</span>. So we find that <span class="math inline">\(\hat\theta\)</span> is the value of <span class="math inline">\(\theta\)</span> for which the theoretical mean <span class="math inline">\(\hat\mu = b'(\hat\theta)\)</span> matches the sample mean <span class="math inline">\(\bar{y}\)</span>.</p>
<blockquote class="blockquote">
<p><strong>Example: MLE of the Poisson distribution</strong></p>
<p>For the Poisson distribution, <span class="math inline">\(\text{Po}(\lambda)\)</span>, we have found that <span class="math inline">\(b(\theta) = e^\theta\)</span> and therefore <span class="math inline">\(b'(\theta) = e^\theta\)</span>. Hence, the MLE of natural parameter <span class="math inline">\(\theta\)</span> is found as the solution of <span class="math inline">\(b'(\hat\theta) = e^{\hat\theta} = \bar{y}\)</span>, that is <span class="math inline">\(\hat\theta = \log(\bar{y})\)</span>.</p>
</blockquote>
</section>
<section id="estimation-accuracy" class="level3" data-number="4.1.2">
<h3 data-number="4.1.2" class="anchored" data-anchor-id="estimation-accuracy"><span class="header-section-number">4.1.2</span> Estimation accuracy</h3>
<p>For our i.i.d. sample <span class="math inline">\(y_i,\ i=1,\ldots,n\)</span>, we have <span class="math inline">\(b'(\hat\theta) = \hat\mu = \bar y\)</span>. Let <span class="math inline">\(\theta_0\)</span> be the true value of <span class="math inline">\(\theta\)</span> with corresponding mean <span class="math inline">\(\mu_0\)</span>, i.e.&nbsp;<span id="eq-mu0"><span class="math display">\[
b'(\theta_0) = \mu_0.  
\tag{4.5}\]</span></span> How accurate is <span class="math inline">\(\hat\theta\)</span>? We know that <span id="eq-Ebary"><span class="math display">\[
\mbox{E}[\bar Y] = \mbox{E}\left[\frac{1}{n}\sum_{i=1}^n Y_i\right]
= \frac{1}{n} \sum_{i=1}^n \mbox{E}[Y_i]
= \mu_0  
= b'(\theta_0),
\tag{4.6}\]</span></span> using <a href="#eq-mu0">Equation&nbsp;<span class="quarto-unresolved-ref">eq-mu0</span></a>, and <span class="math display">\[
\mbox{Var}[\bar Y] = \mbox{Var}\left[\frac{1}{n} \sum_{i=1}^n Y_i\right]
= \frac{1}{n^2}  \sum_{i=1}^n \mbox{Var}[Y_i]
\]</span> because the observations are independent. Then, using the result <a href="#eq-exponential-moments">Equation&nbsp;<span class="quarto-unresolved-ref">eq-exponential-moments</span></a> <span id="eq-varbary"><span class="math display">\[
\mbox{Var}[\bar Y]= \frac{1}{n} \ b''(\theta_0) \phi.
\tag{4.7}\]</span></span></p>
<p>We can use Taylor’s theorem to expand <span class="math inline">\(b'(\hat\theta)\)</span> about <span class="math inline">\(\theta_0\)</span>: <span class="math display">\[
\bar y = b'(\hat\theta) \approx  b'(\theta_0) + (\hat\theta - \theta_0) b''(\theta_0),
\]</span> which implies that <span id="eq-hatthetatheta0"><span class="math display">\[
(\hat\theta - \theta_0) \approx  b''(\theta_0)^{-1}\{b'(\hat\theta) - b'(\theta_0)\}  
= b''(\theta_0)^{-1}(\bar Y - \mu_0),
\tag{4.8}\]</span></span> using <a href="#eq-exponentialfamilybary">Equation&nbsp;<span class="quarto-unresolved-ref">eq-exponentialfamilybary</span></a> and <a href="#eq-mu0">Equation&nbsp;<span class="quarto-unresolved-ref">eq-mu0</span></a>. We can use <a href="#eq-hatthetatheta0">Equation&nbsp;<span class="quarto-unresolved-ref">eq-hatthetatheta0</span></a> to get approximations to the mean and variance of <span class="math inline">\(\hat\theta\)</span>: <span class="math display">\[
\mbox{E}[\hat\theta - \theta_0]  \approx   b''(\theta_0)^{-1} \mbox{E}[\bar Y - \mu_0]
= 0,
\]</span> using <a href="#eq-Ebary">Equation&nbsp;<span class="quarto-unresolved-ref">eq-Ebary</span></a>, so <span id="eq-Ehattheta"><span class="math display">\[
\mbox{E}[\hat\theta] \approx \theta_0,
\tag{4.9}\]</span></span> and <span class="math display">\[
\mbox{Var}(\hat\theta) \approx \mbox{E}\left[(\hat\theta - \theta_0)^2\right]
\]</span> using <a href="#eq-Ehattheta">Equation&nbsp;<span class="quarto-unresolved-ref">eq-Ehattheta</span></a>, <span class="math display">\[
\mbox{Var}(\hat\theta) \approx \mbox{E}\left[\left(b''(\theta_0)^{-1}(\bar Y - \mu_0)\right)^2\right]
\]</span> using <a href="#eq-hatthetatheta0">Equation&nbsp;<span class="quarto-unresolved-ref">eq-hatthetatheta0</span></a>, <span class="math display">\[
\mbox{Var}(\hat\theta)\approx  \left(b''(\theta_0)\right)^{-2} \mbox{Var}[\bar Y]  
\]</span> using <a href="#eq-Ebary">Equation&nbsp;<span class="quarto-unresolved-ref">eq-Ebary</span></a>, <span id="eq-Varhattheta"><span class="math display">\[
\mbox{Var}(\hat\theta) = \frac{\phi}{n \ b''(\theta_0)}
\tag{4.10}\]</span></span> using <a href="#eq-varbary">Equation&nbsp;<span class="quarto-unresolved-ref">eq-varbary</span></a>.</p>
<p>Thus we see that the first two derivatives of <span class="math inline">\(b(\theta)\)</span> play a key role in inference.</p>
<blockquote class="blockquote">
<p><strong>Example: Accuracy for the Poisson distribution</strong></p>
<p>For the Poisson distribution, <span class="math inline">\(\text{Po}(\lambda)\)</span>, we have found that <span class="math inline">\(\hat\theta = \log(\bar{Y})\)</span>. Using <a href="#eq-Ehattheta">Equation&nbsp;<span class="quarto-unresolved-ref">eq-Ehattheta</span></a> we know that <span class="math inline">\(\hat\theta\)</span> is, at least, approximately unbiased. Then, using that <span class="math inline">\(\phi=1\)</span> (<a href="#tbl-GLM-poisson">Table&nbsp;<span class="quarto-unresolved-ref">tbl-GLM-poisson</span></a>), <span class="math inline">\(b'(\theta)=e^{\theta}\)</span> (<a href="#tbl-canonicalrange">Table&nbsp;<span class="quarto-unresolved-ref">tbl-canonicalrange</span></a>) hence <span class="math inline">\(b''(\theta)=e^{\theta}\)</span>, and using <a href="#eq-Varhattheta">Equation&nbsp;<span class="quarto-unresolved-ref">eq-Varhattheta</span></a>, leads to the result <span class="math display">\[
\mbox{Var}(\hat\theta) = \frac{\phi}{n \ b''(\theta_0)}
=
\frac{1}{n e^{\theta_0}}.
\]</span></p>
</blockquote>
</section>
</section>
<section id="sec-mlegeneral" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="sec-mlegeneral"><span class="header-section-number">4.2</span> The general case</h2>
<section id="mle-estimation" class="level3" data-number="4.2.1">
<h3 data-number="4.2.1" class="anchored" data-anchor-id="mle-estimation"><span class="header-section-number">4.2.1</span> MLE Estimation</h3>
<p>Suppose that now the <span class="math inline">\(n\)</span> independent observations <span class="math inline">\(\{y_i,\ i=1,\dots,n\}\)</span> are not identically distributed. They are, however, sampled from the same exponential family density but with differing parameters, that is <span class="math display">\[
f(y_i; \theta_i,\phi) = \exp \left\{ \frac{y_i\theta_i  - b(\theta_i)}{\phi} + c(y_i, \phi) \right\},
\]</span> for <span class="math inline">\(i=1,\dots,n.\)</span> In this case, the canonical parameter does depend on <span class="math inline">\(i\)</span> – but we assume that the scale parameter <span class="math inline">\(\phi\)</span> does not – and let <span class="math inline">\(\boldsymbol{\theta} = \{\theta_i,\ i=1,\dots,n\}\)</span>.</p>
<p>In most applications, we are not interested in estimation of <span class="math inline">\(\boldsymbol{\theta}\)</span> but instead we are interested in the linear predictor parameters <span class="math inline">\(\boldsymbol{\beta} =\{\beta_1,\dots,\beta_p\}\)</span>. Note, however, that each <span class="math inline">\(\theta_i\)</span> will depend on all <span class="math inline">\(\beta_1,\dots,\beta_p\)</span>. This is most obvious for the canonical parameter case where a convenient choice of link function is obtained using <span class="math inline">\(\boldsymbol{\theta} = \eta = X\boldsymbol{\beta}\)</span>, hence <span class="math inline">\(\theta_i= \mathbf{x}_i^T \boldsymbol{\beta} =\sum_{j=1}^p x_{ij}\beta_{j}\)</span> and each <span class="math inline">\(\theta_i\)</span> clearly depends on all <span class="math inline">\(\beta_1,\dots,\beta_p\)</span>.</p>
<p>The principle of maximum likelihood will be used to estimate the model parameters <span class="math inline">\(\boldsymbol{\beta}\)</span>. Using the independence of <span class="math inline">\(y_i, i=1,\dots,n\)</span>, given the parameters <span class="math inline">\(\boldsymbol{\beta}\)</span>, the likelihood function is: <span id="eq-likelihoodgeneral"><span class="math display">\[
L(\boldsymbol{\beta}; \mathbf{y}, \phi)
= f(\mathbf{y}; X, \boldsymbol{\beta},\phi)
= \prod_{i=1}^n f(y_i; \mathbf{x}_i, \boldsymbol{\beta}, \phi)
\tag{4.11}\]</span></span> and the log-likelihood by <span id="eq-loglikelihoodgeneral"><span class="math display">\[
l (\boldsymbol{\beta}; \mathbf{y}, \phi)
= \log L(\boldsymbol{\beta}; \mathbf{y}, \phi)
= \sum_{i=1}^n \log f(y_i;
\mathbf{x}_i,
\boldsymbol{\beta}, \phi).
\tag{4.12}\]</span></span> Then we wish to find the value of <span class="math inline">\(\boldsymbol{\beta}\)</span> which maximizes the log-likelihood function, <span class="math display">\[
\hat{\boldsymbol{\beta}} =
\max_{\boldsymbol{\beta}}
l (\boldsymbol{\beta}; \mathbf{y}, \phi).
\]</span> For generalized linear models there is usually no closed-form expression for the MLE <span class="math inline">\(\hat{\boldsymbol{\beta}}.\)</span> Instead, an iterative approach based on the Newton–Raphson algorithm is usually adopted.</p>
<p>For the normal linear regression model, however, that is where the exponential family is Gaussian and the link function <span class="math inline">\(g(\mu)\)</span> is the identity function, we have the familiar closed-form expression <span id="eq-linearregressionMLE"><span class="math display">\[
\hat{\boldsymbol{\beta}} = \left(X^T X\right)^{-1} X^T \mathbf{y}.
\tag{4.13}\]</span></span></p>
</section>
<section id="sec-score" class="level3" data-number="4.2.2">
<h3 data-number="4.2.2" class="anchored" data-anchor-id="sec-score"><span class="header-section-number">4.2.2</span> The score function and Fisher information</h3>
<p>We define the <em>score function</em>: <span id="eq-U"><span class="math display">\[
U(\boldsymbol{\beta}) = \frac{\partial l(\boldsymbol{\beta}; \mathbf{y},\phi)} {\partial \boldsymbol{\beta}},
\tag{4.14}\]</span></span> which is a <span class="math inline">\(p \times 1\)</span> vector. We define the <em>observed Fisher information</em>: <span id="eq-I"><span class="math display">\[
{\cal I}(\boldsymbol{\beta}) = - \frac{\partial U(\boldsymbol{\beta})} {\partial \boldsymbol{\beta}^T}
          = -\frac{\partial^2 l(\boldsymbol{\beta}; \mathbf{y},\phi)} {\partial \boldsymbol{\beta} \, \partial \boldsymbol{\beta}^T},
\tag{4.15}\]</span></span> which is a <span class="math inline">\(p \times p\)</span> matrix whose <span class="math inline">\((j,k)\)</span>th element is: <span class="math inline">\(-\frac{\partial^2 l(\boldsymbol{\beta}; \mathbf{y},\phi)} {\partial \beta_j\partial \beta_k}\)</span>. We also define the <em>expected Fisher information</em>: <span id="eq-J"><span class="math display">\[
{\cal J}(\boldsymbol{\beta}) = \mbox{E}\left[ -\frac{\partial^2 l(\boldsymbol{\beta}; \mathbf{y},\phi)} {\partial \boldsymbol{\beta} \, \partial \boldsymbol{\beta}^T} \right],
\tag{4.16}\]</span></span> which is also a <span class="math inline">\(p \times p\)</span> matrix.</p>
<div id="prp-UJ" class="proposition theorem proposition">
<p><span class="theorem-title"><strong>Proposition 4.1 </strong></span>With definitions <a href="#eq-U">Equation&nbsp;<span class="quarto-unresolved-ref">eq-U</span></a>, <a href="#eq-I">Equation&nbsp;<span class="quarto-unresolved-ref">eq-I</span></a>, <a href="#eq-J">Equation&nbsp;<span class="quarto-unresolved-ref">eq-J</span></a> above,</p>
<ul>
<li><span class="math inline">\(\mbox{E}\left[U(\boldsymbol{\beta})\right] = 0\)</span>,</li>
<li><span class="math inline">\({\cal J}(\boldsymbol{\beta}) = \mbox{E}\left[U(\boldsymbol{\beta}) U^T(\boldsymbol{\beta})\right]\)</span>.</li>
</ul>
</div>
<p><strong>Proof</strong> We give the proof for continuous random variables. For the discrete case, replace integration by sums – see Exercises.</p>
<p>To start the proof, notice that we can re-write the joint density of the data given the parameters as <span class="math display">\[
f(\mathbf{y}; X, \boldsymbol{\beta},\phi) = L(\boldsymbol{\beta}; \mathbf{y}, \phi) =\exp\left\{ l(\boldsymbol{\beta}; \mathbf{y},\phi)\right\}
\]</span> and then <span class="math display">\[
1 = \int f(\mathbf{y}; X \boldsymbol{\beta},\phi) d\mathbf{y}
  = \int \exp\{ l(\boldsymbol{\beta}; \mathbf{y},\phi)\} d\mathbf{y},
\]</span> where <span class="math inline">\(d\mathbf{y}=dy_1\cdots dy_n\)</span>. Differentiating this with respect to <span class="math inline">\(\boldsymbol{\beta}=(\beta_1,\dots,\beta_p)^T\)</span> gives: <span class="math display">\[\begin{align*}
0 &amp; = \int \frac{\partial l(\boldsymbol{\beta}; \mathbf{y},\phi)} {\partial \boldsymbol{\beta}}
      \exp\{ l(\boldsymbol{\beta}; \mathbf{y},\phi)\} d\mathbf{y}  \\
&amp; = \int U(\boldsymbol{\beta}) f(\mathbf{y}; X, \boldsymbol{\beta},\phi) d\mathbf{y}  \tag{$\star$} \\
&amp;= \mbox{E}\left[U(\boldsymbol{\beta})\right].
\end{align*}\]</span> Proving the first part.</p>
<p>Next, differentiating <span class="math inline">\((\star)\)</span> by parts with respect to &nbsp;<span class="math inline">\(\boldsymbol{\beta}^T\)</span>, <span class="math display">\[\begin{align*}
0 &amp; = \int \frac{\partial U(\boldsymbol{\beta})} {\partial \boldsymbol{\beta}^T}
      f(\mathbf{y}; X, \boldsymbol{\beta},\phi)  + \frac{\partial l(\boldsymbol{\beta}; \mathbf{y},\phi)}
      {\partial \boldsymbol{\beta}} \frac{\partial l(\boldsymbol{\beta}; \mathbf{y},\phi)}
      {\partial \boldsymbol{\beta}^T} f(\mathbf{y}; X, \boldsymbol{\beta},\phi) d\mathbf{y} \\[2mm]
&amp; = \int - {\cal I}(\boldsymbol{\beta}) f(\mathbf{y}; X, \boldsymbol{\beta},\phi) d\mathbf{y}
    + \int U(\boldsymbol{\beta}) U^T(\boldsymbol{\beta}) f(\mathbf{y}; X, \boldsymbol{\beta},\phi) d\mathbf{y} \\[2mm]
&amp; = - {\cal J}(\boldsymbol{\beta}) + \mbox{E}\left[U(\boldsymbol{\beta}) U^T(\boldsymbol{\beta})\right].
\end{align*}\]</span> proving the second part.</p>
<div id="prp-asymptotic" class="proposition theorem proposition">
<p><span class="theorem-title"><strong>Proposition 4.2 </strong></span>Under some regularity conditions, the MLE <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> of <span class="math inline">\(\boldsymbol{\beta}\)</span> has the following asymptotic properties:</p>
<ul>
<li><span class="math inline">\(\mbox{E}(\hat{\boldsymbol{\beta}}) = \boldsymbol{\beta}\)</span>; i.e.&nbsp;<span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> is unbiased for <span class="math inline">\(\boldsymbol{\beta}\)</span>.</li>
<li><span class="math inline">\(\mbox{Var}(\hat{\boldsymbol{\beta}}) = {\cal J}^{-1}(\boldsymbol{\beta})\)</span>.</li>
<li><span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> follows a <span class="math inline">\(p\)</span>-dimensional Normal distribution.</li>
</ul>
<p>Combining these three statements we have that asymptotically <span id="eq-betahatdistribution"><span class="math display">\[
\hat{\boldsymbol{\beta}} \sim N_p (\boldsymbol{\beta}, {\cal J}^{-1}(\boldsymbol{\beta})).
\tag{4.17}\]</span></span></p>
</div>
<p><strong>Proof:</strong> Omitted. Similar to the proof using Taylor’s theorem in <a href="#sec-mleiid"><span class="quarto-unresolved-ref">sec-mleiid</span></a>.</p>
<p>From <a href="#eq-betahatdistribution">Equation&nbsp;<span class="quarto-unresolved-ref">eq-betahatdistribution</span></a>, variances <span class="math inline">\(\text{Var}(\hat{\beta}_{k})\)</span>, standard errors <span class="math inline">\(\text{se}(\hat{\beta}_{k})\)</span> and correlations between parameter estimates <span class="math inline">\(\text{Corr}(\hat{\beta}_{k},\hat{\beta}_{h})\)</span> can be estimated.</p>
</section>
<section id="sec-mlesaturated" class="level3" data-number="4.2.3">
<h3 data-number="4.2.3" class="anchored" data-anchor-id="sec-mlesaturated"><span class="header-section-number">4.2.3</span> The saturated case</h3>
<p>Again we assume the observations <span class="math inline">\(y_i,\ i=1,\dots,n\)</span> are independent but now we assume that <span class="math inline">\(y_i\)</span> is sampled from an exponential family probability function with canonical parameter <span class="math inline">\(\theta_i\)</span>, <span class="math display">\[
f(y_i; \theta_i,\phi) = \exp \left\{ \frac{y_i\theta_i  - b(\theta_i)}{\phi} + c(y_i, \phi) \right\},
\]</span> for <span class="math inline">\(i=1,\dots,n.\)</span> We can form the log-likelihood in the usual way to give <span class="math display">\[
l (\boldsymbol{\theta}; \mathbf{y}, \phi) = \sum_{i=1}^n \log f(y_i; \theta_i, \phi)
= \sum_{i=1}^n \left[\frac{y_i\theta_i  - b(\theta_i)}{\phi} + c(y_i, \phi)\right]
\]</span> and so we find the the MLE of <span class="math inline">\(\boldsymbol{\theta}\)</span> using <span class="math display">\[
\hat \theta_i = (b')^{-1}(y_i),
\quad i=1,\dots,n.
\]</span> Note that each parameter is only a function of the corresponding observation.</p>
<p>Further, from <a href="#prp-moments">Proposition&nbsp;<span class="quarto-unresolved-ref">prp-moments</span></a>, <span class="math inline">\(\mbox{E}[Y_i] = \mu_i =b'(\theta_i)\)</span> and if we again let <span class="math inline">\(\hat\mu_i\)</span> denote the MLE of <span class="math inline">\(\mu_i\)</span>, then <span class="math inline">\(\hat\mu_i = b'(\hat{\theta_i})\)</span>, hence we have <span class="math inline">\(\hat\mu_i = y_i\)</span>. Thus we see that, under the saturated model, the mean of the distribution of <span class="math inline">\(y_i\)</span> is estimated to be equal to <span class="math inline">\(y_i\)</span> itself. That is, the data are fitted exactly by the model. Of course this model is quite useless for explanation or prediction, since it misinterprets random variation as systematic variation. Nevertheless, the saturated model is useful as a benchmark for comparing models, as we will see later.</p>
<p>It is worth noting that the same situation can occur even when modelling in terms of the regression parameters <span class="math inline">\(\beta_j,\ j=1,\dots,p\)</span>. When <span class="math inline">\(p\geq n\)</span>, and if the covariates are linearly independent, each <span class="math inline">\(\theta_i\)</span> can take on any value independently of the others and so estimating the <span class="math inline">\(\beta_j\)</span>’s is equivalent to estimating the <span class="math inline">\(\theta_i\)</span>’s. That is we are also considering the <em>saturated</em> or <em>full</em> model. This highlights the danger of putting too many covariates into the model. There is a big literature on how to deal with more parameters then data using techniques of <em>regularized regression</em>.</p>
</section>
</section>
<section id="sec-deviance" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="sec-deviance"><span class="header-section-number">4.3</span> Model deviance</h2>
<p>The <em>deviance</em> is a quantity we use to assess the fit of a model to the data. Let <span class="math inline">\(M\)</span> be a model of interest with fitted parameters <span class="math inline">\(\hat{\boldsymbol\theta}\)</span> and corresponding fitted values <span class="math inline">\(\hat{\boldsymbol\mu}\)</span>. Also consider the saturated model with fitted parameters <span class="math inline">\(\tilde{\boldsymbol\theta}\)</span> and fitted values <span class="math inline">\(\tilde{\boldsymbol\mu}\)</span>.</p>
<p>The <em>deviance</em> of model <span class="math inline">\(M\)</span> is defined as twice the difference between the log-likelihood of the saturated model, <span class="math inline">\(l(\tilde{\boldsymbol\theta};\mathbf{y},\phi)\)</span>, and the log-likelihood of model <span class="math inline">\(M\)</span>, <span class="math inline">\(l(\hat{\boldsymbol\theta}; \mathbf{y},\phi)\)</span>, multiplied by <span class="math inline">\(\phi\)</span>, <span class="math display">\[\begin{eqnarray}
D
&amp; = &amp; 2 \phi \left\{ l(\tilde{\boldsymbol\theta};\mathbf{y},\phi) - l(\hat{\boldsymbol\theta}; \mathbf{y},\phi) \right\} \label{eq:deviance}\\[4mm]
&amp; = &amp;  \left\{ \begin{array}{ll}
    \sum_{i=1}^n  (y_i - \hat\mu_i)^2
   = \mbox{Residual sum of squares} &amp;  \mbox{ Normal} \\[3mm]
   2 \sum_{i=1}^n \left\{ y_i \log \left( \frac{y_i}{\hat\mu_i} \right)
   + (m_i - y_i) \log \left( \frac{m_i - y_i}{m_i - \hat\mu_i} \right) \right\}
   &amp;  \mbox{ Binomial} \\[3mm]
   2 \sum_{i=1}^n \left\{ y_i \log \left( \frac{y_i}{\hat\mu_i} \right)
   - y_i + \hat\mu_i \right\} &amp;  \mbox{ Poisson} \\ \end{array} \right.\notag
\end{eqnarray}\]</span> Note that Dobson, and others, call <span class="math inline">\(D^*= D/\phi=2 \{ l(\tilde{\theta};y,\phi) - l(\hat{\theta}; y,\phi)\}\)</span> the <em>scaled</em> deviance.</p>
<p>We now consider two situations:</p>
<p><strong>Scale parameter <span class="math inline">\(\phi\)</span> known.</strong> For some data-types (e.g.&nbsp;Poisson, Binomial), we know <span class="math inline">\(\phi=1\)</span>. Consider two nested models <span class="math inline">\(M_1\)</span> and <span class="math inline">\(M_2\)</span> with <span class="math inline">\(r_1\)</span> and <span class="math inline">\(r_2\)</span> parameters respectively where the parameters in <span class="math inline">\(M_1\)</span> are a subset of those in <span class="math inline">\(M_2\)</span> and hence <span class="math inline">\(r_1 &lt; r_2\)</span>. Further, let <span class="math inline">\(D_1\)</span> and <span class="math inline">\(D_2\)</span> be the deviances of model <span class="math inline">\(M_1\)</span> and <span class="math inline">\(M_2\)</span> respectively.</p>
<p>Then, asymptotically,</p>
<ul>
<li><p>the log likelihood-ratio statistic <span class="math inline">\(D_1 - D_2 \sim \chi^2_{r_2 - r_1}\)</span> can be used to test the importance of the extra parameters in <span class="math inline">\(M_2\)</span> not included in <span class="math inline">\(M_1\)</span>;</p></li>
<li><p>a goodness-of-fit test for <span class="math inline">\(M_2\)</span> can be done based on <span class="math inline">\(D_2 \sim \chi^2_{n - r_2}\)</span>.</p></li>
</ul>
<p>The quality of the approximations involved depends on there being a large amount of <em>information</em>, for example, large counts for Binomial and Poisson data, or a large sample size for Normal data.</p>
<p><strong>Scale parameter <span class="math inline">\(\phi\)</span> unknown.</strong> For some data-types (e.g.&nbsp;Normal, Gamma), <span class="math inline">\(\phi\)</span> is not known (typically <span class="math inline">\(\phi = \sigma^2\)</span>). We must find a model <span class="math inline">\(M_3\)</span> <em>big</em> enough to be believed, then estimate <span class="math inline">\(\phi\)</span> by the residual mean square: <span id="eq-phihat"><span class="math display">\[
    \hat{\phi} = \frac{D_3}{n - r_3}.
     \tag{4.18}\]</span></span> Then test <span class="math inline">\(M_1\)</span> against <span class="math inline">\(M_2\)</span> using <span id="eq-deviance-ratio"><span class="math display">\[
    \mbox{F}=\frac{(D_1 - D_2) / (r_2 - r_1)}{\hat{\phi}} = \frac{(D_1 - D_2) / (r_2 - r_1)}{D_3/(n-r_3)}
     \tag{4.19}\]</span></span> with <span id="eq-Ftest"><span class="math display">\[
    \mbox{F} \sim F_{r_2 - r_1, n - r_3}.
     \tag{4.20}\]</span></span> So if the observed value of the statistic <a href="#eq-deviance-ratio">Equation&nbsp;<span class="quarto-unresolved-ref">eq-deviance-ratio</span></a> was within the upper (say 5%) tail of the <span class="math inline">\(F\)</span>-distribution <a href="#eq-Ftest">Equation&nbsp;<span class="quarto-unresolved-ref">eq-Ftest</span></a>, we would infer that Model <span class="math inline">\(M_2\)</span> is better than Model <span class="math inline">\(M_1\)</span>.</p>
</section>
<section id="model-residuals" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="model-residuals"><span class="header-section-number">4.4</span> Model residuals</h2>
<p>Consider a generalized linear model with observed values <span class="math inline">\(y_i, i=1,\dots,n\)</span> and fitted values <span class="math inline">\(\hat\mu_i\)</span>. Then the <em>raw</em> or <em>response</em> residuals are defined by <span class="math display">\[
e_i^\text{raw} = y_i - \hat\mu_i.
\]</span></p>
<p>More useful are the <em>standardized</em> or <em>Pearson</em> residuals defined by <span class="math display">\[
e_i^\text{std} =
e_i^\text{P} = \frac{y_i - \hat\mu_i}{\sqrt{ b''(\theta_i)}}.
\]</span> Recall from <a href="#eq-exponential-moments">Equation&nbsp;<span class="quarto-unresolved-ref">eq-exponential-moments</span></a> that <span class="math inline">\(\text{Var}(Y_i) = \phi \, b''(\theta_i)\)</span>.</p>
<p><em>Deviance residuals</em> are defined so that the sum of squared deviance residuals equals the total deviance. Thus we set <span class="math display">\[
e_i^\text{dev} = \text{sign}(y_i - \hat\mu_i) \sqrt{d_i},
\]</span> where <span class="math inline">\(d_i\)</span> is the contribution of observation <span class="math inline">\(i\)</span> to the deviance, <span class="math inline">\(D\)</span>. For example, when <span class="math inline">\(y_i\)</span> has a Poisson distribution with estimated mean <span class="math inline">\(\hat{\mu}_i\)</span>, we have <span class="math display">\[
e_i^\text{dev} = \text{sign}(y_i - \hat\mu_i) \sqrt{2\left[y_i \log\left(\frac{y_i}{\hat{\mu}_i}\right) - y_i + \hat{\mu}_i\right]}.
\]</span></p>
<p>Residuals are useful for assessing the overall fit of a model to the data, and for identifying where the model might need to be improved.</p>
</section>
<section id="fitting-generalized-linear-models-in-r" class="level2" data-number="4.5">
<h2 data-number="4.5" class="anchored" data-anchor-id="fitting-generalized-linear-models-in-r"><span class="header-section-number">4.5</span> Fitting generalized linear models in <strong>R</strong></h2>
<section id="glm-related-r-commands" class="level3" data-number="4.5.1">
<h3 data-number="4.5.1" class="anchored" data-anchor-id="glm-related-r-commands"><span class="header-section-number">4.5.1</span> GLM-related <strong>R</strong> commands</h3>
<p>The function used to fit a generalized linear model in <span class="math inline">\(\mathbf R\)</span> is <span class="math display">\[\texttt{glm(formula, family)}.\]</span></p>
<p>Let <span class="math inline">\(\texttt{x,y,z,a,b,c,}\)</span> <span class="math inline">\(\dots\)</span> be a set of vectors all of the same length <span class="math inline">\(n\)</span> (perhaps read in from a data file using the <span class="math inline">\(\texttt{read.table}\)</span> and <span class="math inline">\(\texttt{attach}\)</span> commands). If <span class="math inline">\(\texttt{a,b,c}\)</span> are qualitative variables, then they first need to be declared as factors by <span class="math inline">\(\texttt{a = as.factor(a)}\)</span>, etc.</p>
<p>The <span class="math inline">\(\texttt{formula}\)</span> argument of <span class="math inline">\(\texttt{glm}\)</span> specifies the required model in compact notation, e.g. <span class="math inline">\(\texttt{y} \sim \texttt{x*a}\)</span> or <span class="math inline">\(\texttt{y} \sim \texttt{x + z*a}\)</span> where <span class="math inline">\(\sim, \texttt{+,*}\)</span> have the same meaning as in <a href="#sec-shorthand"><span class="quarto-unresolved-ref">sec-shorthand</span></a>.</p>
<p>The <span class="math inline">\(\texttt{family}\)</span> argument specifies which exponential family is to be used. We shall use <span class="math inline">\(\texttt{gaussian, poisson}\)</span> and <span class="math inline">\(\texttt{binomial}\)</span>; <span class="math inline">\(\texttt{gaussian}\)</span> is the default. Other options are available; see <span class="math inline">\(\texttt{help(family)}\)</span> for further information.</p>
<p>Along with the family, a link function can be specified. The possible choices are:</p>
<ul>
<li><span class="math inline">\(\texttt{gaussian} - \texttt{"identity"}\)</span> (default)</li>
<li><span class="math inline">\(\texttt{poisson} - \texttt{"log"}\)</span> (default), <span class="math inline">\(\texttt{"sqrt"}\)</span>, <span class="math inline">\(\texttt{"identity"}\)</span></li>
<li><span class="math inline">\(\texttt{binomial} - \texttt{"logit"}\)</span> (default), <span class="math inline">\(\texttt{"probit"}\)</span>, <span class="math inline">\(\texttt{"cloglog"}\)</span>.</li>
</ul>
<p><strong>R</strong> assumes the default options unless we state otherwise. For example,</p>
<p><span class="math inline">\(\texttt{glm(y}\sim\texttt{a+b)}\)</span> # Gaussian errors, identity link<br>
<span class="math inline">\(\texttt{glm(y}\sim a+b\texttt{, poisson)}\)</span> # Poisson errors, log link<br>
<span class="math inline">\(\texttt{glm(y}\sim\texttt{a+b, poisson("sqrt"))}\)</span> # Poisson errors, sqrt link</p>
<p><strong>Note that for the binomial case</strong>, the response variable should be an <span class="math inline">\(n \times 2\)</span> matrix <span class="math inline">\(\texttt{ym}\)</span>, say, not a vector, where the first column contains the numbers of successes and the second column the numbers of failures, for example:</p>
<p><span class="math inline">\(\texttt{glm(ym}\sim\texttt{ a+b, binomial)}\)</span> # binomial errors, logit link</p>
<p>To extract information about a fitted generalized linear model, it is best to store the result of <span class="math inline">\(\texttt{glm}\)</span> as a variable and then to use the following functions:</p>
<ul>
<li>To fit a GLM and store the result in <span class="math inline">\(\texttt{y.glm}\)</span> (for example):<br>
<span class="math inline">\(\texttt{y.glm = glm(y}\sim\texttt{a*b, poisson("sqrt"))}\)</span></li>
<li>To print various pieces of information including deviance residuals, parameter estimates and standard errors, deviances, and (if specified) correlations of parameter estimates:<br>
<span class="math inline">\(\texttt{summary(y.glm, correlation=T)}\)</span></li>
<li>To print the anova table of the fitted model:<br>
<span class="math inline">\(\texttt{anova(y.glm)}\)</span></li>
<li>To print the deviance of the fitted model:<br>
<span class="math inline">\(\texttt{deviance(y.glm)}\)</span></li>
<li>To print the residual degrees of freedom of the fitted model:<br>
<span class="math inline">\(\texttt{df.residual(y.glm)}\)</span></li>
<li>To print the vector of fitted values under the fitted model:<br>
<span class="math inline">\(\texttt{fitted.values(y.glm)}\)</span></li>
<li>To print the residuals from the fitted model:<br>
<span class="math inline">\(\texttt{residuals(y.glm, type)}\)</span><br>
Note: <span class="math inline">\(\texttt{type}\)</span> should be <span class="math inline">\(\texttt{"deviance"}\)</span> (default), <span class="math inline">\(\texttt{"pearson"}\)</span>, or <span class="math inline">\(\texttt{"response"}\)</span></li>
<li>To print the parameter estimates from the fitted model:<br>
<span class="math inline">\(\texttt{coefficients(y.glm)}\)</span></li>
<li>To print the design matrix for a specified model formula: <span class="math inline">\(\texttt{model.matrix(y}\sim\texttt{a*b)}\)</span></li>
</ul>
<p>The functions <span class="math inline">\(\texttt{summary}\)</span>, <span class="math inline">\(\texttt{anova}\)</span>, and possibly <span class="math inline">\(\texttt{model.matrix}\)</span> are the most useful for printing out information about the fitted model. The results of the other functions can be saved as variables for further computation, if desired.</p>
</section>
<section id="example-of-fitting-poisson-glm-in-r" class="level3" data-number="4.5.2">
<h3 data-number="4.5.2" class="anchored" data-anchor-id="example-of-fitting-poisson-glm-in-r"><span class="header-section-number">4.5.2</span> Example of fitting Poisson GLM in <strong>R</strong></h3>
<p>Here is a toy example of <strong>R</strong> commands for modelling a response in terms of two qualitative explanatory variables (that is <em>factors</em>). The model assumes the data are Poisson-distributed and uses the logarithmic link function.</p>
<div class="cell">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">7</span>, <span class="dv">8</span>, <span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">7</span>, <span class="dv">10</span>, <span class="dv">2</span>, <span class="dv">8</span>, <span class="dv">16</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>a <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>,<span class="at">times=</span><span class="dv">3</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>b <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>,<span class="at">each=</span><span class="dv">4</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>a <span class="ot">=</span> <span class="fu">as.factor</span>(a)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>b <span class="ot">=</span> <span class="fu">as.factor</span>(b)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>y.glm <span class="ot">=</span> <span class="fu">glm</span>(y <span class="sc">~</span> a<span class="sc">+</span>b, poisson)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(y.glm, <span class="at">correlation=</span>T)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
glm(formula = y ~ a + b, family = poisson)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.8740  -0.6834   0.1287   0.7151   1.5956  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   1.1408     0.3351   3.404 0.000663 ***
a2           -0.3054     0.3522  -0.867 0.385934    
a3            0.1466     0.3132   0.468 0.639712    
a4            0.4568     0.2932   1.558 0.119269    
b2            0.9163     0.3162   2.898 0.003761 ** 
b3            0.9445     0.3150   2.999 0.002712 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 31.725  on 11  degrees of freedom
Residual deviance: 13.150  on  6  degrees of freedom
AIC: 68.227

Number of Fisher Scoring iterations: 5

Correlation of Coefficients:
   (Intercept) a2    a3    a4    b2   
a2 -0.45                              
a3 -0.50        0.48                  
a4 -0.54        0.51  0.57            
b2 -0.67        0.00  0.00  0.00      
b3 -0.68        0.00  0.00  0.00  0.72</code></pre>
</div>
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(y.glm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Analysis of Deviance Table

Model: poisson, link: log

Response: y

Terms added sequentially (first to last)

     Df Deviance Resid. Df Resid. Dev
NULL                    11     31.725
a     3   6.2793         8     25.445
b     2  12.2947         6     13.150</code></pre>
</div>
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">deviance</span>(y.glm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 13.15047</code></pre>
</div>
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">df.residual</span>(y.glm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 6</code></pre>
</div>
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">fitted.values</span>(y.glm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>        1         2         3         4         5         6         7         8 
 3.129412  2.305882  3.623529  4.941176  7.823529  5.764706  9.058824 12.352941 
        9        10        11        12 
 8.047059  5.929412  9.317647 12.705882 </code></pre>
</div>
</div>
</section>
</section>
<section id="exercises" class="level2" data-number="4.6">
<h2 data-number="4.6" class="anchored" data-anchor-id="exercises"><span class="header-section-number">4.6</span> Exercises</h2>
<p>4.1 Use <a href="#eq-exponentialfamilybary2">Equation&nbsp;<span class="quarto-unresolved-ref">eq-exponentialfamilybary2</span></a> to obtain estimation equations for the natural parameter <span class="math inline">\(\theta\)</span>, based on a sample <span class="math inline">\(\mathbf{y}=\{y_1,\dots, y_n\}\)</span>, for each of the following situations:</p>
<ol type="a">
<li>the binomial, <span class="math inline">\(Y\sim\mbox{Bin}(n,p)\)</span>,</li>
<li>the geometric, <span class="math inline">\(Y\sim\mbox{Ge}(p)\)</span>,</li>
<li>the exponential, <span class="math inline">\(Y\sim\mbox{Exp}(\lambda)\)</span>.</li>
</ol>
<p>Are these estimators unbiased for <span class="math inline">\(\theta\)</span>? What is the variance of the estimator in each case?</p>
<p>4.2 For a sample of size <span class="math inline">\(n\)</span> from the normal distribution, <span class="math inline">\(Y\sim N(\mu, \sigma^2)\)</span>, how do the results produced using <a href="#eq-Ehattheta">Equation&nbsp;<span class="quarto-unresolved-ref">eq-Ehattheta</span></a> and <a href="#eq-Varhattheta">Equation&nbsp;<span class="quarto-unresolved-ref">eq-Varhattheta</span></a> compare with the familiar results <span class="math inline">\(\hat\mu =\bar y\)</span>, <span class="math inline">\(\text{E}[\hat\mu]=\mu\)</span>, and <span class="math inline">\(\text{Var}[\hat\mu] =\sigma^2/n\)</span>?</p>
<p>4.3 For the normal linear regression model, <span class="math inline">\(\mathbf{Y}=X\boldsymbol{\beta}+\boldsymbol{\epsilon}\)</span> where <span class="math inline">\(\boldsymbol{\epsilon}\sim N_n(0,\sigma^2 I_n)\)</span> use the principle of maximum likelihood to show that the MLE has the closed form given in <a href="#eq-linearregressionMLE">Equation&nbsp;<span class="quarto-unresolved-ref">eq-linearregressionMLE</span></a>.</p>
<p><strong>Further questions to be added later</strong></p>


</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Using the result that the MLE of any function of a parameter is given by the same function applied to the MLE of the parameter.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./3_GLM-Theory.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">GLM Theory</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./5_logisticmodel.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Modelling Proportions</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>